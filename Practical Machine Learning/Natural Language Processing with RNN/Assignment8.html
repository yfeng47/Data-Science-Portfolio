<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Assignment8</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Visuals</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="k">import</span> <span class="n">imshow</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>

<span class="c1"># Python chakin package previously installed by </span>
<span class="c1">#    pip install chakin</span>
<span class="kn">import</span> <span class="nn">chakin</span>  

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">defaultdict</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">chakin</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s1">&#39;English&#39;</span><span class="p">)</span>  <span class="c1"># lists available indices in English</span>

<span class="c1"># Specify English embeddings file to download and install</span>
<span class="c1"># by index number, number of dimensions, and subfoder name</span>
<span class="c1"># Note that GloVe 50-, 100-, 200-, and 300-dimensional folders</span>
<span class="c1"># are downloaded with a single zip download</span>
<span class="n">CHAKIN_INDEX</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">NUMBER_OF_DIMENSIONS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">SUBFOLDER_NAME</span> <span class="o">=</span> <span class="s2">&quot;glove.twitter.27B&quot;</span>

<span class="n">DATA_FOLDER</span> <span class="o">=</span> <span class="s2">&quot;embeddings&quot;</span>
<span class="n">ZIP_FILE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.zip&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SUBFOLDER_NAME</span><span class="p">))</span>
<span class="n">ZIP_FILE_ALT</span> <span class="o">=</span> <span class="s2">&quot;glove&quot;</span> <span class="o">+</span> <span class="n">ZIP_FILE</span><span class="p">[</span><span class="mi">5</span><span class="p">:]</span>  <span class="c1"># sometimes it&#39;s lowercase only...</span>
<span class="n">UNZIP_FOLDER</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">,</span> <span class="n">SUBFOLDER_NAME</span><span class="p">)</span>
<span class="k">if</span> <span class="n">SUBFOLDER_NAME</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;d&quot;</span><span class="p">:</span>
    <span class="n">GLOVE_FILENAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">UNZIP_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SUBFOLDER_NAME</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">GLOVE_FILENAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.</span><span class="si">{}</span><span class="s2">d.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">SUBFOLDER_NAME</span><span class="p">,</span> <span class="n">NUMBER_OF_DIMENSIONS</span><span class="p">))</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloading embeddings to &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">))</span>
    <span class="n">chakin</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">number</span><span class="o">=</span><span class="n">CHAKIN_INDEX</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;./</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings already downloaded.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">zipfile</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE_ALT</span><span class="p">):</span>
        <span class="n">ZIP_FILE</span> <span class="o">=</span> <span class="n">ZIP_FILE_ALT</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracting embeddings to &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">))</span>
        <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings already extracted.&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Run complete&#39;</span><span class="p">)</span>

<span class="c1">#-------------------------------------------------------------------------</span>

<span class="n">CHAKIN_INDEX</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">NUMBER_OF_DIMENSIONS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">SUBFOLDER_NAME</span> <span class="o">=</span> <span class="s2">&quot;gloVe.6B&quot;</span>

<span class="n">DATA_FOLDER</span> <span class="o">=</span> <span class="s2">&quot;embeddings&quot;</span>
<span class="n">ZIP_FILE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.zip&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SUBFOLDER_NAME</span><span class="p">))</span>
<span class="n">ZIP_FILE_ALT</span> <span class="o">=</span> <span class="s2">&quot;glove&quot;</span> <span class="o">+</span> <span class="n">ZIP_FILE</span><span class="p">[</span><span class="mi">5</span><span class="p">:]</span>  <span class="c1"># sometimes it&#39;s lowercase only...</span>
<span class="n">UNZIP_FOLDER</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">,</span> <span class="n">SUBFOLDER_NAME</span><span class="p">)</span>
<span class="k">if</span> <span class="n">SUBFOLDER_NAME</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;d&quot;</span><span class="p">:</span>
    <span class="n">GLOVE_FILENAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">UNZIP_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SUBFOLDER_NAME</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">GLOVE_FILENAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.</span><span class="si">{}</span><span class="s2">d.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">SUBFOLDER_NAME</span><span class="p">,</span> <span class="n">NUMBER_OF_DIMENSIONS</span><span class="p">))</span>


<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">):</span>
    <span class="c1"># GloVe by Stanford is licensed Apache 2.0:</span>
    <span class="c1">#     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE</span>
    <span class="c1">#     http://nlp.stanford.edu/data/glove.twitter.27B.zip</span>
    <span class="c1">#     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloading embeddings to &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">))</span>
    <span class="n">chakin</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">number</span><span class="o">=</span><span class="n">CHAKIN_INDEX</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;./</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">DATA_FOLDER</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings already downloaded.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">zipfile</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">ZIP_FILE_ALT</span><span class="p">):</span>
        <span class="n">ZIP_FILE</span> <span class="o">=</span> <span class="n">ZIP_FILE_ALT</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">ZIP_FILE</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracting embeddings to &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">))</span>
        <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">UNZIP_FOLDER</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings already extracted.&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Run complete&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>                   Name  Dimension                     Corpus VocabularySize  \
2          fastText(en)        300                  Wikipedia           2.5M   
11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   
12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   
13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   
14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   
15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   
16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   
17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   
18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   
19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   
20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   
21  word2vec.GoogleNews        300          Google News(100B)           3.0M   

      Method Language    Author  
2   fastText  English  Facebook  
11     GloVe  English  Stanford  
12     GloVe  English  Stanford  
13     GloVe  English  Stanford  
14     GloVe  English  Stanford  
15     GloVe  English  Stanford  
16     GloVe  English  Stanford  
17     GloVe  English  Stanford  
18     GloVe  English  Stanford  
19     GloVe  English  Stanford  
20     GloVe  English  Stanford  
21  word2vec  English    Google  
Embeddings already downloaded.
Embeddings already extracted.

Run complete
Embeddings already downloaded.
Embeddings already extracted.

Run complete
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">os</span>  <span class="c1"># operating system functions</span>
<span class="kn">import</span> <span class="nn">os.path</span>  <span class="c1"># for manipulation of file path names</span>

<span class="kn">import</span> <span class="nn">re</span>  <span class="c1"># regular expressions</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="k">import</span> <span class="n">TreebankWordTokenizer</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">9999</span>





<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="k">import</span> <span class="n">stopwords</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="k">import</span> <span class="n">one_hot</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="k">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="k">import</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">GlobalMaxPooling1D</span>
<span class="kn">from</span> <span class="nn">keras.layers.embeddings</span> <span class="k">import</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="k">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.layers.convolutional</span> <span class="k">import</span> <span class="n">Conv1D</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create file paths to gdrive</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">9999</span>

<span class="k">def</span> <span class="nf">reset_graph</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">RANDOM_SEED</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;Makes output stable across runs&#39;&#39;&#39;</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Declare Variables</span>
<span class="n">REMOVE_STOPWORDS</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># no stopword removal</span>
<span class="n">EVOCABSIZE</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># specify desired size of pre-defined embedding vocabulary</span>

<span class="c1"># Select the pre-defined embeddings source</span>
<span class="n">embeddings_dir_one</span> <span class="o">=</span> <span class="s2">&quot;/Users/dannyarenson/Desktop/MSDS422/run-jump-start-rnn-sentiment-v002/embeddings/gloVe.6B&quot;</span>
<span class="n">embeddings_dir_two</span> <span class="o">=</span> <span class="s2">&quot;/Users/dannyarenson/Desktop/MSDS422/run-jump-start-rnn-sentiment-v002/embeddings/gloVe.6B&quot;</span>

<span class="n">filename_50</span> <span class="o">=</span> <span class="s2">&quot;glove.6B.50d.txt&quot;</span>
<span class="n">filename_100</span> <span class="o">=</span> <span class="s2">&quot;glove.6B.100d.txt&quot;</span>

<span class="n">embeddings_filename_50</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">embeddings_dir_one</span><span class="p">,</span> 
    <span class="n">embeddings_dir_two</span> <span class="p">,</span> 
    <span class="n">filename_50</span>
<span class="p">)</span>

<span class="n">embeddings_filename_100</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">embeddings_dir_one</span><span class="p">,</span> 
    <span class="n">embeddings_dir_two</span> <span class="p">,</span> 
    <span class="n">filename_100</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">reset_graph</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span> <span class="n">RANDOM_SEED</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">REMOVE_STOPWORDS</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># no stopword removal </span>
<span class="n">EVOCABSIZE</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># specify desired size of pre-defined embedding vocabulary</span>

<span class="n">embeddings_directory</span> <span class="o">=</span> <span class="s1">&#39;embeddings/glove.twitter.27B&#39;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;glove.twitter.27B.100d.txt&#39;</span>

<span class="n">embeddings_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">embeddings_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="n">embeddings_filename</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Utility function for loading embeddings</span>
<span class="c1"># Creates the Python defaultdict dictionary word_to_embedding_dict</span>
<span class="c1"># for the requested pre-trained word embeddings</span>
<span class="c1"># Note the use of defaultdict data structure from the Python Standard Library</span>
<span class="c1"># collections_defaultdict.py lets the caller specify a default value up front</span>
<span class="c1"># The default value will be retuned if the key is not a known dictionary key</span>
<span class="c1"># That is, unknown words are represented by a vector of zeros</span>
<span class="c1"># For word embeddings, this default value is a vector of zeros</span>

<span class="k">def</span> <span class="nf">load_embedding_from_disks</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Read a embeddings txt file. If `with_indexes=True`,</span>
<span class="sd">    we return a tuple of two dictionnaries</span>
<span class="sd">    `(word_to_index_dict, index_to_embedding_array)`,</span>
<span class="sd">    otherwise we return only a direct</span>
<span class="sd">    `word_to_embedding_dict` dictionnary mapping</span>
<span class="sd">    from a string to a numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
        <span class="n">word_to_index_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">index_to_embedding_array</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">word_to_embedding_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">embeddings_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings_file</span><span class="p">):</span>

            <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>

            <span class="n">word</span> <span class="o">=</span> <span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">representation</span> <span class="o">=</span> <span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">representation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">representation</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
                <span class="n">word_to_index_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">index_to_embedding_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_to_embedding_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">representation</span>

    <span class="c1"># Empty representation for unknown words.</span>
    <span class="n">_WORD_NOT_FOUND</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
        <span class="n">_LAST_INDEX</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">word_to_index_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">_LAST_INDEX</span><span class="p">,</span> <span class="n">word_to_index_dict</span><span class="p">)</span>
        <span class="n">index_to_embedding_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">index_to_embedding_array</span> <span class="o">+</span> <span class="p">[</span><span class="n">_WORD_NOT_FOUND</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">word_to_index_dict</span><span class="p">,</span> <span class="n">index_to_embedding_array</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word_to_embedding_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">_WORD_NOT_FOUND</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">word_to_embedding_dict</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Loading embeddings from&#39;</span><span class="p">,</span> <span class="n">embeddings_filename</span><span class="p">)</span>
<span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_embedding</span> <span class="o">=</span> \
    <span class="n">load_embedding_from_disks</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding loaded from disks.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Loading embeddings from embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt
Embedding loaded from disks.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load embeddings for glove 50</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading embeddings from&quot;</span><span class="p">,</span> <span class="n">embeddings_filename_50</span><span class="p">)</span>

<span class="n">word_to_index_50</span><span class="p">,</span> <span class="n">index_to_embedding_50</span> <span class="o">=</span> <span class="n">load_embedding_from_disks</span><span class="p">(</span>
    <span class="n">embeddings_filename_50</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding loaded from disks.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Loading embeddings from /Users/dannyarenson/Desktop/MSDS422/run-jump-start-rnn-sentiment-v002/embeddings/gloVe.6B/glove.6B.50d.txt
Embedding loaded from disks.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load embeddings for glove 100</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading embeddings from&quot;</span><span class="p">,</span> <span class="n">embeddings_filename_100</span><span class="p">)</span>

<span class="n">word_to_index_100</span><span class="p">,</span> <span class="n">index_to_embedding_100</span> <span class="o">=</span> <span class="n">load_embedding_from_disks</span><span class="p">(</span>
    <span class="n">embeddings_filename_100</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding loaded from disks.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Loading embeddings from /Users/dannyarenson/Desktop/MSDS422/run-jump-start-rnn-sentiment-v002/embeddings/gloVe.6B/glove.6B.100d.txt
Embedding loaded from disks.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check embedding size/shapes</span>
<span class="n">vocab_size_50</span><span class="p">,</span> <span class="n">embedding_dim_50</span> <span class="o">=</span> <span class="n">index_to_embedding_50</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;50 Dimension embedding ---------------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding is of shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index_to_embedding_50</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The first words are words that tend occur more often.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">vocab_size_100</span><span class="p">,</span> <span class="n">embedding_dim_100</span> <span class="o">=</span> <span class="n">index_to_embedding_100</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;100 Dimension embedding ---------------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding is of shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index_to_embedding_100</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This means (number of words, number of dimensions per word)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The first words are words that tend occur more often.&quot;</span><span class="p">)</span>

<span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding is of shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This means (number of words, number of dimensions per word)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The first words are words that tend occur more often.&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Note: for unknown words, the representation is an empty vector,</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="s2">&quot;and the index is the last one. The dictionnary has a limit:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2"> --&gt; </span><span class="si">{}</span><span class="s2"> --&gt; </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;A word&quot;</span><span class="p">,</span> <span class="s2">&quot;Index in embedding&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;Representation&quot;</span><span class="p">))</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;worsdfkljsdf&quot;</span>  <span class="c1"># a word obviously not in the vocabulary</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="c1"># index for word obviously not in the vocabulary</span>
<span class="n">complete_vocabulary_size</span> <span class="o">=</span> <span class="n">idx</span> 
<span class="n">embd</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">))</span> <span class="c1"># &quot;int&quot; compact print</span>
<span class="c1">#print(&quot;    {} --&gt; {} --&gt; {}&quot;.format(word, idx, embd))</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;the&quot;</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
<span class="n">embd</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>50 Dimension embedding ---------------------------------------
Embedding is of shape: (400001, 50)
The first words are words that tend occur more often.

100 Dimension embedding ---------------------------------------
Embedding is of shape: (400001, 100)
This means (number of words, number of dimensions per word)

The first words are words that tend occur more often.
Embedding is of shape: (1193515, 100)
This means (number of words, number of dimensions per word)

The first words are words that tend occur more often.
Note: for unknown words, the representation is an empty vector,
and the index is the last one. The dictionnary has a limit:
    A word --&gt; Index in embedding --&gt; Representation
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Show how to use embeddings dictionaries with a test sentence</span>
<span class="c1"># This is a famous typing exercise with all letters of the alphabet</span>
<span class="c1"># https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog</span>
<span class="n">a_typing_test_sentence</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumps over the lazy dog&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test sentence: &quot;</span><span class="p">,</span> <span class="n">a_typing_test_sentence</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">words_in_test_sentence</span> <span class="o">=</span> <span class="n">a_typing_test_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_in_test_sentence</span><span class="p">:</span>
    <span class="n">word_</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">index_to_embedding_50</span><span class="p">[</span><span class="n">word_to_index_50</span><span class="p">[</span><span class="n">word_</span><span class="p">]]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word_</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Test sentence:  The quick brown fox jumps over the lazy dog 

the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02
 -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01
 -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01
 -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01
 -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01
  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03
  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01
 -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01
 -1.1514e-01 -7.8581e-01]
quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868
 -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647
 -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287
 -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902
  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061
  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156
  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465
 -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268
  0.57892    0.64483  ]
brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698
 -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371
  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394
  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486
  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938
 -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434
 -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057
  0.73274 ]
fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593
 -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472
  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174
  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122
  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935
 -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251
 -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204
  1.5064  ]
jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248
  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302
 -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953
 -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146
  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648
  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087
 -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422
  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728
  0.55305   -0.0028062]
over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831
 -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317
 -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495
 -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255
 -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189
  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163
 -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157
  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912
 -0.60515   -0.9827   ]
the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02
 -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01
 -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01
 -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01
 -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01
  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03
  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01
 -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01
 -1.1514e-01 -7.8581e-01]
lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014
 -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669
 -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139
  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596
  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842
 -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736
 -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752
  1.1672  ]
dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994
 -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878
  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397
 -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555
  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327
  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118
  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068
 -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118
  0.7158     0.38519  ]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check 100 embeddings</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_in_test_sentence</span><span class="p">:</span>
    <span class="n">word_</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">index_to_embedding_100</span><span class="p">[</span><span class="n">word_to_index_100</span><span class="p">[</span><span class="n">word_</span><span class="p">]]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word_</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141
  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384
 -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464
 -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155
 -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021
  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531
  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559
 -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243
  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514
  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044
  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212
 -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148
 -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215
 -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459
  0.8278    0.27062 ]
quick:  [-0.43146   -0.22037   -0.22684   -0.10215   -0.31863   -0.11809
 -0.093402  -0.069789  -0.29029   -0.34006    0.099652  -0.059301
 -0.43764    0.19464    0.36997    0.73648   -0.53429   -0.3469
 -0.21415    0.62954    0.54868    0.29429   -0.32889   -0.61771
 -0.039648   0.91639   -0.64046    0.28725    0.095922  -0.38774
 -0.62958    0.33443   -0.4856    -0.2287     0.84277   -0.2204
 -0.13264   -0.18188    0.077686   0.080045  -0.018909  -0.26018
  0.29542   -0.89173   -0.39373   -0.35662    0.011656  -0.37658
  0.64576   -0.86503    0.12615    0.18984   -0.26936    0.56216
  0.38218   -2.1389    -0.0096116  0.15041    1.2586    -0.35475
 -0.33285    0.07292   -0.077262   0.049068   0.90212   -0.27539
 -0.20839    0.26349   -0.26515   -0.70593   -0.68474    0.38424
 -0.21889   -0.88545    0.38583    0.26481   -0.7641    -0.037501
 -0.020606  -0.71318    1.1045     0.0453    -0.41902   -0.47667
 -1.4088    -0.50376    0.88062    0.0072194 -0.42083   -0.62586
  0.59608    0.30444   -0.40999   -0.28204   -0.52321   -0.44695
  0.21083   -0.010209   0.0086056  0.63263  ]
brown:  [-4.3812e-01 -9.9389e-02 -2.6038e-01 -1.1084e+00  1.0550e-01 -5.4542e-02
  4.4868e-01  6.1750e-02 -5.8803e-01 -2.1738e-01 -3.6304e-01 -4.0887e-01
  3.7877e-02  8.4201e-01  1.0108e-01 -1.8530e-01  5.0486e-01 -3.4252e-01
  2.2516e-01 -2.6942e-02 -4.6399e-01  9.9140e-02  1.9596e-02 -6.7435e-01
  6.3123e-01  9.5930e-01  1.6215e-01 -4.3166e-01 -2.6642e-01  1.9136e-01
  4.5626e-01  6.8918e-01  3.6808e-01 -2.8273e-01 -4.6525e-01  5.9984e-01
  1.5369e-01  8.6585e-01  2.7917e-01  5.8380e-01 -4.6627e-01 -1.3590e+00
 -1.0387e-01  6.0146e-02 -5.2733e-01  1.3135e-01 -3.3766e-01  1.7893e-01
  4.4812e-01 -7.0502e-01  6.3793e-01 -7.9508e-01  1.3176e-01  9.7769e-01
 -2.3153e-01 -2.6450e+00 -1.1464e-01  2.7907e-01  4.9121e-01  5.1274e-01
  7.9559e-04  1.7932e-01 -2.9938e-01 -3.3465e-01  9.9161e-01 -6.0262e-01
  7.2080e-01  8.4681e-01 -2.3669e-01  1.3666e-01 -3.5330e-01  3.9442e-01
 -7.2818e-01  9.1664e-02  3.0441e-01  4.8352e-02 -4.1140e-01  3.4362e-01
  1.2569e-01  4.2484e-01  4.5470e-01  1.6292e-01 -1.3630e-01 -2.1827e-01
 -3.8261e-01 -9.2620e-01  5.1256e-01 -3.5184e-01  1.8316e-01  1.9807e-01
 -1.9681e-02 -7.2242e-01 -4.3439e-01  1.3449e-01 -8.4339e-01  1.3815e-02
 -1.1325e+00  1.8143e-01 -1.9537e-01 -3.6954e-01]
fox:  [ 0.16917   -0.99783    0.24429   -0.79687    0.036447  -0.56127
  0.17305    0.29287   -0.43291   -0.82274   -0.11437   -0.28808
  0.20501   -0.4878     0.50534   -0.2117     0.48474    0.20959
  0.26642    0.6839    -0.2629     0.14794    0.087969  -0.17349
  0.61804    0.63733    0.41145    0.46401   -0.2165     0.5
  0.65265    1.0608     0.19275    0.141      0.51356    0.72558
 -0.044848  -0.35761    0.49862    0.73592   -0.38307    0.12159
 -0.75345    0.80579   -0.48075   -0.40283   -0.49931   -0.60309
  0.26126   -0.24109   -0.55885   -0.10622    0.11289    0.49708
  0.015915  -2.452     -0.32529    0.20437    0.55361    0.60879
 -0.083061   0.60856    0.13958   -0.71847    1.1409     0.023752
  0.050995   0.29621   -0.16247    1.1456     0.16929   -0.0042113
 -0.4026    -0.073144   0.096698  -0.15248   -0.69435    0.28032
 -1.0238     0.58777   -0.34573   -0.60871    0.1842    -0.18736
 -0.49948   -0.18095   -0.71161    0.69437    0.37298   -0.308
  0.2455    -0.94515    0.20393   -0.14885   -1.1153    -0.52266
 -0.27841    0.027184   0.39712    0.17933  ]
jumps:  [ 0.87831   0.76211   0.24562  -0.05516   0.10355  -0.6789   -0.36757
  0.52207  -0.37174  -0.10266   1.0164    0.97297   0.028706  0.22013
  0.36371   0.79072  -1.5199    0.72657   0.24994   0.07658   0.79373
  0.32268  -0.28497   0.30724   0.25493   0.049801 -0.68182   0.059687
  0.40362  -0.73308  -0.5968    0.2901    0.15876   0.070044  0.57204
  0.70252  -0.86423  -0.1618   -0.026244  0.19154  -0.14515   0.34694
 -0.62756   0.15429  -0.56114   0.15854  -0.56041  -0.39705   0.31183
 -0.19028  -0.53601   0.061462  0.12484   1.3302    0.34361  -1.1603
  0.10341   0.33138   0.74712   0.11517   0.17949   0.059578  0.22881
  0.52396  -0.43749   0.33677   0.028801 -0.67852   0.21443   0.038026
 -0.87474  -0.22532   0.020465  1.0772    0.71369  -0.14903  -0.53563
 -0.049547  0.23989  -0.19058   0.13683   0.29553  -0.20244  -0.40515
 -0.24246  -1.0324    0.32728  -0.46241   0.27757  -0.23512  -0.23432
  0.1031   -0.54905   0.21484  -0.16597  -0.34962  -0.16015  -0.2617
  0.41802  -0.055161]
over:  [-2.9574e-01  3.5345e-01  6.3326e-01  1.9576e-01 -3.0256e-02  5.4244e-01
 -2.1091e-01  3.2894e-01 -4.8888e-01  1.8379e-01  2.4242e-01  4.0346e-01
  1.1973e-01  1.3143e-02  2.4154e-01 -4.0184e-01  2.2176e-01 -2.7837e-01
 -4.6930e-01 -5.4899e-02  6.5148e-01  1.5958e-01  5.9556e-01  3.3167e-01
  7.2649e-01 -4.3182e-01  1.7208e-01 -1.1584e-02 -2.6389e-01 -2.2073e-01
 -2.8538e-01  3.5863e-01  2.4592e-01  2.2143e-01 -7.6221e-01  3.9352e-01
 -2.3915e-02  4.3028e-01 -4.7099e-01  2.5162e-01 -5.9507e-01 -1.0495e+00
  1.7973e-01 -3.1621e-01  2.3788e-01 -8.8560e-02  3.4751e-01 -5.5950e-01
  1.2997e-01 -7.0101e-01  2.8850e-01  1.8111e-01 -2.3004e-01  2.0682e+00
 -1.4925e-01 -2.8700e+00 -4.6722e-03 -2.2819e-01  1.6623e+00  6.5951e-01
  2.1892e-01  6.3600e-01  1.0332e-01  1.3176e-03  4.4414e-01  2.0222e-01
  5.2490e-01  6.4131e-01  2.7416e-01  1.0695e-01 -1.2030e-01  4.7109e-02
 -5.3503e-01 -4.6869e-01 -7.6050e-02  1.0654e-03 -3.8456e-01 -2.4067e-02
 -7.5877e-01  5.2622e-01  1.3285e+00 -3.9051e-01 -1.2174e-01  5.1886e-01
 -1.0374e+00 -3.3789e-01  7.4933e-02  2.0036e-01  2.4703e-02 -2.9090e-01
 -3.2043e-01  2.0445e-02 -9.9185e-01  1.6802e-02 -6.0819e-01 -2.6601e-01
 -1.9549e-01  2.3127e-01  9.4771e-01 -9.5560e-02]
the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141
  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384
 -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464
 -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155
 -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021
  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531
  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559
 -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243
  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514
  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044
  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212
 -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148
 -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215
 -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459
  0.8278    0.27062 ]
lazy:  [ 0.14481   -0.20397    0.3596    -0.59938   -0.93979    0.59784
 -0.21619    0.73051   -0.36588   -0.19962    0.14571    0.1642
  0.1086    -0.78575    0.53327    0.37127   -0.33013   -0.082276
  0.73923    0.86931    0.37934    1.2427    -0.19554   -0.53849
  0.20681    0.76727   -0.9714    -0.016255  -0.12529    0.36231
  0.13313    0.60993    0.44345   -0.3654     0.22531    0.72985
 -0.69992    0.14427    0.85324    0.21268   -0.46674    0.25746
 -0.88493    0.042164  -0.24125   -0.11241   -0.52837    0.38905
  0.35523    0.29078   -0.47363   -0.30561    0.072255   0.31778
 -0.64297   -0.3527     0.49651    0.29722    0.68888   -0.54184
  0.04863    0.26221   -0.61438   -0.2591     0.66305    0.25526
  0.42406   -0.22196   -0.053041  -0.80721   -0.89748   -0.1165
  0.45258    0.24817   -0.14874   -0.20952   -0.58499    0.5573
  0.47503   -0.6429    -0.11219    0.2627    -0.4951    -0.0085495
 -0.86135   -0.21422    0.0086754  0.35554   -0.48077   -0.39897
 -0.012746   0.13761   -0.20283    0.40565    0.056275  -0.35009
 -0.745     -0.42987   -0.56238   -0.13433  ]
dog:  [ 0.30817    0.30938    0.52803   -0.92543   -0.73671    0.63475
  0.44197    0.10262   -0.09142   -0.56607   -0.5327     0.2013
  0.7704    -0.13983    0.13727    1.1128     0.89301   -0.17869
 -0.0019722  0.57289    0.59479    0.50428   -0.28991   -1.3491
  0.42756    1.2748    -1.1613    -0.41084    0.042804   0.54866
  0.18897    0.3759     0.58035    0.66975    0.81156    0.93864
 -0.51005   -0.070079   0.82819   -0.35346    0.21086   -0.24412
 -0.16554   -0.78358   -0.48482    0.38968   -0.86356   -0.016391
  0.31984   -0.49246   -0.069363   0.018869  -0.098286   1.3126
 -0.12116   -1.2399    -0.091429   0.35294    0.64645    0.089642
  0.70294    1.1244     0.38639    0.52084    0.98787    0.79952
 -0.34625    0.14095    0.80167    0.20987   -0.86007   -0.15308
  0.074523   0.40816    0.019208   0.51587   -0.34428   -0.24525
 -0.77984    0.27425    0.22418    0.20164    0.017431  -0.014697
 -1.0235    -0.39695   -0.0056188  0.30569    0.31748    0.021404
  0.11837   -0.11319    0.42456    0.53405   -0.16717   -0.27185
 -0.6255     0.12883    0.62529   -0.52086  ]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-Preprocessing">Model Preprocessing<a class="anchor-link" href="#Model-Preprocessing">&#182;</a></h3><h4 id="File-Load-Preparation">File Load Preparation<a class="anchor-link" href="#File-Load-Preparation">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define vocabulary size for the language model</span>
<span class="c1"># To reduce the size of the vocabulary to the n most frequently used words</span>

<span class="k">def</span> <span class="nf">default_factory</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">EVOCABSIZE</span>  <span class="c1"># last/unknown-word row in limited_index_to_embedding</span>


<span class="c1"># dictionary has the items() function, returns list of (key, value) tuples</span>
<span class="n">limited_word_to_index_50</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
    <span class="n">default_factory</span><span class="p">,</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word_to_index_50</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">EVOCABSIZE</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Select the first EVOCABSIZE rows to the index_to_embedding</span>
<span class="n">limited_index_to_embedding_50</span> <span class="o">=</span> <span class="n">index_to_embedding_50</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">EVOCABSIZE</span><span class="p">,</span> <span class="p">:]</span>

<span class="c1"># Set the unknown-word row to be all zeros as previously</span>
<span class="n">limited_index_to_embedding_50</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">limited_index_to_embedding_50</span><span class="p">,</span>
    <span class="n">index_to_embedding_50</span><span class="p">[</span><span class="n">index_to_embedding_50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim_50</span><span class="p">),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Delete large numpy array to clear some CPU RAM</span>
<span class="k">del</span> <span class="n">index_to_embedding_50</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dictionary has the items() function, returns list of (key, value) tuples</span>
<span class="n">limited_word_to_index_100</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
    <span class="n">default_factory</span><span class="p">,</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word_to_index_100</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">EVOCABSIZE</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Select the first EVOCABSIZE rows to the index_to_embedding</span>
<span class="n">limited_index_to_embedding_100</span> <span class="o">=</span> <span class="n">index_to_embedding_100</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">EVOCABSIZE</span><span class="p">,</span> <span class="p">:]</span>

<span class="c1"># Set the unknown-word row to be all zeros as previously</span>
<span class="n">limited_index_to_embedding_100</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">limited_index_to_embedding_100</span><span class="p">,</span>
    <span class="n">index_to_embedding_100</span><span class="p">[</span><span class="n">index_to_embedding_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim_100</span><span class="p">),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Delete large numpy array to clear some CPU RAM</span>
<span class="k">del</span> <span class="n">index_to_embedding_100</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># code for working with movie reviews data</span>
<span class="c1"># Source: Miller, T. W. (2016). Web and Network Data Science.</span>
<span class="c1">#    Upper Saddle River, N.J.: Pearson Education.</span>
<span class="c1">#    ISBN-13: 978-0-13-388644-3</span>
<span class="c1"># This original study used a simple bag-of-words approach</span>
<span class="c1"># to sentiment analysis, along with pre-defined lists of</span>
<span class="c1"># negative and positive words.</span>
<span class="c1"># Code available at:  https://github.com/mtpa/wnds</span>
<span class="c1"># ------------------------------------------------------------</span>
<span class="c1"># Utility function to get file names within a directory</span>
<span class="k">def</span> <span class="nf">listdir_no_hidden</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">start_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">end_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">start_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">file</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">):</span>
            <span class="n">end_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">end_list</span>


<span class="c1"># define list of codes to be dropped from document</span>
<span class="c1"># carriage-returns, line-feeds, tabs</span>
<span class="n">codelist</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We will not remove stopwords in this exercise because they are</span>
<span class="c1"># important to keeping sentences intact</span>
<span class="k">if</span> <span class="n">REMOVE_STOPWORDS</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">))</span>

    <span class="c1"># previous analysis of a list of top terms showed a number of words, along</span>
    <span class="c1"># with contractions and other word strings to drop from further analysis, add</span>
    <span class="c1"># these to the usual English stopwords to be dropped from a document collection</span>
    <span class="n">more_stop_words</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;cant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;didnt&quot;</span><span class="p">,</span>
        <span class="s2">&quot;doesnt&quot;</span><span class="p">,</span>
        <span class="s2">&quot;dont&quot;</span><span class="p">,</span>
        <span class="s2">&quot;goes&quot;</span><span class="p">,</span>
        <span class="s2">&quot;isnt&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hes&quot;</span><span class="p">,</span>
        <span class="s2">&quot;shes&quot;</span><span class="p">,</span>
        <span class="s2">&quot;thats&quot;</span><span class="p">,</span>
        <span class="s2">&quot;theres&quot;</span><span class="p">,</span>
        <span class="s2">&quot;theyre&quot;</span><span class="p">,</span>
        <span class="s2">&quot;wont&quot;</span><span class="p">,</span>
        <span class="s2">&quot;youll&quot;</span><span class="p">,</span>
        <span class="s2">&quot;youre&quot;</span><span class="p">,</span>
        <span class="s2">&quot;youve&quot;</span><span class="p">,</span>
        <span class="s2">&quot;br&quot;</span> <span class="s2">&quot;ve&quot;</span><span class="p">,</span>
        <span class="s2">&quot;re&quot;</span><span class="p">,</span>
        <span class="s2">&quot;vs&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">some_proper_nouns_to_remove</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;dick&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ginger&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hollywood&quot;</span><span class="p">,</span>
        <span class="s2">&quot;jack&quot;</span><span class="p">,</span>
        <span class="s2">&quot;jill&quot;</span><span class="p">,</span>
        <span class="s2">&quot;john&quot;</span><span class="p">,</span>
        <span class="s2">&quot;karloff&quot;</span><span class="p">,</span>
        <span class="s2">&quot;kudrow&quot;</span><span class="p">,</span>
        <span class="s2">&quot;orson&quot;</span><span class="p">,</span>
        <span class="s2">&quot;peter&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tcm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tom&quot;</span><span class="p">,</span>
        <span class="s2">&quot;toni&quot;</span><span class="p">,</span>
        <span class="s2">&quot;welles&quot;</span><span class="p">,</span>
        <span class="s2">&quot;william&quot;</span><span class="p">,</span>
        <span class="s2">&quot;wolheim&quot;</span><span class="p">,</span>
        <span class="s2">&quot;nikita&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># start with the initial list and add to it for movie text work</span>
    <span class="n">stoplist</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">more_stop_words</span>
        <span class="o">+</span> <span class="n">some_proper_nouns_to_remove</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># text parsing function for creating text documents</span>
<span class="c1"># there is more we could do for data preparation</span>
<span class="c1"># stemming... looking for contractions... possessives...</span>
<span class="c1"># but we will work with what we have in this parsing function</span>
<span class="c1"># if we want to do stemming at a later time, we can use</span>
<span class="c1">#     porter = nltk.PorterStemmer()</span>
<span class="c1"># in a construction like this</span>
<span class="c1">#     words_stemmed =  [porter.stem(word) for word in initial_words]</span>
<span class="k">def</span> <span class="nf">text_parse</span><span class="p">(</span><span class="n">string</span><span class="p">):</span>
    <span class="c1"># replace non-alphanumeric with space</span>
    <span class="n">temp_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[^a-zA-Z]&quot;</span><span class="p">,</span> <span class="s2">&quot;  &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="c1"># replace codes with space</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">codelist</span><span class="p">)):</span>
        <span class="n">stopstring</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">codelist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;  &quot;</span>
        <span class="n">temp_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">stopstring</span><span class="p">,</span> <span class="s2">&quot;  &quot;</span><span class="p">,</span> <span class="n">temp_string</span><span class="p">)</span>
    <span class="c1"># replace single-character words with space</span>
    <span class="n">temp_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;\s.\s&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">temp_string</span><span class="p">)</span>
    <span class="c1"># convert uppercase to lowercase</span>
    <span class="n">temp_string</span> <span class="o">=</span> <span class="n">temp_string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">REMOVE_STOPWORDS</span><span class="p">:</span>
        <span class="c1"># replace selected character strings/stop-words with space</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stoplist</span><span class="p">)):</span>
            <span class="n">stopstring</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">stoplist</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span>
            <span class="n">temp_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">stopstring</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">temp_string</span><span class="p">)</span>
    <span class="c1"># replace multiple blank characters with one blank character</span>
    <span class="n">temp_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">temp_string</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">temp_string</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-Review-Data">Load Review Data<a class="anchor-link" href="#Load-Review-Data">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------</span>
<span class="c1"># gather data for 500 negative movie reviews</span>
<span class="c1"># -----------------------------------------------</span>
<span class="n">dir_name</span> <span class="o">=</span> <span class="s1">&#39;movie-reviews-negative&#39;</span>
    
<span class="n">filenames</span> <span class="o">=</span> <span class="n">listdir_no_hidden</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">dir_name</span><span class="p">)</span>
<span class="n">num_files</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)):</span>
    <span class="n">file_exists</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">assert</span> <span class="n">file_exists</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Directory:&#39;</span><span class="p">,</span><span class="n">dir_name</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> files found&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">))</span>

<span class="c1"># Read data for negative movie reviews</span>
<span class="c1"># Data will be stored in a list of lists where the each list represents </span>
<span class="c1"># a document and document is a list of words.</span>
<span class="c1"># We then break the text into words.</span>

<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>

  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">text_parse</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># The Penn Treebank</span>

  <span class="k">return</span> <span class="n">data</span>

<span class="n">negative_documents</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Processing document files under&#39;</span><span class="p">,</span> <span class="n">dir_name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_files</span><span class="p">):</span>
    <span class="c1">## print(&#39; &#39;, filenames[i])</span>

    <span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">negative_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="c1"># print(&#39;Data size (Characters) (Document %d) %d&#39; %(i,len(words)))</span>
    <span class="c1"># print(&#39;Sample string (Document %d) %s&#39;%(i,words[:50]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Directory: movie-reviews-negative
500 files found

Processing document files under movie-reviews-negative
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------</span>
<span class="c1"># gather data for 500 positive movie reviews</span>
<span class="c1"># -----------------------------------------------</span>
<span class="n">dir_name</span> <span class="o">=</span> <span class="s1">&#39;movie-reviews-positive&#39;</span>  
<span class="n">filenames</span> <span class="o">=</span> <span class="n">listdir_no_hidden</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">dir_name</span><span class="p">)</span>
<span class="n">num_files</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)):</span>
    <span class="n">file_exists</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">assert</span> <span class="n">file_exists</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Directory:&#39;</span><span class="p">,</span><span class="n">dir_name</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> files found&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">))</span>

<span class="c1"># Read data for positive movie reviews</span>
<span class="c1"># Data will be stored in a list of lists where the each list </span>
<span class="c1"># represents a document and document is a list of words.</span>
<span class="c1"># We then break the text into words.</span>

<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>

  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">text_parse</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># The Penn Treebank</span>

  <span class="k">return</span> <span class="n">data</span>

<span class="n">positive_documents</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Processing document files under&#39;</span><span class="p">,</span> <span class="n">dir_name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_files</span><span class="p">):</span>
    <span class="c1">## print(&#39; &#39;, filenames[i])</span>

    <span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">positive_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="c1"># print(&#39;Data size (Characters) (Document %d) %d&#39; %(i,len(words)))</span>
    <span class="c1"># print(&#39;Sample string (Document %d) %s&#39;%(i,words[:50]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Directory: movie-reviews-positive
500 files found

Processing document files under movie-reviews-positive
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------</span>
<span class="c1"># convert positive/negative documents into numpy array</span>
<span class="c1"># note that reviews vary from 22 to 1052 words</span>
<span class="c1"># so we use the first 20 and last 20 words of each review</span>
<span class="c1"># as our word sequences for analysis</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">max_review_length</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max_review_length:&quot;</span><span class="p">,</span> <span class="n">max_review_length</span><span class="p">)</span>

<span class="n">min_review_length</span> <span class="o">=</span> <span class="n">max_review_length</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;min_review_length:&quot;</span><span class="p">,</span> <span class="n">min_review_length</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>max_review_length: 1052
min_review_length: 22
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># construct list of 1000 lists with 40 words in each list</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">chain</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>
    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create list of lists of lists for embeddings</span>
<span class="n">embeddings_50</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="n">embedding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding_50</span><span class="p">[</span><span class="n">limited_word_to_index_50</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span>
    <span class="n">embeddings_50</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create list of lists of lists for embeddings</span>
<span class="n">embeddings_100</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="n">embedding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding_100</span><span class="p">[</span><span class="n">limited_word_to_index_100</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span>
    <span class="n">embeddings_100</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------</span>
<span class="c1"># Check on the embeddings list of list of lists</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="c1"># Show the first word in the first document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First word in first document:&quot;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Embedding for this word:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">limited_index_to_embedding_50</span><span class="p">[</span><span class="n">limited_word_to_index_50</span><span class="p">[</span><span class="n">test_word</span><span class="p">]],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">embeddings_50</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:],</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>First word in first document: while
Embedding for this word:
 [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815
 -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271
  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716
  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573
  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699
 -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352
  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538
 -0.39141 ]
Corresponding embedding from embeddings list of list of lists
 [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815
 -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271
  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716
  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573
  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699
 -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352
  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538
 -0.39141 ]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------</span>
<span class="c1"># Make embeddings a numpy array for use in an RNN</span>
<span class="c1"># Create training and test sets with Scikit Learn</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">embeddings_array_50</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings_50</span><span class="p">)</span>

<span class="c1"># Define the labels to be used 500 negative (0) and 500 positive (1)</span>
<span class="n">thumbs_down_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddings_array_100</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings_100</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Review the shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embeddings_array_50</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">thumbs_down_up</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(1000, 40, 50)
(1000, 40, 100)
(1000,)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Modeling">Modeling<a class="anchor-link" href="#Modeling">&#182;</a></h3><h4 id="Train-Test-Split">Train Test Split<a class="anchor-link" href="#Train-Test-Split">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Scikit Learn for random splitting of the data</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="c1"># Random splitting of the data in to training (80%) and test (20%)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">embeddings_array_50</span><span class="p">,</span> <span class="n">thumbs_down_up</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">RANDOM_SEED</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Random splitting of the data in to training (80%) and test (20%)</span>
<span class="n">X_train_100</span><span class="p">,</span> <span class="n">X_test_100</span><span class="p">,</span> <span class="n">y_train_100</span><span class="p">,</span> <span class="n">y_test_100</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">embeddings_array_100</span><span class="p">,</span> <span class="n">thumbs_down_up</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">RANDOM_SEED</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_100</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(800, 40, 50)
(800, 40, 100)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-1:">Model 1:<a class="anchor-link" href="#Model-1:">&#182;</a></h4><ul>
<li>20 Neurons</li>
<li>50 dimensions of pre-trained embeddings</li>
<li>50 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build Model</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array_50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array_50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 50&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  ---- Epoch &quot;</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot; ----&quot;</span><span class="p">)</span>
        <span class="n">epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;End of Training&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From &lt;ipython-input-31-9399a2541457&gt;:14: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From &lt;ipython-input-31-9399a2541457&gt;:15: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /Users/dannyarenson/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:456: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From /Users/dannyarenson/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:460: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From &lt;ipython-input-31-9399a2541457&gt;:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /Users/dannyarenson/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
Start of Training: Embeddings = 50

  ---- Epoch  1  ----
Train accuracy: 0.5 Test accuracy: 0.51

  ---- Epoch  2  ----
Train accuracy: 0.5 Test accuracy: 0.445

  ---- Epoch  3  ----
Train accuracy: 0.5 Test accuracy: 0.495

  ---- Epoch  4  ----
Train accuracy: 0.53 Test accuracy: 0.5

  ---- Epoch  5  ----
Train accuracy: 0.56 Test accuracy: 0.495

  ---- Epoch  6  ----
Train accuracy: 0.56 Test accuracy: 0.515

  ---- Epoch  7  ----
Train accuracy: 0.59 Test accuracy: 0.53

  ---- Epoch  8  ----
Train accuracy: 0.61 Test accuracy: 0.55

  ---- Epoch  9  ----
Train accuracy: 0.65 Test accuracy: 0.575

  ---- Epoch  10  ----
Train accuracy: 0.68 Test accuracy: 0.585

  ---- Epoch  11  ----
Train accuracy: 0.69 Test accuracy: 0.605

  ---- Epoch  12  ----
Train accuracy: 0.7 Test accuracy: 0.64

  ---- Epoch  13  ----
Train accuracy: 0.74 Test accuracy: 0.65

  ---- Epoch  14  ----
Train accuracy: 0.74 Test accuracy: 0.655

  ---- Epoch  15  ----
Train accuracy: 0.74 Test accuracy: 0.67

  ---- Epoch  16  ----
Train accuracy: 0.76 Test accuracy: 0.66

  ---- Epoch  17  ----
Train accuracy: 0.78 Test accuracy: 0.655

  ---- Epoch  18  ----
Train accuracy: 0.79 Test accuracy: 0.65

  ---- Epoch  19  ----
Train accuracy: 0.78 Test accuracy: 0.63

  ---- Epoch  20  ----
Train accuracy: 0.79 Test accuracy: 0.62

  ---- Epoch  21  ----
Train accuracy: 0.79 Test accuracy: 0.6

  ---- Epoch  22  ----
Train accuracy: 0.8 Test accuracy: 0.625

  ---- Epoch  23  ----
Train accuracy: 0.81 Test accuracy: 0.635

  ---- Epoch  24  ----
Train accuracy: 0.8 Test accuracy: 0.66

  ---- Epoch  25  ----
Train accuracy: 0.8 Test accuracy: 0.66

  ---- Epoch  26  ----
Train accuracy: 0.81 Test accuracy: 0.655

  ---- Epoch  27  ----
Train accuracy: 0.8 Test accuracy: 0.665

  ---- Epoch  28  ----
Train accuracy: 0.82 Test accuracy: 0.66

  ---- Epoch  29  ----
Train accuracy: 0.83 Test accuracy: 0.66

  ---- Epoch  30  ----
Train accuracy: 0.83 Test accuracy: 0.655

  ---- Epoch  31  ----
Train accuracy: 0.83 Test accuracy: 0.66

  ---- Epoch  32  ----
Train accuracy: 0.83 Test accuracy: 0.66

  ---- Epoch  33  ----
Train accuracy: 0.85 Test accuracy: 0.65

  ---- Epoch  34  ----
Train accuracy: 0.85 Test accuracy: 0.65

  ---- Epoch  35  ----
Train accuracy: 0.85 Test accuracy: 0.655

  ---- Epoch  36  ----
Train accuracy: 0.85 Test accuracy: 0.66

  ---- Epoch  37  ----
Train accuracy: 0.86 Test accuracy: 0.665

  ---- Epoch  38  ----
Train accuracy: 0.86 Test accuracy: 0.66

  ---- Epoch  39  ----
Train accuracy: 0.86 Test accuracy: 0.665

  ---- Epoch  40  ----
Train accuracy: 0.85 Test accuracy: 0.66

  ---- Epoch  41  ----
Train accuracy: 0.86 Test accuracy: 0.66

  ---- Epoch  42  ----
Train accuracy: 0.86 Test accuracy: 0.665

  ---- Epoch  43  ----
Train accuracy: 0.86 Test accuracy: 0.655

  ---- Epoch  44  ----
Train accuracy: 0.86 Test accuracy: 0.65

  ---- Epoch  45  ----
Train accuracy: 0.86 Test accuracy: 0.65

  ---- Epoch  46  ----
Train accuracy: 0.87 Test accuracy: 0.65

  ---- Epoch  47  ----
Train accuracy: 0.88 Test accuracy: 0.645

  ---- Epoch  48  ----
Train accuracy: 0.87 Test accuracy: 0.67

  ---- Epoch  49  ----
Train accuracy: 0.86 Test accuracy: 0.675

  ---- Epoch  50  ----
Train accuracy: 0.86 Test accuracy: 0.675
End of Training
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-2:">Model 2:<a class="anchor-link" href="#Model-2:">&#182;</a></h4><ul>
<li>30 Neurons</li>
<li>50 dimensions of pre-trained embeddings</li>
<li>25 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build the model</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array_50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array_50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 50&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  ---- Epoch &quot;</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot; ----&quot;</span><span class="p">)</span>
        <span class="n">epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;End of Training&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Start of Training: Embeddings = 50

  ---- Epoch  1  ----
Train accuracy: 0.49 Test accuracy: 0.455

  ---- Epoch  2  ----
Train accuracy: 0.49 Test accuracy: 0.455

  ---- Epoch  3  ----
Train accuracy: 0.53 Test accuracy: 0.5

  ---- Epoch  4  ----
Train accuracy: 0.48 Test accuracy: 0.54

  ---- Epoch  5  ----
Train accuracy: 0.51 Test accuracy: 0.53

  ---- Epoch  6  ----
Train accuracy: 0.57 Test accuracy: 0.525

  ---- Epoch  7  ----
Train accuracy: 0.59 Test accuracy: 0.545

  ---- Epoch  8  ----
Train accuracy: 0.59 Test accuracy: 0.565

  ---- Epoch  9  ----
Train accuracy: 0.61 Test accuracy: 0.565

  ---- Epoch  10  ----
Train accuracy: 0.6 Test accuracy: 0.56

  ---- Epoch  11  ----
Train accuracy: 0.62 Test accuracy: 0.56

  ---- Epoch  12  ----
Train accuracy: 0.63 Test accuracy: 0.565

  ---- Epoch  13  ----
Train accuracy: 0.65 Test accuracy: 0.57

  ---- Epoch  14  ----
Train accuracy: 0.67 Test accuracy: 0.58

  ---- Epoch  15  ----
Train accuracy: 0.68 Test accuracy: 0.57

  ---- Epoch  16  ----
Train accuracy: 0.69 Test accuracy: 0.565

  ---- Epoch  17  ----
Train accuracy: 0.7 Test accuracy: 0.565

  ---- Epoch  18  ----
Train accuracy: 0.72 Test accuracy: 0.565

  ---- Epoch  19  ----
Train accuracy: 0.71 Test accuracy: 0.585

  ---- Epoch  20  ----
Train accuracy: 0.73 Test accuracy: 0.58

  ---- Epoch  21  ----
Train accuracy: 0.75 Test accuracy: 0.585

  ---- Epoch  22  ----
Train accuracy: 0.76 Test accuracy: 0.6

  ---- Epoch  23  ----
Train accuracy: 0.77 Test accuracy: 0.635

  ---- Epoch  24  ----
Train accuracy: 0.8 Test accuracy: 0.615

  ---- Epoch  25  ----
Train accuracy: 0.82 Test accuracy: 0.63
End of Training
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-3:">Model 3:<a class="anchor-link" href="#Model-3:">&#182;</a></h4><ul>
<li>20 neurons</li>
<li>100 dimensions of pre-trained embeddings</li>
<li>50 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build the model</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 100&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  ---- Epoch &quot;</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot; ----&quot;</span><span class="p">)</span>
        <span class="n">epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test_100</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test_100</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;End of Training&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Start of Training: Embeddings = 100

  ---- Epoch  1  ----
Train accuracy: 0.46 Test accuracy: 0.52

  ---- Epoch  2  ----
Train accuracy: 0.5 Test accuracy: 0.565

  ---- Epoch  3  ----
Train accuracy: 0.53 Test accuracy: 0.55

  ---- Epoch  4  ----
Train accuracy: 0.58 Test accuracy: 0.57

  ---- Epoch  5  ----
Train accuracy: 0.59 Test accuracy: 0.58

  ---- Epoch  6  ----
Train accuracy: 0.6 Test accuracy: 0.55

  ---- Epoch  7  ----
Train accuracy: 0.61 Test accuracy: 0.565

  ---- Epoch  8  ----
Train accuracy: 0.62 Test accuracy: 0.58

  ---- Epoch  9  ----
Train accuracy: 0.63 Test accuracy: 0.595

  ---- Epoch  10  ----
Train accuracy: 0.65 Test accuracy: 0.585

  ---- Epoch  11  ----
Train accuracy: 0.64 Test accuracy: 0.59

  ---- Epoch  12  ----
Train accuracy: 0.65 Test accuracy: 0.595

  ---- Epoch  13  ----
Train accuracy: 0.67 Test accuracy: 0.6

  ---- Epoch  14  ----
Train accuracy: 0.67 Test accuracy: 0.6

  ---- Epoch  15  ----
Train accuracy: 0.69 Test accuracy: 0.6

  ---- Epoch  16  ----
Train accuracy: 0.72 Test accuracy: 0.6

  ---- Epoch  17  ----
Train accuracy: 0.73 Test accuracy: 0.605

  ---- Epoch  18  ----
Train accuracy: 0.72 Test accuracy: 0.62

  ---- Epoch  19  ----
Train accuracy: 0.74 Test accuracy: 0.62

  ---- Epoch  20  ----
Train accuracy: 0.77 Test accuracy: 0.63

  ---- Epoch  21  ----
Train accuracy: 0.78 Test accuracy: 0.635

  ---- Epoch  22  ----
Train accuracy: 0.78 Test accuracy: 0.645

  ---- Epoch  23  ----
Train accuracy: 0.79 Test accuracy: 0.65

  ---- Epoch  24  ----
Train accuracy: 0.8 Test accuracy: 0.655

  ---- Epoch  25  ----
Train accuracy: 0.8 Test accuracy: 0.66

  ---- Epoch  26  ----
Train accuracy: 0.81 Test accuracy: 0.675

  ---- Epoch  27  ----
Train accuracy: 0.81 Test accuracy: 0.675

  ---- Epoch  28  ----
Train accuracy: 0.81 Test accuracy: 0.665

  ---- Epoch  29  ----
Train accuracy: 0.82 Test accuracy: 0.675

  ---- Epoch  30  ----
Train accuracy: 0.83 Test accuracy: 0.675

  ---- Epoch  31  ----
Train accuracy: 0.84 Test accuracy: 0.675

  ---- Epoch  32  ----
Train accuracy: 0.86 Test accuracy: 0.68

  ---- Epoch  33  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  34  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  35  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  36  ----
Train accuracy: 0.89 Test accuracy: 0.665

  ---- Epoch  37  ----
Train accuracy: 0.88 Test accuracy: 0.67

  ---- Epoch  38  ----
Train accuracy: 0.88 Test accuracy: 0.685

  ---- Epoch  39  ----
Train accuracy: 0.88 Test accuracy: 0.685

  ---- Epoch  40  ----
Train accuracy: 0.89 Test accuracy: 0.695

  ---- Epoch  41  ----
Train accuracy: 0.89 Test accuracy: 0.69

  ---- Epoch  42  ----
Train accuracy: 0.91 Test accuracy: 0.68

  ---- Epoch  43  ----
Train accuracy: 0.92 Test accuracy: 0.685

  ---- Epoch  44  ----
Train accuracy: 0.91 Test accuracy: 0.685

  ---- Epoch  45  ----
Train accuracy: 0.91 Test accuracy: 0.685

  ---- Epoch  46  ----
Train accuracy: 0.92 Test accuracy: 0.68

  ---- Epoch  47  ----
Train accuracy: 0.92 Test accuracy: 0.67

  ---- Epoch  48  ----
Train accuracy: 0.92 Test accuracy: 0.67

  ---- Epoch  49  ----
Train accuracy: 0.92 Test accuracy: 0.67

  ---- Epoch  50  ----
Train accuracy: 0.94 Test accuracy: 0.665
End of Training
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-4:">Model 4:<a class="anchor-link" href="#Model-4:">&#182;</a></h4><ul>
<li>30 Neurons</li>
<li>100 dimensions of pre-trained embeddings</li>
<li>25 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Build the model</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 100&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  ---- Epoch &quot;</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot; ----&quot;</span><span class="p">)</span>
        <span class="n">epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test_100</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test_100</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;End of Training&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Start of Training: Embeddings = 100

  ---- Epoch  1  ----
Train accuracy: 0.56 Test accuracy: 0.54

  ---- Epoch  2  ----
Train accuracy: 0.57 Test accuracy: 0.56

  ---- Epoch  3  ----
Train accuracy: 0.59 Test accuracy: 0.555

  ---- Epoch  4  ----
Train accuracy: 0.59 Test accuracy: 0.595

  ---- Epoch  5  ----
Train accuracy: 0.62 Test accuracy: 0.615

  ---- Epoch  6  ----
Train accuracy: 0.65 Test accuracy: 0.62

  ---- Epoch  7  ----
Train accuracy: 0.69 Test accuracy: 0.605

  ---- Epoch  8  ----
Train accuracy: 0.69 Test accuracy: 0.605

  ---- Epoch  9  ----
Train accuracy: 0.72 Test accuracy: 0.605

  ---- Epoch  10  ----
Train accuracy: 0.76 Test accuracy: 0.61

  ---- Epoch  11  ----
Train accuracy: 0.76 Test accuracy: 0.625

  ---- Epoch  12  ----
Train accuracy: 0.77 Test accuracy: 0.62

  ---- Epoch  13  ----
Train accuracy: 0.79 Test accuracy: 0.64

  ---- Epoch  14  ----
Train accuracy: 0.76 Test accuracy: 0.64

  ---- Epoch  15  ----
Train accuracy: 0.79 Test accuracy: 0.63

  ---- Epoch  16  ----
Train accuracy: 0.81 Test accuracy: 0.635

  ---- Epoch  17  ----
Train accuracy: 0.82 Test accuracy: 0.625

  ---- Epoch  18  ----
Train accuracy: 0.77 Test accuracy: 0.66

  ---- Epoch  19  ----
Train accuracy: 0.81 Test accuracy: 0.67

  ---- Epoch  20  ----
Train accuracy: 0.84 Test accuracy: 0.66

  ---- Epoch  21  ----
Train accuracy: 0.87 Test accuracy: 0.665

  ---- Epoch  22  ----
Train accuracy: 0.83 Test accuracy: 0.665

  ---- Epoch  23  ----
Train accuracy: 0.83 Test accuracy: 0.685

  ---- Epoch  24  ----
Train accuracy: 0.83 Test accuracy: 0.685

  ---- Epoch  25  ----
Train accuracy: 0.86 Test accuracy: 0.66
End of Training
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-5:">Model 5:<a class="anchor-link" href="#Model-5:">&#182;</a></h4><ul>
<li>30 Neurons</li>
<li>100 dimensions of pre-trained embeddings</li>
<li>40 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build the model</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array_100</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 50&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  ---- Epoch &quot;</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot; ----&quot;</span><span class="p">)</span>
        <span class="n">epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train_100</span><span class="p">[</span><span class="n">iteration</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test_100</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test_100</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;End of Training&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Start of Training: Embeddings = 50

  ---- Epoch  1  ----
Train accuracy: 0.46 Test accuracy: 0.52

  ---- Epoch  2  ----
Train accuracy: 0.5 Test accuracy: 0.565

  ---- Epoch  3  ----
Train accuracy: 0.53 Test accuracy: 0.55

  ---- Epoch  4  ----
Train accuracy: 0.58 Test accuracy: 0.57

  ---- Epoch  5  ----
Train accuracy: 0.59 Test accuracy: 0.58

  ---- Epoch  6  ----
Train accuracy: 0.6 Test accuracy: 0.55

  ---- Epoch  7  ----
Train accuracy: 0.61 Test accuracy: 0.565

  ---- Epoch  8  ----
Train accuracy: 0.62 Test accuracy: 0.58

  ---- Epoch  9  ----
Train accuracy: 0.63 Test accuracy: 0.595

  ---- Epoch  10  ----
Train accuracy: 0.65 Test accuracy: 0.585

  ---- Epoch  11  ----
Train accuracy: 0.64 Test accuracy: 0.59

  ---- Epoch  12  ----
Train accuracy: 0.65 Test accuracy: 0.595

  ---- Epoch  13  ----
Train accuracy: 0.67 Test accuracy: 0.6

  ---- Epoch  14  ----
Train accuracy: 0.67 Test accuracy: 0.6

  ---- Epoch  15  ----
Train accuracy: 0.69 Test accuracy: 0.6

  ---- Epoch  16  ----
Train accuracy: 0.72 Test accuracy: 0.6

  ---- Epoch  17  ----
Train accuracy: 0.73 Test accuracy: 0.605

  ---- Epoch  18  ----
Train accuracy: 0.72 Test accuracy: 0.62

  ---- Epoch  19  ----
Train accuracy: 0.74 Test accuracy: 0.62

  ---- Epoch  20  ----
Train accuracy: 0.77 Test accuracy: 0.63

  ---- Epoch  21  ----
Train accuracy: 0.78 Test accuracy: 0.635

  ---- Epoch  22  ----
Train accuracy: 0.78 Test accuracy: 0.645

  ---- Epoch  23  ----
Train accuracy: 0.79 Test accuracy: 0.65

  ---- Epoch  24  ----
Train accuracy: 0.8 Test accuracy: 0.655

  ---- Epoch  25  ----
Train accuracy: 0.8 Test accuracy: 0.66

  ---- Epoch  26  ----
Train accuracy: 0.81 Test accuracy: 0.675

  ---- Epoch  27  ----
Train accuracy: 0.81 Test accuracy: 0.675

  ---- Epoch  28  ----
Train accuracy: 0.81 Test accuracy: 0.665

  ---- Epoch  29  ----
Train accuracy: 0.82 Test accuracy: 0.675

  ---- Epoch  30  ----
Train accuracy: 0.83 Test accuracy: 0.675

  ---- Epoch  31  ----
Train accuracy: 0.84 Test accuracy: 0.675

  ---- Epoch  32  ----
Train accuracy: 0.86 Test accuracy: 0.68

  ---- Epoch  33  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  34  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  35  ----
Train accuracy: 0.88 Test accuracy: 0.675

  ---- Epoch  36  ----
Train accuracy: 0.89 Test accuracy: 0.665

  ---- Epoch  37  ----
Train accuracy: 0.88 Test accuracy: 0.67

  ---- Epoch  38  ----
Train accuracy: 0.88 Test accuracy: 0.685

  ---- Epoch  39  ----
Train accuracy: 0.88 Test accuracy: 0.685

  ---- Epoch  40  ----
Train accuracy: 0.89 Test accuracy: 0.695
End of Training
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">default_factory</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">EVOCABSIZE</span>  <span class="c1"># last/unknown-word row in limited_index_to_embedding</span>
<span class="c1"># dictionary has the items() function, returns list of (key, value) tuples</span>
<span class="n">limited_word_to_index</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">default_factory</span><span class="p">,</span> \
    <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">EVOCABSIZE</span><span class="p">})</span>

<span class="c1"># Select the first EVOCABSIZE rows to the index_to_embedding</span>
<span class="n">limited_index_to_embedding</span> <span class="o">=</span> <span class="n">index_to_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">EVOCABSIZE</span><span class="p">,:]</span>
<span class="c1"># Set the unknown-word row to be all zeros as previously</span>
<span class="n">limited_index_to_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding</span><span class="p">,</span> 
    <span class="n">index_to_embedding</span><span class="p">[</span><span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span>\
        <span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">),</span> 
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Delete large numpy array to clear some CPU RAM</span>
<span class="k">del</span> <span class="n">index_to_embedding</span>

<span class="c1"># Verify the new vocabulary: should get same embeddings for test sentence</span>
<span class="c1"># Note that a small EVOCABSIZE may yield some zero vectors for embeddings</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test sentence embeddings from vocabulary of&#39;</span><span class="p">,</span> <span class="n">EVOCABSIZE</span><span class="p">,</span> <span class="s1">&#39;words:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_in_test_sentence</span><span class="p">:</span>
    <span class="n">word_</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">word_</span><span class="p">]]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word_</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Test sentence embeddings from vocabulary of 10000 words:

the:  [ 9.5152e-02  3.7024e-01  5.4291e-01  1.9621e-01  4.8205e-02  3.2033e-01
 -5.9638e-01  1.5868e-02 -1.2989e-01 -6.3028e-01  8.1944e-02  2.4164e-01
 -6.0990e+00 -6.8557e-01  5.0354e-01 -3.4089e-02  1.1705e-01 -7.7403e-03
 -8.6512e-02  4.3617e-01 -4.3982e-01  2.6125e-01 -4.0348e-02 -1.9194e-01
  8.3204e-02 -5.8246e-01 -3.1923e-02  1.2630e-01  4.0120e-01  6.8906e-02
 -1.0517e-01 -2.0804e-01 -4.2554e-01  4.7799e-01  3.4651e-01  2.4057e-01
  5.0244e-02 -7.2587e-02 -2.4347e-03 -5.0342e-01 -1.0601e+00 -3.1586e-01
 -3.2457e-02 -7.6317e-02  7.9045e-01  8.6367e-02 -1.9632e-01  5.7566e-02
  8.4129e-01 -4.2020e-01 -1.1335e-03 -8.5632e-02  6.1910e-02  2.1423e-01
 -1.0356e-01 -3.6946e-02 -2.6005e-01 -3.5657e-01  5.4321e-02  3.0875e-02
  1.4092e-01 -9.1998e-02 -4.1841e-01 -3.1135e-01 -1.4937e-01 -2.2699e-04
 -3.3454e-01 -1.4848e-01 -1.1944e-01 -2.7174e-01  3.1320e-01 -1.0998e-01
 -4.7524e-01  1.4056e-01  3.9641e-01 -4.9413e-02 -4.2601e-01 -2.3576e-01
  6.1482e-02 -3.5313e-02  2.4161e+00  2.8979e-01  3.8882e-01  3.6779e-01
  2.0685e-01  1.3992e-01 -4.2459e-01  4.4590e-01  2.6234e-01 -4.4834e-01
  3.7196e-03 -2.2521e-01  1.4764e-01 -3.6417e-01 -1.8493e-01  2.2282e-01
  4.7626e-01 -5.1083e-01  4.6877e-01  3.4882e-01]
quick:  [ 0.50111    0.37708   -0.19973   -0.55111    0.17148    0.019936
  0.50052    0.017863  -0.43901    0.4485    -0.22766   -0.087691
 -3.5079    -0.62763   -0.75083   -0.19767   -0.39356    0.3996
 -0.081026  -0.53157   -0.38539   -0.61069    0.10148   -0.10846
 -0.29013    0.61234    0.027151  -0.044352  -0.40846    0.42045
 -0.22149    0.018245  -0.25989   -0.049784   0.28018    0.26186
 -0.22841   -0.28096    0.046061   0.26917   -0.41851    0.25948
  0.10509    0.75517    0.43909    0.07024    0.053149   0.59465
 -0.23239    0.37033   -0.29459   -0.040892  -0.37618    0.015432
  0.056196  -0.25702   -0.16717    0.2405     0.29895   -0.64143
  0.91313   -0.057541   0.20291    1.0468     0.65415   -0.94901
  0.49342    0.014261   0.14139    0.17338   -0.76048    0.53518
  0.26007    0.34376    0.057837  -0.55036    0.66677   -0.31764
  0.41491   -0.025773   1.5507     0.394     -0.31088   -0.53684
  0.15205    0.70041   -0.1879    -0.24963   -0.16778   -0.34475
 -0.51597    0.010533  -0.59016   -0.44993    0.80113    0.051259
 -0.49647    0.59636    0.0075998  0.28048  ]
brown:  [-0.26106   -0.75489   -0.022668   0.055802  -0.77145    0.05871
  0.3852     0.40926   -0.97445   -0.33838    0.47742   -0.01054
 -3.1085    -0.55482    0.35536    0.44814    0.29137    0.16997
  0.66486    0.22324    0.32805   -0.40968   -0.19862    0.3546
  0.30566   -0.55413   -0.54773    0.25429   -0.72556   -0.22337
  0.16802    0.14168   -1.0443    -0.57601   -0.21027    0.18212
 -0.81012   -0.71126   -0.39691   -0.13592   -0.37764   -0.52612
 -0.80185    0.31638   -0.073107  -0.74961    0.44858   -0.0039955
 -0.22895   -0.95689   -0.70048   -0.15495    0.30279    0.51368
 -0.51663    0.053121  -0.23784    0.49018    0.47278    0.29428
 -0.42305    0.39041   -0.051611  -0.30997    0.12854   -0.67797
 -0.23172    0.13328    0.43269   -0.28219    0.56389   -0.52302
  0.52544    0.20713   -0.4926     0.2071    -0.012374   0.62647
  0.38548    0.5472     1.5739     0.38571   -0.095062  -0.70715
 -0.37873   -0.065873   0.34776    0.80396   -0.34771    0.43994
 -0.23445   -0.36284   -0.11516   -0.68272   -0.027322   0.24447
 -0.088484   0.34491   -0.55879    0.343    ]
fox:  [ 0.64344    0.0086088  0.50145   -0.70381   -0.36289   -0.51602
  0.3751    -0.0078184  0.10752   -0.29124    0.61808   -0.036332
 -2.4467    -0.0050135  0.18236   -0.18152   -0.19349   -0.19442
  0.3793     0.46691    0.03579   -0.48468   -0.45103   -0.045509
  0.6732    -1.4904    -0.23975   -0.26736   -0.058426   0.11573
  0.79477    0.09746   -0.36717   -0.20758    0.099006  -0.51114
 -0.023912   0.14275   -0.87894    0.13728   -0.26524   -0.33326
  0.25857   -0.27703    0.5022     0.7164    -0.26708    0.018559
  0.39153   -0.42015   -0.55746   -0.2797    -0.36874    0.090716
 -0.29017    0.25543   -0.016203   0.014775  -0.45174   -0.48211
 -0.18746    0.59934   -0.20146   -0.3756    -0.11143    0.26213
  0.15496    0.53471    0.43618   -0.7356     0.34366   -0.036715
 -0.2377    -0.3525    -0.5546     0.44059   -0.17759    0.50194
 -0.59675   -0.0427     1.5432     0.22326    0.40868    0.70572
 -0.17751    0.071547   0.84483    0.3794    -0.67034   -0.54685
 -0.55382   -0.88651   -0.25728   -0.1996    -0.15984    0.37977
  0.62406    0.037116  -0.427      0.029686 ]
jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0.]
over:  [-1.3037e-01  2.0490e-01  4.2575e-01 -3.1239e-01 -5.4739e-01  2.1011e-01
 -7.2276e-03 -6.3219e-02 -1.2984e-02 -8.2143e-02  2.5385e-01  3.2791e-01
 -4.9173e+00  3.1567e-01 -2.0232e-01 -2.5671e-01 -1.8498e-03  4.3715e-01
 -1.0066e+00  2.5198e-02 -3.9015e-02 -3.4754e-01 -2.8745e-02  6.5716e-01
  1.0906e+00  2.3102e-01  5.5719e-01 -4.6840e-01 -5.8515e-01 -2.9006e-01
 -2.6508e-01  3.9253e-01 -5.1165e-01  2.4492e-02  8.1263e-01 -4.2014e-01
 -3.4857e-01  3.5984e-01  1.5941e-01 -6.9736e-01 -1.4426e+00 -9.9337e-03
 -2.3335e-01 -4.6266e-01  2.6243e-01 -2.9373e-01  4.8860e-01  7.2830e-01
 -3.2475e-02  6.2540e-01 -4.3399e-01 -1.0553e-01  3.1752e-01 -1.5631e-01
 -2.4268e-01 -3.9298e-01 -3.7478e-01 -6.6699e-02  1.5477e-01  7.4870e-01
 -2.3318e-01  9.7446e-02 -4.4590e-01 -6.1845e-02  1.7504e-01  7.3357e-01
  8.8520e-01 -1.9843e-01  2.5146e-01 -3.8909e-01 -3.0322e-01  4.3190e-01
  5.9478e-02 -2.7233e-01 -3.8758e-01  5.1850e-01 -1.6175e-01 -7.5551e-01
  5.5890e-01  1.0797e-01  1.4943e+00  1.6329e-01  6.6365e-01  1.2885e-01
 -9.8670e-02 -4.8738e-02  1.3253e-01 -1.6620e-01 -4.2653e-01 -1.7694e-01
 -2.6400e-01  1.0666e-01 -1.9857e-02  1.2652e-01  1.5045e-01 -7.6070e-02
 -3.4198e-01 -1.4165e-01  4.8806e-01  5.2860e-01]
the:  [ 9.5152e-02  3.7024e-01  5.4291e-01  1.9621e-01  4.8205e-02  3.2033e-01
 -5.9638e-01  1.5868e-02 -1.2989e-01 -6.3028e-01  8.1944e-02  2.4164e-01
 -6.0990e+00 -6.8557e-01  5.0354e-01 -3.4089e-02  1.1705e-01 -7.7403e-03
 -8.6512e-02  4.3617e-01 -4.3982e-01  2.6125e-01 -4.0348e-02 -1.9194e-01
  8.3204e-02 -5.8246e-01 -3.1923e-02  1.2630e-01  4.0120e-01  6.8906e-02
 -1.0517e-01 -2.0804e-01 -4.2554e-01  4.7799e-01  3.4651e-01  2.4057e-01
  5.0244e-02 -7.2587e-02 -2.4347e-03 -5.0342e-01 -1.0601e+00 -3.1586e-01
 -3.2457e-02 -7.6317e-02  7.9045e-01  8.6367e-02 -1.9632e-01  5.7566e-02
  8.4129e-01 -4.2020e-01 -1.1335e-03 -8.5632e-02  6.1910e-02  2.1423e-01
 -1.0356e-01 -3.6946e-02 -2.6005e-01 -3.5657e-01  5.4321e-02  3.0875e-02
  1.4092e-01 -9.1998e-02 -4.1841e-01 -3.1135e-01 -1.4937e-01 -2.2699e-04
 -3.3454e-01 -1.4848e-01 -1.1944e-01 -2.7174e-01  3.1320e-01 -1.0998e-01
 -4.7524e-01  1.4056e-01  3.9641e-01 -4.9413e-02 -4.2601e-01 -2.3576e-01
  6.1482e-02 -3.5313e-02  2.4161e+00  2.8979e-01  3.8882e-01  3.6779e-01
  2.0685e-01  1.3992e-01 -4.2459e-01  4.4590e-01  2.6234e-01 -4.4834e-01
  3.7196e-03 -2.2521e-01  1.4764e-01 -3.6417e-01 -1.8493e-01  2.2282e-01
  4.7626e-01 -5.1083e-01  4.6877e-01  3.4882e-01]
lazy:  [ 1.4021e-01 -6.1686e-01  6.6047e-01  4.5844e-01 -4.7073e-02  5.6833e-01
  4.7711e-01 -3.0135e-01  2.5490e-01  2.7677e-01 -7.2243e-01 -4.7596e-01
 -3.1877e+00 -3.0520e-01 -1.1225e+00  1.1409e-01 -1.6397e-01 -6.2531e-01
 -6.4549e-01 -7.0767e-01 -1.3721e-01  1.6656e-01 -1.5643e-01 -5.8997e-01
  5.3493e-01  4.2989e-01 -1.6078e-01  3.1838e-01 -1.7478e-01 -6.6117e-02
 -9.1278e-02 -2.2732e-01 -6.2848e-01  3.7686e-01 -6.0958e-01  3.7723e-02
  1.3443e-01  5.8768e-01  1.0611e-01  1.0578e+00 -7.9843e-01  1.5644e-02
  5.1333e-01 -2.6829e-01  8.6280e-02 -4.8820e-01 -7.8925e-02  5.7910e-01
 -8.3873e-01  7.4992e-01 -4.7451e-01  5.3792e-01  2.5934e-01 -2.5577e-01
 -7.2746e-01  7.2324e-01 -3.5029e-01  2.3883e-01  2.2178e-01  2.3307e-01
 -2.4567e-01  2.3833e-01  6.6281e-01 -1.1956e-01 -2.3183e-02 -7.2004e-01
 -4.5729e-02  6.8426e-01  3.5203e-01  5.6147e-01 -6.6437e-01  4.0224e-01
 -3.9397e-01 -1.1179e-01  1.5747e-01 -1.4167e-03  1.0760e+00  6.7952e-01
 -3.5587e-01 -7.7132e-02  2.0712e+00  4.2989e-01 -3.2253e-01  1.9375e-02
  6.2629e-01  3.2018e-01  3.3936e-01 -9.2320e-02  2.8323e-01  1.4915e-01
  2.3714e-01  4.1720e-01 -1.6513e-01  1.8810e-01  7.0461e-01  2.5950e-01
 -1.0690e-01  9.0640e-01  2.2023e-01 -1.9887e-01]
dog:  [ 5.0779e-01 -1.0274e+00  4.8136e-01 -9.4170e-02  4.4837e-01 -5.2291e-01
  5.1498e-01 -3.8927e-02  3.5867e-01 -6.5994e-02 -8.2882e-01  7.6179e-01
 -3.8030e+00 -1.0576e-02  2.1654e-01  5.9712e-01  3.7424e-01 -2.2629e-02
 -1.0331e-02 -3.3966e-01  9.4336e-02  2.6253e-01 -4.0161e-01 -7.9532e-03
  1.0206e+00 -3.5793e-01 -5.6500e-01  5.8815e-01 -8.1847e-01  3.0293e-01
  4.7199e-01 -9.7429e-02 -6.1226e-01 -1.7797e-01 -1.1616e-01  3.2586e-01
  1.1498e-01 -1.9030e-01  1.1591e-02  4.6478e-01 -1.6805e-01  2.1972e-01
 -2.5938e-01 -1.3541e-02  7.0714e-01  7.8106e-01  7.9917e-01  1.0389e+00
  5.2792e-01 -1.1160e-01 -6.2275e-01  3.0692e-02  3.3847e-01 -5.3092e-01
 -9.9688e-02  2.1596e-01  6.0522e-01  1.2356e+00 -3.4528e-03 -9.7514e-02
 -2.4938e-01  2.1539e-01  4.4643e-01  9.5375e-02 -2.7366e-01 -2.8537e-01
 -4.0894e-01  4.8223e-01  3.0318e-01  1.9440e-01  8.3242e-01 -5.0378e-01
  3.0090e-01 -4.9792e-01  5.0297e-01  3.2685e-02 -5.1790e-01 -2.3541e-01
  2.2960e-01 -6.3588e-01  1.6270e+00  6.2832e-01 -7.4846e-01  6.0073e-01
 -1.1215e-02 -3.2113e-01  1.4339e-01 -6.0809e-02  8.8218e-02  6.5936e-01
 -4.6127e-01 -3.7644e-01 -1.1330e-01  1.5875e-01  3.9119e-01  6.7659e-01
 -7.1224e-02  1.7458e-01 -3.3406e-02  7.3152e-01]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------</span>
<span class="c1"># convert positive/negative documents into numpy array</span>
<span class="c1"># note that reviews vary from 22 to 1052 words   </span>
<span class="c1"># so we use the first 20 and last 20 words of each review </span>
<span class="c1"># as our word sequences for analysis</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">max_review_length</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;max_review_length:&#39;</span><span class="p">,</span> <span class="n">max_review_length</span><span class="p">)</span> 

<span class="n">min_review_length</span> <span class="o">=</span> <span class="n">max_review_length</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;min_review_length:&#39;</span><span class="p">,</span> <span class="n">min_review_length</span><span class="p">)</span> 

<span class="c1"># construct list of 1000 lists with 20 words in each list</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">chain</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>    

<span class="c1"># create list of lists of lists for embeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
       <span class="n">embedding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span> 
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
    
<span class="c1">#embeddings</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>max_review_length: 1052
min_review_length: 22
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------    </span>
<span class="c1"># Check on the embeddings list of list of lists </span>
<span class="c1"># -----------------------------------------------------</span>
<span class="c1"># Show the first word in the first document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:])</span>

<span class="c1"># Show the seventh word in the tenth document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">6</span><span class="p">][</span><span class="mi">9</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">6</span><span class="p">][</span><span class="mi">9</span><span class="p">][:])</span>

<span class="c1"># Show the last word in the last document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="mi">39</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="mi">39</span><span class="p">][:])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>First word in first document: while
Embedding for this word:
 [-4.7197e-02 -2.4357e-01  1.0880e-01 -5.6693e-01 -3.8555e-02  1.5236e-01
 -4.4097e-02 -3.5602e-02  2.5351e-01 -6.9209e-01 -5.5410e-04  1.8290e-03
 -5.1479e+00  3.6846e-01 -3.4871e-01 -9.0599e-02 -2.9809e-01 -1.1419e-01
 -8.5266e-01 -1.8206e-01 -7.7734e-01 -1.2525e-02  2.4790e-01 -4.6548e-04
  1.9668e-01  6.5513e-01 -4.8212e-01 -1.7646e-01  2.6732e-01  2.8195e-01
  4.1784e-01  2.3964e-02 -2.9772e-01  3.6287e-01 -7.5949e-03  1.8756e-01
 -8.4115e-02 -1.3346e-01  1.1355e-01  4.3278e-01 -7.8362e-02  1.9060e-01
  3.5403e-01  1.4928e-01  7.2068e-01 -3.5885e-01  1.1589e-01  5.2705e-01
 -4.1823e-01  2.0411e-01 -5.0177e-01 -2.2404e-01  5.5086e-01 -2.2030e-01
 -5.2023e-02  5.7555e-02 -1.8871e-01  3.0119e-02  6.2221e-01  1.0051e-01
  1.5656e-01 -2.9829e-02  2.8033e-01 -4.5078e-01  5.2535e-01 -8.6973e-03
  1.4169e-01  2.4950e-01  2.9821e-01  1.5145e-01 -1.7910e-01  1.4797e-01
  7.3218e-02 -8.1712e-01 -6.1936e-02  1.8336e-01 -1.0639e-01 -2.1006e-01
  1.4606e-01  2.3040e-01  1.2416e+00  8.3053e-02 -4.7140e-01  4.7603e-01
  1.3378e-01 -5.0239e-01  1.3375e-01  1.4129e-01  2.0460e-01  1.4739e-01
  5.0854e-01 -1.6517e-01 -3.5384e-01  2.1834e-02 -5.1504e-01  9.7128e-02
  1.3943e-01 -1.3130e-01  1.1166e-01  3.2966e-02]
Corresponding embedding from embeddings list of list of lists
 [-4.7197e-02 -2.4357e-01  1.0880e-01 -5.6693e-01 -3.8555e-02  1.5236e-01
 -4.4097e-02 -3.5602e-02  2.5351e-01 -6.9209e-01 -5.5410e-04  1.8290e-03
 -5.1479e+00  3.6846e-01 -3.4871e-01 -9.0599e-02 -2.9809e-01 -1.1419e-01
 -8.5266e-01 -1.8206e-01 -7.7734e-01 -1.2525e-02  2.4790e-01 -4.6548e-04
  1.9668e-01  6.5513e-01 -4.8212e-01 -1.7646e-01  2.6732e-01  2.8195e-01
  4.1784e-01  2.3964e-02 -2.9772e-01  3.6287e-01 -7.5949e-03  1.8756e-01
 -8.4115e-02 -1.3346e-01  1.1355e-01  4.3278e-01 -7.8362e-02  1.9060e-01
  3.5403e-01  1.4928e-01  7.2068e-01 -3.5885e-01  1.1589e-01  5.2705e-01
 -4.1823e-01  2.0411e-01 -5.0177e-01 -2.2404e-01  5.5086e-01 -2.2030e-01
 -5.2023e-02  5.7555e-02 -1.8871e-01  3.0119e-02  6.2221e-01  1.0051e-01
  1.5656e-01 -2.9829e-02  2.8033e-01 -4.5078e-01  5.2535e-01 -8.6973e-03
  1.4169e-01  2.4950e-01  2.9821e-01  1.5145e-01 -1.7910e-01  1.4797e-01
  7.3218e-02 -8.1712e-01 -6.1936e-02  1.8336e-01 -1.0639e-01 -2.1006e-01
  1.4606e-01  2.3040e-01  1.2416e+00  8.3053e-02 -4.7140e-01  4.7603e-01
  1.3378e-01 -5.0239e-01  1.3375e-01  1.4129e-01  2.0460e-01  1.4739e-01
  5.0854e-01 -1.6517e-01 -3.5384e-01  2.1834e-02 -5.1504e-01  9.7128e-02
  1.3943e-01 -1.3130e-01  1.1166e-01  3.2966e-02]
First word in first document: officially
Embedding for this word:
 [ 0.1998     0.40867   -0.24593   -0.78369   -0.37458   -0.44166
  0.26885    0.047121  -0.36617   -0.42266   -0.044305   0.13179
 -2.8828     0.40975    0.81898    0.57803   -0.63093    0.74253
  0.0043389 -1.0883     0.19534   -0.27862   -0.049884  -0.39319
 -0.17738    0.74282    0.90957   -0.94145    0.32948   -0.048619
 -0.371      0.23122   -0.36304   -0.88419   -0.057837   0.25015
  0.25152   -0.19786    0.36983    0.037088  -0.18546   -0.27191
 -0.13579   -0.5118     0.050769  -0.69832    0.29462   -0.76752
 -0.47306    0.82282   -0.35439    0.3978     0.5492    -0.33755
 -0.44981    0.22497   -0.36921   -0.09661   -0.36811   -0.16032
 -0.46999    0.073361  -0.31066   -1.2852     0.099859  -0.13739
 -0.089406   0.37154    0.54826   -0.40302    0.40518    0.044081
 -0.34476    0.50937    0.37899    0.10643   -0.17064   -0.15113
 -0.26793   -0.16286    2.0682    -0.092807  -0.14596    1.02
  0.074494  -0.13407   -0.8924     0.37816    0.10594   -0.78997
 -0.39631    0.72183    0.28285    0.035282  -0.060631  -0.41459
 -0.33693    0.53924    0.28935   -1.1945   ]
Corresponding embedding from embeddings list of list of lists
 [ 0.1998     0.40867   -0.24593   -0.78369   -0.37458   -0.44166
  0.26885    0.047121  -0.36617   -0.42266   -0.044305   0.13179
 -2.8828     0.40975    0.81898    0.57803   -0.63093    0.74253
  0.0043389 -1.0883     0.19534   -0.27862   -0.049884  -0.39319
 -0.17738    0.74282    0.90957   -0.94145    0.32948   -0.048619
 -0.371      0.23122   -0.36304   -0.88419   -0.057837   0.25015
  0.25152   -0.19786    0.36983    0.037088  -0.18546   -0.27191
 -0.13579   -0.5118     0.050769  -0.69832    0.29462   -0.76752
 -0.47306    0.82282   -0.35439    0.3978     0.5492    -0.33755
 -0.44981    0.22497   -0.36921   -0.09661   -0.36811   -0.16032
 -0.46999    0.073361  -0.31066   -1.2852     0.099859  -0.13739
 -0.089406   0.37154    0.54826   -0.40302    0.40518    0.044081
 -0.34476    0.50937    0.37899    0.10643   -0.17064   -0.15113
 -0.26793   -0.16286    2.0682    -0.092807  -0.14596    1.02
  0.074494  -0.13407   -0.8924     0.37816    0.10594   -0.78997
 -0.39631    0.72183    0.28285    0.035282  -0.060631  -0.41459
 -0.33693    0.53924    0.28935   -1.1945   ]
First word in first document: super
Embedding for this word:
 [ 5.5345e-01 -1.9958e-01  5.7767e-02 -7.2263e-01  2.3701e-03  3.2270e-01
  2.7307e-01 -3.0110e-02  2.8296e-01 -8.3467e-01 -3.7672e-01  3.0227e-01
 -2.6061e+00 -3.4585e-01  2.7894e-01  2.5327e-02 -5.0592e-02 -3.7881e-01
 -6.8229e-01 -2.9768e-01 -3.3582e-01  4.5297e-01 -6.2998e-01 -3.1890e-02
  1.8998e-01 -1.9589e+00 -1.0523e-01 -1.1150e-01  7.1567e-02 -1.0410e+00
 -4.4984e-01 -7.5302e-01 -3.2122e-01  1.0739e-02  8.2496e-01 -6.2249e-02
 -1.1524e-01 -5.0346e-01 -2.3861e-01  1.6046e-02 -1.5762e+00 -7.2895e-01
 -2.7072e-01  1.2879e+00  1.2085e-01 -1.0910e-01  2.3711e-01  1.0399e-01
 -1.0260e-01  4.3540e-01 -6.0826e-01  5.3611e-01 -2.2289e-01  2.5897e-01
 -9.6692e-02  4.3631e-01  3.7832e-02  3.2776e-01 -1.5046e-01 -7.1888e-01
 -1.9168e-02 -7.9830e-01  3.9109e-01 -4.8538e-01  6.7950e-01  7.0861e-02
 -6.8388e-01 -2.9827e-01 -2.2636e-01 -4.9680e-02 -6.0364e-03 -1.6391e-01
 -9.3996e-01  8.5387e-01 -6.5163e-01  1.4891e-01 -9.4438e-02  2.6607e-01
  6.0155e-01 -9.2130e-01  1.2482e+00 -1.6858e-01  3.4999e-01  4.3099e-01
  3.2709e-01  1.1848e+00  1.4483e-01 -7.9712e-01 -2.9971e-01  1.9729e-01
 -1.2445e-01  2.5371e-01  1.1641e-01 -3.4436e-02  4.3177e-01  6.8567e-01
 -4.5507e-01 -3.3365e-01 -3.2757e-01 -8.6044e-02]
Corresponding embedding from embeddings list of list of lists
 [ 5.5345e-01 -1.9958e-01  5.7767e-02 -7.2263e-01  2.3701e-03  3.2270e-01
  2.7307e-01 -3.0110e-02  2.8296e-01 -8.3467e-01 -3.7672e-01  3.0227e-01
 -2.6061e+00 -3.4585e-01  2.7894e-01  2.5327e-02 -5.0592e-02 -3.7881e-01
 -6.8229e-01 -2.9768e-01 -3.3582e-01  4.5297e-01 -6.2998e-01 -3.1890e-02
  1.8998e-01 -1.9589e+00 -1.0523e-01 -1.1150e-01  7.1567e-02 -1.0410e+00
 -4.4984e-01 -7.5302e-01 -3.2122e-01  1.0739e-02  8.2496e-01 -6.2249e-02
 -1.1524e-01 -5.0346e-01 -2.3861e-01  1.6046e-02 -1.5762e+00 -7.2895e-01
 -2.7072e-01  1.2879e+00  1.2085e-01 -1.0910e-01  2.3711e-01  1.0399e-01
 -1.0260e-01  4.3540e-01 -6.0826e-01  5.3611e-01 -2.2289e-01  2.5897e-01
 -9.6692e-02  4.3631e-01  3.7832e-02  3.2776e-01 -1.5046e-01 -7.1888e-01
 -1.9168e-02 -7.9830e-01  3.9109e-01 -4.8538e-01  6.7950e-01  7.0861e-02
 -6.8388e-01 -2.9827e-01 -2.2636e-01 -4.9680e-02 -6.0364e-03 -1.6391e-01
 -9.3996e-01  8.5387e-01 -6.5163e-01  1.4891e-01 -9.4438e-02  2.6607e-01
  6.0155e-01 -9.2130e-01  1.2482e+00 -1.6858e-01  3.4999e-01  4.3099e-01
  3.2709e-01  1.1848e+00  1.4483e-01 -7.9712e-01 -2.9971e-01  1.9729e-01
 -1.2445e-01  2.5371e-01  1.1641e-01 -3.4436e-02  4.3177e-01  6.8567e-01
 -4.5507e-01 -3.3365e-01 -3.2757e-01 -8.6044e-02]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddings_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="c1"># -----------------------------------------------------    </span>
<span class="c1"># Make embeddings a numpy array for use in an RNN </span>
<span class="c1"># Create training and test sets with Scikit Learn</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">embeddings_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Define the labels to be used 500 negative (0) and 500 positive (1)</span>
<span class="n">thumbs_down_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> 
                      <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Scikit Learn for random splitting of the data  </span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="c1"># Random splitting of the data in to training (80%) and test (20%)  </span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">embeddings_array</span><span class="p">,</span> <span class="n">thumbs_down_up</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> 
                     <span class="n">random_state</span> <span class="o">=</span> <span class="n">RANDOM_SEED</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-6:">Model 6:<a class="anchor-link" href="#Model-6:">&#182;</a></h4><ul>
<li>20 Neurons</li>
<li>100 dimensions of pre-trained embeddings</li>
<li>50 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document </span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">basic_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                          <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start of Training: Embeddings = 100&quot;</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">  ---- Epoch &#39;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39; ----</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>          
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  Batch &#39;</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39; training observations from &#39;</span><span class="p">,</span>  
                  <span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,</span> <span class="s1">&#39; to &#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">  Train accuracy:&#39;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Start of Training: Embeddings = 100

  ---- Epoch  0  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.52 Test accuracy: 0.51

  ---- Epoch  1  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.5 Test accuracy: 0.525

  ---- Epoch  2  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.49 Test accuracy: 0.51

  ---- Epoch  3  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.5 Test accuracy: 0.54

  ---- Epoch  4  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.55 Test accuracy: 0.555

  ---- Epoch  5  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.57 Test accuracy: 0.555

  ---- Epoch  6  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.58 Test accuracy: 0.565

  ---- Epoch  7  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.63 Test accuracy: 0.57

  ---- Epoch  8  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.65 Test accuracy: 0.59

  ---- Epoch  9  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.66 Test accuracy: 0.59

  ---- Epoch  10  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.67 Test accuracy: 0.575

  ---- Epoch  11  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.69 Test accuracy: 0.575

  ---- Epoch  12  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.7 Test accuracy: 0.585

  ---- Epoch  13  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.7 Test accuracy: 0.6

  ---- Epoch  14  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.7 Test accuracy: 0.605

  ---- Epoch  15  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.69 Test accuracy: 0.62

  ---- Epoch  16  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.71 Test accuracy: 0.625

  ---- Epoch  17  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.71 Test accuracy: 0.625

  ---- Epoch  18  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.72 Test accuracy: 0.63

  ---- Epoch  19  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.74 Test accuracy: 0.63

  ---- Epoch  20  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.75 Test accuracy: 0.625

  ---- Epoch  21  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.77 Test accuracy: 0.625

  ---- Epoch  22  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.76 Test accuracy: 0.62

  ---- Epoch  23  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.78 Test accuracy: 0.635

  ---- Epoch  24  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.81 Test accuracy: 0.63

  ---- Epoch  25  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.82 Test accuracy: 0.625

  ---- Epoch  26  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.82 Test accuracy: 0.635

  ---- Epoch  27  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.81 Test accuracy: 0.63

  ---- Epoch  28  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.83 Test accuracy: 0.63

  ---- Epoch  29  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.84 Test accuracy: 0.63

  ---- Epoch  30  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.85 Test accuracy: 0.64

  ---- Epoch  31  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.86 Test accuracy: 0.645

  ---- Epoch  32  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.85 Test accuracy: 0.645

  ---- Epoch  33  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.84 Test accuracy: 0.63

  ---- Epoch  34  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.84 Test accuracy: 0.63

  ---- Epoch  35  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.83 Test accuracy: 0.615

  ---- Epoch  36  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.83 Test accuracy: 0.61

  ---- Epoch  37  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.85 Test accuracy: 0.615

  ---- Epoch  38  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.87 Test accuracy: 0.615

  ---- Epoch  39  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.87 Test accuracy: 0.625

  ---- Epoch  40  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.86 Test accuracy: 0.65

  ---- Epoch  41  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.87 Test accuracy: 0.645

  ---- Epoch  42  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.66

  ---- Epoch  43  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.66

  ---- Epoch  44  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.66

  ---- Epoch  45  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.665

  ---- Epoch  46  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.67

  ---- Epoch  47  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.88 Test accuracy: 0.665

  ---- Epoch  48  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.89 Test accuracy: 0.665

  ---- Epoch  49  ----

  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799

  Train accuracy: 0.93 Test accuracy: 0.66
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[89]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Save off the data for comparison dataframe</span>
<span class="n">model_six</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="n">epochs</span><span class="p">,</span>
    <span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="n">train_acc</span><span class="p">,</span>
    <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">,</span>
    <span class="s2">&quot;neurons&quot;</span><span class="p">:</span> <span class="n">n_neurons</span>
<span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">reset_graph</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span> <span class="n">RANDOM_SEED</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">REMOVE_STOPWORDS</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># no stopword removal </span>
<span class="n">EVOCABSIZE</span> <span class="o">=</span> <span class="mi">400000</span>

<span class="n">embeddings_directory</span> <span class="o">=</span> <span class="s1">&#39;embeddings/glove.6B&#39;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;glove.6B.300d.txt&#39;</span>

<span class="n">embeddings_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">embeddings_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="n">embeddings_filename</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[44]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;embeddings/glove.6B/glove.6B.300d.txt&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_embedding_from_disks</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
        <span class="n">word_to_index_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">index_to_embedding_array</span> <span class="o">=</span> <span class="p">[]</span>
  
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word_to_embedding_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">embeddings_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings_file</span><span class="p">):</span>

            <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>

            <span class="n">word</span> <span class="o">=</span> <span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">representation</span> <span class="o">=</span> <span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">representation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">representation</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
                <span class="n">word_to_index_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">index_to_embedding_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_to_embedding_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">representation</span>

    <span class="c1"># Empty representation for unknown words.</span>
    <span class="n">_WORD_NOT_FOUND</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_indexes</span><span class="p">:</span>
        <span class="n">_LAST_INDEX</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">word_to_index_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
            <span class="k">lambda</span><span class="p">:</span> <span class="n">_LAST_INDEX</span><span class="p">,</span> <span class="n">word_to_index_dict</span><span class="p">)</span>
        <span class="n">index_to_embedding_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">index_to_embedding_array</span> <span class="o">+</span> <span class="p">[</span><span class="n">_WORD_NOT_FOUND</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">word_to_index_dict</span><span class="p">,</span> <span class="n">index_to_embedding_array</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word_to_embedding_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">_WORD_NOT_FOUND</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">word_to_embedding_dict</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Loading embeddings from&#39;</span><span class="p">,</span> <span class="n">embeddings_filename</span><span class="p">)</span>
<span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_embedding</span> <span class="o">=</span> \
    <span class="n">load_embedding_from_disks</span><span class="p">(</span><span class="n">embeddings_filename</span><span class="p">,</span> <span class="n">with_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding loaded from disks.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Loading embeddings from embeddings/glove.6B/glove.6B.300d.txt
Embedding loaded from disks.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding is of shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This means (number of words, number of dimensions per word)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The first words are words that tend occur more often.&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Note: for unknown words, the representation is an empty vector,</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="s2">&quot;and the index is the last one. The dictionnary has a limit:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2"> --&gt; </span><span class="si">{}</span><span class="s2"> --&gt; </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;A word&quot;</span><span class="p">,</span> <span class="s2">&quot;Index in embedding&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;Representation&quot;</span><span class="p">))</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;worsdfkljsdf&quot;</span>  <span class="c1"># a word obviously not in the vocabulary</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="c1"># index for word obviously not in the vocabulary</span>
<span class="n">complete_vocabulary_size</span> <span class="o">=</span> <span class="n">idx</span> 
<span class="n">embd</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">))</span> <span class="c1"># &quot;int&quot; compact print</span>
<span class="c1">#print(&quot;    {} --&gt; {} --&gt; {}&quot;.format(word, idx, embd))</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;the&quot;</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
<span class="n">embd</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">index_to_embedding</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>  <span class="c1"># &quot;int&quot; for compact print only.</span>
<span class="c1">#print(&quot;    {} --&gt; {} --&gt; {}&quot;.format(word, idx, embd))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Embedding is of shape: (400001, 300)
This means (number of words, number of dimensions per word)

The first words are words that tend occur more often.
Note: for unknown words, the representation is an empty vector,
and the index is the last one. The dictionnary has a limit:
    A word --&gt; Index in embedding --&gt; Representation
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">default_factory</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">EVOCABSIZE</span>  <span class="c1"># last/unknown-word row in limited_index_to_embedding</span>
<span class="c1"># dictionary has the items() function, returns list of (key, value) tuples</span>
<span class="n">limited_word_to_index</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">default_factory</span><span class="p">,</span> \
    <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">EVOCABSIZE</span><span class="p">})</span>

<span class="c1"># Select the first EVOCABSIZE rows to the index_to_embedding</span>
<span class="n">limited_index_to_embedding</span> <span class="o">=</span> <span class="n">index_to_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">EVOCABSIZE</span><span class="p">,:]</span>
<span class="c1"># Set the unknown-word row to be all zeros as previously</span>
<span class="n">limited_index_to_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding</span><span class="p">,</span> 
    <span class="n">index_to_embedding</span><span class="p">[</span><span class="n">index_to_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span>\
        <span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">),</span> 
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Delete large numpy array to clear some CPU RAM</span>
<span class="k">del</span> <span class="n">index_to_embedding</span>

<span class="c1"># Verify the new vocabulary: should get same embeddings for test sentence</span>
<span class="c1"># Note that a small EVOCABSIZE may yield some zero vectors for embeddings</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test sentence embeddings from vocabulary of&#39;</span><span class="p">,</span> <span class="n">EVOCABSIZE</span><span class="p">,</span> <span class="s1">&#39;words:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_in_test_sentence</span><span class="p">:</span>
    <span class="n">word_</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">word_</span><span class="p">]]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word_</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Test sentence embeddings from vocabulary of 400000 words:

the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01
 -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02
  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02
 -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01
 -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01
  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02
  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01
 -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02
 -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01
  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01
 -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01
  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01
  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01
 -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01
  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01
 -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01
 -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01
  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01
  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02
  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01
  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01
  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01
  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01
 -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03
 -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01
  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01
 -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01
  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01
  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01
 -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01
  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01
 -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01
 -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02
 -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01
  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01
  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02
  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01
 -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01
 -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01
  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01
 -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01
  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02
  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02
  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03
 -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02
 -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01
 -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01
 -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02
  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01
  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]
quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801
  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258
 -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292
 -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052
  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715
 -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699
 -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276
  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866
 -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246
 -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208
  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175
  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005
  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614
  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947
  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862
  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244
 -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623
  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113
 -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157
  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463
  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464
  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627
 -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031
  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818
 -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701
 -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758
 -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589
  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676
 -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779
 -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525
  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478
 -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803
 -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353
  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055
 -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267
 -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908
  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236
 -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384
  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896
 -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886
  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215
  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488
  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332
 -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296
  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057
 -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197
 -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943
  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682
 -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828
  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]
brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335
 -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953
 -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766
 -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895
 -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592
 -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345
  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059
  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363
 -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243
  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724
 -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304
  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844
  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575
  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766
  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334
 -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906
  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614
 -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238
  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901
  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468
 -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414
 -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594
 -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055
 -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505
 -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667
  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919
  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635
  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491
 -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028
 -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925
 -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068
 -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333
 -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291
 -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758
  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732
  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711
  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494
  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999
  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599
 -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556
 -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044
  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129
  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565
  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345
 -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615
 -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575
 -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018
  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111
  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419
  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]
fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01
 -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01
  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01
  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01
  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01
 -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01
  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01
  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02
  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01
  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01
  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01
  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01
 -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01
  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02
  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01
 -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01
 -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01
 -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01
  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01
  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01
 -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01
 -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01
 -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01
 -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01
 -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01
  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01
  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03
  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01
 -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01
  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02
  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01
  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01
 -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01
  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01
  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02
  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02
 -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02
  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01
  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01
 -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02
  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01
 -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02
 -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02
  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01
 -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01
  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01
 -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01
  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01
  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01
 -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]
jumps:  [-0.16814   -0.10948    0.2896    -0.21108   -0.29061    0.31201
  0.04039   -0.10149   -0.18526   -0.55483   -0.36055   -0.093569
  0.77334    0.027921   0.13389   -0.1014    -0.06482    0.24753
 -0.068026   0.26147   -0.14252    0.18657    0.030249  -0.07934
  0.87696    0.61781    0.36835   -0.07306   -0.19303    0.3721
 -0.77087   -0.0062782 -0.19233   -0.43884   -0.79847    0.066636
 -0.21387   -0.65263   -0.073964   0.64115   -0.52014   -0.05981
 -0.1692    -0.413      0.060197   0.16327    0.43353    0.070021
  0.063543   0.28527   -0.43474    0.15441    0.098037   0.098685
 -0.3965     0.38171   -0.084065  -0.4813    -0.59213    0.40444
 -0.2026     0.45569    0.039036  -0.41786   -0.20322   -0.11932
  0.23747    0.26336    0.17139    0.12521   -0.55276    0.45515
 -0.63826    0.14054    0.35333   -0.28417    0.3889    -0.13004
 -0.027142  -0.23109    0.034327  -0.10685    0.85855    0.15145
  0.16814    0.2281     0.22235   -0.1825    -0.019222   0.026105
  0.47734    0.42115   -0.087767  -0.17439    0.22166   -0.36831
 -1.069      0.40489   -0.31038   -0.21588   -0.7282     0.29296
 -0.42949    0.23485    0.020585  -0.47795   -0.20216   -0.22146
 -0.45778    0.032547  -0.13727   -0.48945   -0.58148    0.051203
 -0.065926   0.46718   -0.080438  -0.25042   -0.4015     0.40254
  0.12       0.5246     0.21582    0.1333     0.27662    0.2163
 -0.28177    0.67185   -0.025996  -0.40781   -0.23629    0.97455
 -0.51452    0.22697   -0.34857   -0.55928    0.019104  -0.016163
 -0.22626    0.094269   0.10808   -0.018804  -0.080299   0.0058964
 -0.61886    0.44842    0.2494    -0.25172    0.6705     0.23657
 -0.17631    0.28634   -0.39527   -0.22096    0.23069    0.0644
  0.13151   -0.030479   0.38503   -0.084794   0.66178   -0.34578
  0.3868     0.31506    0.20155   -0.026189   0.051188   0.16359
 -0.37096    0.21546   -0.19758   -0.083407   0.48437   -0.5487
  0.35188   -0.43539    0.3976    -0.026514  -0.14485   -0.11205
  0.59835    0.38055    0.1582    -0.24293    0.39837    0.44276
  0.36113   -0.2358    -0.32568    0.54672   -0.36544   -0.33871
  0.2154    -0.36877   -0.18857    0.42323   -0.56938   -0.20276
  0.35452   -0.0167     1.157      0.22296   -0.35115    0.3662
 -0.20903    0.73497    0.018414   0.27861    0.044337   0.14504
 -0.23001   -0.54025   -0.26259   -0.94587    0.22239    0.44651
 -0.46075   -0.42224    0.0022315 -0.25522   -0.13135    0.65923
  0.21267   -0.26498   -0.16777   -0.047013  -0.18551    0.12607
  0.43205    0.153     -0.33748    0.010732  -0.18332   -0.28507
  0.23396    0.3868    -0.34724    0.41692    0.56315    0.32366
  0.55303    0.02256   -0.48281    0.54561   -0.51086   -0.55446
 -0.018992   0.62551    0.11436   -0.10939   -0.61894   -0.18419
  0.27755   -0.29704   -0.21562   -0.011765   0.051859   0.04041
 -0.16798    0.65697   -0.36868    0.068085   0.34986    0.076252
  0.12035    0.16001   -0.0088939  0.14892   -0.70374   -0.031891
  0.2803    -0.26916   -0.61974    0.67423    0.080773  -0.32835
 -0.276     -0.60342   -0.60185    0.10982   -0.20312    0.42309
  0.63701    0.51885   -0.31899    0.34256   -0.035933  -0.049542
 -0.89278   -0.03686    0.12862    0.41469   -0.37262   -0.20523
 -0.02476   -0.11959   -0.12129    0.099622   0.24538   -0.026313 ]
over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01
 -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01
 -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01
  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01
 -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01
 -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01
 -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01
 -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01
 -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01
  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01
  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01
  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01
  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01
 -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01
 -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02
  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01
 -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02
  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01
 -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02
 -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01
  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01
 -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01
  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02
  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01
 -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01
  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01
 -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01
  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02
 -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01
 -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01
 -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01
 -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01
 -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01
  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01
  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02
  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01
 -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01
 -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02
 -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02
  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02
 -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03
  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01
  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01
 -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01
 -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03
 -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02
 -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01
  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01
  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01
  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]
the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01
 -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02
  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02
 -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01
 -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01
  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02
  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01
 -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02
 -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01
  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01
 -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01
  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01
  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01
 -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01
  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01
 -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01
 -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01
  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01
  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02
  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01
  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01
  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01
  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01
 -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03
 -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01
  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01
 -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01
  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01
  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01
 -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01
  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01
 -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01
 -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02
 -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01
  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01
  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02
  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01
 -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01
 -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01
  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01
 -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01
  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02
  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02
  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03
 -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02
 -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01
 -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01
 -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02
  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01
  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]
lazy:  [ 4.2791e-01 -1.6070e-01  2.4912e-01  3.9763e-01 -3.2224e-01  1.9783e-04
  8.8576e-02  4.0501e-01 -2.4655e-01  1.6410e-02 -2.3331e-01 -1.2307e-01
 -3.9679e-01  2.1877e-02  1.6211e-01  3.8852e-01  2.5025e-01 -4.3968e-02
  1.0819e+00  6.6517e-01  2.1829e-01  7.9898e-01 -6.1695e-01 -1.0184e-01
  2.2900e-01 -3.1982e-01  3.1205e-01 -1.0453e-01  4.4810e-01  3.6708e-02
 -1.1376e-02  7.0543e-01  2.7454e-01  2.5835e-01 -5.1821e-01  8.2996e-01
 -8.0672e-02 -4.5489e-01 -3.4843e-01  5.1421e-01 -5.7408e-01  6.4612e-01
 -2.1564e-01 -5.4848e-01  2.3656e-01  1.8453e-02  8.6403e-01 -1.0357e-01
 -5.6097e-02 -4.8564e-01  2.7927e-01 -6.1733e-01  5.2877e-01 -1.9221e-01
 -1.4684e-01  4.2272e-01  5.6312e-02 -3.3168e-01  8.1864e-02  2.2225e-01
  2.7757e-01 -2.1934e-02  2.8499e-01  5.7453e-03 -4.0598e-01  6.1549e-02
  1.1544e-01  7.2053e-02 -5.4945e-02 -3.9549e-02 -1.9371e-01  2.7872e-01
 -1.0139e-01 -7.1846e-02 -3.4043e-01 -2.2490e-02 -2.6097e-03  6.5403e-01
  4.1425e-01 -1.1459e-01 -4.9802e-01 -3.4516e-02 -3.0815e-02 -6.1508e-01
  4.8955e-01 -1.0641e-01  6.3485e-02  5.5039e-01 -1.6282e-01 -7.7526e-02
 -1.5945e-01  3.0791e-01 -3.0439e-02 -3.8283e-01  7.8436e-02  1.1488e-01
  6.7573e-02  2.2181e-01 -1.4319e-02  1.4366e-02  2.8839e-01 -9.1358e-01
 -5.3084e-01  1.3097e-01 -2.0027e-01  2.1495e-01 -3.6158e-01  7.6012e-02
  1.2718e-01  2.5851e-01  6.6530e-02  3.1628e-01 -6.3175e-01 -4.1942e-01
  3.8640e-01  7.3017e-02 -9.2298e-02 -8.8510e-01 -2.1618e-01  2.9006e-01
  3.6404e-01  2.3740e-01 -2.2910e-01  3.2436e-01  6.0954e-01  3.2458e-01
 -8.4691e-02  4.1471e-01  2.6053e-01  6.8716e-02 -4.4528e-01  2.9296e-02
 -4.7913e-02  4.5709e-01 -5.2956e-01 -3.0998e-01  3.2488e-01 -3.6054e-01
  1.8449e-01 -3.8492e-01  3.9918e-02  4.0046e-01 -8.7039e-02 -4.9739e-01
 -7.1877e-01  1.4894e-01  1.5550e-01  3.2388e-01  6.7209e-01  4.0215e-01
 -7.3436e-01  7.0311e-02 -7.3754e-02  3.1062e-01  1.4725e-03  4.3386e-02
  2.9529e-01 -1.0544e-01  7.5028e-01  2.2274e-01  4.3789e-02 -6.7316e-01
  6.8448e-01 -5.5239e-01 -2.2204e-02 -1.8850e-01 -4.0747e-01 -5.7321e-02
  1.8211e-01 -5.8300e-01 -2.0733e-01  2.8411e-01 -3.5269e-01 -1.2422e-03
 -1.7660e-01 -6.9418e-01 -2.7758e-01  3.6022e-01 -1.7472e-01 -2.9792e-01
  4.5945e-01  1.3176e-01 -2.2119e-01 -1.0709e-01  1.6470e-02  1.5636e-01
  4.0296e-01 -7.1640e-01  4.4003e-01 -2.0855e-01 -5.1675e-01  3.3292e-01
  2.5018e-01 -5.7151e-01 -7.5581e-04 -8.8453e-02 -4.6790e-01 -6.3341e-01
  1.7374e-01 -1.2076e-01  5.9592e-01 -2.6351e-02 -1.7858e-02  1.2978e-01
  2.4669e-01  9.1323e-02 -1.4112e-01  4.6466e-02  1.8030e-01  1.7382e-01
 -7.7972e-02 -1.4600e-01 -5.1364e-01  5.1154e-01 -3.3271e-01  5.2346e-01
 -9.4829e-02 -2.0365e-01  5.6919e-01 -1.5709e-01 -5.4340e-01  2.4034e-01
 -4.5061e-02  1.5918e-01 -7.0530e-01 -7.9981e-03  5.1987e-01 -1.6802e-01
 -8.0854e-03  2.4719e-01  3.6062e-01 -2.2302e-01 -3.2196e-01 -7.6371e-01
  1.9203e-02  2.0398e-01 -4.4568e-01  1.1332e-01 -1.3784e-01 -5.6301e-02
  5.5306e-01  4.6183e-01 -8.0710e-01 -4.1624e-01 -5.1206e-01 -8.1953e-01
  7.8391e-03  2.8204e-02  2.0151e-01  4.5986e-01  2.0020e-01 -9.1094e-02
  4.3782e-01  3.4559e-01  3.6562e-01  3.4960e-01  1.7984e-01 -2.3978e-01
 -2.5039e-01  6.7002e-03  1.0974e-01  8.1626e-02 -2.4783e-01  2.6453e-01
 -3.6779e-02  3.1099e-01  6.7982e-01 -4.6699e-01 -2.8060e-01  8.2703e-01
  2.3553e-01 -7.4127e-01  2.9891e-02 -1.3198e-01  2.2106e-01  1.7262e-01
 -4.2037e-01  5.6484e-01 -7.0211e-01 -1.5537e-01 -8.0067e-02 -6.9698e-02
  8.0176e-01 -2.4841e-01 -7.1711e-01 -2.5340e-01  7.2812e-01  1.6527e-01
 -2.2780e-01 -3.2008e-01  2.8609e-01 -5.8733e-02 -5.4448e-01  2.6380e-01
  3.3366e-01 -5.1100e-01 -1.0377e-01 -1.9413e-01 -3.9855e-01  2.2238e-01]
dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02
  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02
 -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01
  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01
 -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01
 -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01
 -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01
  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02
  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01
  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01
  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01
 -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01
 -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01
  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01
 -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01
 -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01
  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03
 -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01
 -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01
  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01
 -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02
 -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01
 -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01
  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03
  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01
 -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02
 -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01
  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01
 -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01
 -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01
  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02
  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01
 -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01
 -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02
 -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01
 -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01
 -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01
  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01
 -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01
  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01
  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01
 -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01
 -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01
 -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02
 -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02
 -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01
 -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01
  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01
  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01
 -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------</span>
<span class="c1"># convert positive/negative documents into numpy array</span>
<span class="c1"># note that reviews vary from 22 to 1052 words   </span>
<span class="c1"># so we use the first 20 and last 20 words of each review </span>
<span class="c1"># as our word sequences for analysis</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">max_review_length</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">max_review_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;max_review_length:&#39;</span><span class="p">,</span> <span class="n">max_review_length</span><span class="p">)</span> 

<span class="n">min_review_length</span> <span class="o">=</span> <span class="n">max_review_length</span>  <span class="c1"># initialize</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">min_review_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_review_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;min_review_length:&#39;</span><span class="p">,</span> <span class="n">min_review_length</span><span class="p">)</span> 

<span class="c1"># construct list of 1000 lists with 40 words in each list</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">chain</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">negative_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">positive_documents</span><span class="p">:</span>
    <span class="n">doc_begin</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
    <span class="n">doc_end</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">20</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">doc_begin</span><span class="p">,</span> <span class="n">doc_end</span><span class="p">])))</span>    

<span class="c1"># create list of lists of lists for embeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>    
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
       <span class="n">embedding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span> 
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>max_review_length: 1052
min_review_length: 22
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># -----------------------------------------------------    </span>
<span class="c1"># Check on the embeddings list of list of lists </span>
<span class="c1"># -----------------------------------------------------</span>
<span class="c1"># Show the first word in the first document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:])</span>

<span class="c1"># Show the seventh word in the tenth document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">6</span><span class="p">][</span><span class="mi">9</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">6</span><span class="p">][</span><span class="mi">9</span><span class="p">][:])</span>

<span class="c1"># Show the last word in the last document</span>
<span class="n">test_word</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="mi">39</span><span class="p">]</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First word in first document:&#39;</span><span class="p">,</span> <span class="n">test_word</span><span class="p">)</span>    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedding for this word:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> 
      <span class="n">limited_index_to_embedding</span><span class="p">[</span><span class="n">limited_word_to_index</span><span class="p">[</span><span class="n">test_word</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Corresponding embedding from embeddings list of list of lists</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="mi">39</span><span class="p">][:])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>First word in first document: while
Embedding for this word:
 [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02
 -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02
  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02
 -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02
 -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01
 -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01
  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02
 -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01
 -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03
 -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03
 -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01
  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02
  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01
 -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01
  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02
  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01
 -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02
 -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01
 -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02
  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02
 -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01
 -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04
  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01
  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01
 -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02
  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01
 -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01
  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02
  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01
  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01
 -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02
 -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01
 -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01
  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01
 -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01
 -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01
 -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02
 -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01
 -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01
  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01
 -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03
  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02
  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01
 -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01
 -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01
 -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01
 -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02
 -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01
  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02
  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]
Corresponding embedding from embeddings list of list of lists
 [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02
 -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02
  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02
 -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02
 -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01
 -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01
  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02
 -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01
 -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03
 -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03
 -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01
  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02
  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01
 -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01
  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02
  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01
 -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02
 -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01
 -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02
  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02
 -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01
 -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04
  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01
  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01
 -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02
  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01
 -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01
  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02
  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01
  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01
 -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02
 -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01
 -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01
  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01
 -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01
 -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01
 -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02
 -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01
 -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01
  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01
 -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03
  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02
  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01
 -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01
 -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01
 -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01
 -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02
 -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01
  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02
  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]
First word in first document: officially
Embedding for this word:
 [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01
 -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01
  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02
  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01
 -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01
  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01
  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01
 -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01
 -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01
 -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01
 -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01
  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02
 -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01
  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02
  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01
 -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01
 -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01
  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01
 -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01
 -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01
 -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01
 -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02
  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01
 -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02
  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01
  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01
 -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01
 -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01
  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02
  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01
  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01
 -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02
 -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01
  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01
 -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01
  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01
 -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01
  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01
 -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02
 -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02
 -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01
  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01
 -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01
  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02
 -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01
  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03
 -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01
  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01
 -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01
 -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]
Corresponding embedding from embeddings list of list of lists
 [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01
 -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01
  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02
  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01
 -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01
  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01
  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01
 -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01
 -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01
 -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01
 -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01
  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02
 -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01
  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02
  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01
 -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01
 -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01
  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01
 -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01
 -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01
 -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01
 -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02
  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01
 -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02
  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01
  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01
 -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01
 -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01
  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02
  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01
  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01
 -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02
 -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01
  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01
 -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01
  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01
 -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01
  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01
 -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02
 -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02
 -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01
  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01
 -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01
  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02
 -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01
  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03
 -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01
  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01
 -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01
 -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]
First word in first document: super
Embedding for this word:
 [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01
  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01
 -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02
 -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01
  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01
 -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01
 -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02
 -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01
  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01
 -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01
  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01
  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01
 -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01
 -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02
  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01
  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01
 -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02
  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02
 -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01
 -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01
  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01
 -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01
  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01
 -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01
 -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01
  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01
 -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01
 -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01
  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02
 -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01
  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01
 -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01
 -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01
  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01
  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02
 -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01
 -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01
  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02
 -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01
  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02
  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01
 -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03
 -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01
  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01
  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01
  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02
 -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01
  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01
 -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01
  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]
Corresponding embedding from embeddings list of list of lists
 [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01
  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01
 -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02
 -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01
  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01
 -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01
 -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02
 -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01
  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01
 -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01
  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01
  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01
 -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01
 -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02
  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01
  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01
 -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02
  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02
 -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01
 -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01
  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01
 -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01
  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01
 -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01
 -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01
  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01
 -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01
 -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01
  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02
 -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01
  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01
 -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01
 -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01
  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01
  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02
 -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01
 -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01
  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02
 -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01
  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02
  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01
 -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03
 -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01
  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01
  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01
  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02
 -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01
  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01
 -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01
  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">9999</span>
<span class="c1"># -----------------------------------------------------    </span>
<span class="c1"># Make embeddings a numpy array for use in an RNN </span>
<span class="c1"># Create training and test sets with Scikit Learn</span>
<span class="c1"># -----------------------------------------------------</span>
<span class="n">embeddings_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Define the labels to be used 500 negative (0) and 500 positive (1)</span>
<span class="n">thumbs_down_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> 
                      <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">500</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Scikit Learn for random splitting of the data  </span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="c1"># Random splitting of the data in to training (80%) and test (20%)  </span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">embeddings_array</span><span class="p">,</span> <span class="n">thumbs_down_up</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> 
                     <span class="n">random_state</span> <span class="o">=</span> <span class="n">RANDOM_SEED</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-7:">Model 7:<a class="anchor-link" href="#Model-7:">&#182;</a></h4><ul>
<li>20 Neurons</li>
<li>300 dimensions of pre-trained embeddings</li>
<li>50 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[98]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document </span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">lstm_cells</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
<span class="n">multi_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">(</span><span class="n">lstm_cells</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">multi_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">top_layer_h_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">top_layer_h_state</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>          
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  Batch &#39;</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39; training observations from &#39;</span><span class="p">,</span>  
                  <span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,</span> <span class="s1">&#39; to &#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
        <span class="n">results</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Train&#39;</span> <span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s1">&#39;Test&#39;</span> <span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Train accuracy =&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy =&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 0 Train accuracy = 0.54 Test accuracy = 0.495
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 1 Train accuracy = 0.55 Test accuracy = 0.505
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 2 Train accuracy = 0.54 Test accuracy = 0.505
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 3 Train accuracy = 0.53 Test accuracy = 0.505
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 4 Train accuracy = 0.53 Test accuracy = 0.51
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 5 Train accuracy = 0.54 Test accuracy = 0.525
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 6 Train accuracy = 0.54 Test accuracy = 0.525
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 7 Train accuracy = 0.53 Test accuracy = 0.545
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 8 Train accuracy = 0.55 Test accuracy = 0.555
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 9 Train accuracy = 0.57 Test accuracy = 0.57
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 10 Train accuracy = 0.62 Test accuracy = 0.585
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 11 Train accuracy = 0.64 Test accuracy = 0.625
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 12 Train accuracy = 0.66 Test accuracy = 0.635
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 13 Train accuracy = 0.66 Test accuracy = 0.65
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 14 Train accuracy = 0.66 Test accuracy = 0.655
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 15 Train accuracy = 0.72 Test accuracy = 0.665
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 16 Train accuracy = 0.72 Test accuracy = 0.65
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 17 Train accuracy = 0.73 Test accuracy = 0.65
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 18 Train accuracy = 0.76 Test accuracy = 0.64
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 19 Train accuracy = 0.77 Test accuracy = 0.655
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 20 Train accuracy = 0.78 Test accuracy = 0.665
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 21 Train accuracy = 0.78 Test accuracy = 0.665
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 22 Train accuracy = 0.78 Test accuracy = 0.68
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 23 Train accuracy = 0.78 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 24 Train accuracy = 0.79 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 25 Train accuracy = 0.79 Test accuracy = 0.715
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 26 Train accuracy = 0.81 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 27 Train accuracy = 0.82 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 28 Train accuracy = 0.82 Test accuracy = 0.73
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 29 Train accuracy = 0.83 Test accuracy = 0.73
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 30 Train accuracy = 0.83 Test accuracy = 0.73
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 31 Train accuracy = 0.83 Test accuracy = 0.73
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 32 Train accuracy = 0.83 Test accuracy = 0.735
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 33 Train accuracy = 0.83 Test accuracy = 0.73
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 34 Train accuracy = 0.83 Test accuracy = 0.735
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 35 Train accuracy = 0.83 Test accuracy = 0.74
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 36 Train accuracy = 0.82 Test accuracy = 0.74
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 37 Train accuracy = 0.82 Test accuracy = 0.745
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 38 Train accuracy = 0.82 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 39 Train accuracy = 0.82 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 40 Train accuracy = 0.82 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 41 Train accuracy = 0.82 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 42 Train accuracy = 0.83 Test accuracy = 0.755
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 43 Train accuracy = 0.84 Test accuracy = 0.755
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 44 Train accuracy = 0.85 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 45 Train accuracy = 0.85 Test accuracy = 0.75
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 46 Train accuracy = 0.85 Test accuracy = 0.755
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 47 Train accuracy = 0.86 Test accuracy = 0.755
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 48 Train accuracy = 0.87 Test accuracy = 0.755
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 49 Train accuracy = 0.88 Test accuracy = 0.76
CPU times: user 55.4 s, sys: 11 s, total: 1min 6s
Wall time: 34.7 s
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-8:">Model 8:<a class="anchor-link" href="#Model-8:">&#182;</a></h4><ul>
<li>30 Neurons</li>
<li>300 dimensions of pre-trained embeddings</li>
<li>50 epochs</li>
<li>100 batch size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of words per document </span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">embeddings_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># dimension of  pre-trained embeddings</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># analyst specified number of neurons</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># thumbs-down or thumbs-up</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>

<span class="n">lstm_cells</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
<span class="n">multi_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">(</span><span class="n">lstm_cells</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">multi_cell</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">top_layer_h_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">top_layer_h_state</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>          
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,:]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  Batch &#39;</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39; training observations from &#39;</span><span class="p">,</span>  
                  <span class="n">iteration</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,</span> <span class="s1">&#39; to &#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Train accuracy =&quot;</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;Test accuracy =&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 0 Train accuracy = 0.65 Test accuracy = 0.56
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 1 Train accuracy = 0.72 Test accuracy = 0.58
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 2 Train accuracy = 0.72 Test accuracy = 0.63
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 3 Train accuracy = 0.79 Test accuracy = 0.67
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 4 Train accuracy = 0.76 Test accuracy = 0.68
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 5 Train accuracy = 0.77 Test accuracy = 0.69
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 6 Train accuracy = 0.79 Test accuracy = 0.715
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 7 Train accuracy = 0.8 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 8 Train accuracy = 0.8 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 9 Train accuracy = 0.74 Test accuracy = 0.615
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 10 Train accuracy = 0.87 Test accuracy = 0.69
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 11 Train accuracy = 0.86 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 12 Train accuracy = 0.87 Test accuracy = 0.715
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 13 Train accuracy = 0.83 Test accuracy = 0.66
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 14 Train accuracy = 0.89 Test accuracy = 0.7
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 15 Train accuracy = 0.92 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 16 Train accuracy = 0.93 Test accuracy = 0.725
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 17 Train accuracy = 0.96 Test accuracy = 0.7
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 18 Train accuracy = 0.97 Test accuracy = 0.685
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 19 Train accuracy = 0.94 Test accuracy = 0.655
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 20 Train accuracy = 0.97 Test accuracy = 0.69
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 21 Train accuracy = 0.96 Test accuracy = 0.675
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 22 Train accuracy = 0.93 Test accuracy = 0.66
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 23 Train accuracy = 0.87 Test accuracy = 0.62
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 24 Train accuracy = 0.96 Test accuracy = 0.66
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 25 Train accuracy = 0.92 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 26 Train accuracy = 0.94 Test accuracy = 0.685
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 27 Train accuracy = 0.97 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 28 Train accuracy = 0.98 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 29 Train accuracy = 0.99 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 30 Train accuracy = 0.98 Test accuracy = 0.685
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 31 Train accuracy = 1.0 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 32 Train accuracy = 1.0 Test accuracy = 0.67
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 33 Train accuracy = 1.0 Test accuracy = 0.685
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 34 Train accuracy = 1.0 Test accuracy = 0.7
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 35 Train accuracy = 1.0 Test accuracy = 0.695
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 36 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 37 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 38 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 39 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 40 Train accuracy = 1.0 Test accuracy = 0.7
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 41 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 42 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 43 Train accuracy = 1.0 Test accuracy = 0.705
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 44 Train accuracy = 1.0 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 45 Train accuracy = 1.0 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 46 Train accuracy = 1.0 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 47 Train accuracy = 1.0 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 48 Train accuracy = 1.0 Test accuracy = 0.71
  Batch  0  training observations from  0  to  99
  Batch  1  training observations from  100  to  199
  Batch  2  training observations from  200  to  299
  Batch  3  training observations from  300  to  399
  Batch  4  training observations from  400  to  499
  Batch  5  training observations from  500  to  599
  Batch  6  training observations from  600  to  699
  Batch  7  training observations from  700  to  799
Epoch 49 Train accuracy = 1.0 Test accuracy = 0.71
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nn_summary_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;Word Vector&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;GloVe.6B.50d&quot;</span><span class="p">,</span> <span class="s2">&quot;GloVe.6B.50d&quot;</span><span class="p">,</span><span class="s2">&quot;GloVe.6B.100d&quot;</span><span class="p">,</span><span class="s2">&quot;GloVe.6B.100d&quot;</span><span class="p">,</span><span class="s2">&quot;GloVe.6B.100d&quot;</span><span class="p">,</span><span class="s2">&quot;GloVe.Twitter.100d&quot;</span><span class="p">,</span><span class="s2">&quot;GloVe.6B.300d&quot;</span><span class="p">,</span> <span class="s2">&quot;GloVe.6B.300d&quot;</span><span class="p">],</span>
        <span class="s2">&quot;RNN Model Type&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Basic&quot;</span><span class="p">,</span> <span class="s2">&quot;Basic&quot;</span><span class="p">,</span><span class="s2">&quot;Basic&quot;</span><span class="p">,</span><span class="s2">&quot;Basic&quot;</span><span class="p">,</span><span class="s2">&quot;Basic&quot;</span><span class="p">,</span><span class="s2">&quot;Basic&quot;</span><span class="p">,</span><span class="s2">&quot;LSTM&quot;</span><span class="p">,</span> <span class="s2">&quot;LSTM&quot;</span><span class="p">],</span>
        <span class="s2">&quot;Neurons&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
        <span class="s2">&quot;Dimensions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
        <span class="s2">&quot;Epochs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
        <span class="s2">&quot;TrainingAccuracy&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">,</span> <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
        <span class="s2">&quot;TestingAccuracy&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.68</span><span class="p">,</span> <span class="mf">0.63</span><span class="p">,</span> <span class="mf">0.67</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.67</span><span class="p">,</span> <span class="mf">0.76</span><span class="p">,</span> <span class="mf">0.71</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Model 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 4&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 5&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 6&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 7&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 8&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">nn_summary_df</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[3]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Word Vector</th>
      <th>RNN Model Type</th>
      <th>Neurons</th>
      <th>Dimensions</th>
      <th>Epochs</th>
      <th>TrainingAccuracy</th>
      <th>TestingAccuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Model 1</th>
      <td>GloVe.6B.50d</td>
      <td>Basic</td>
      <td>20</td>
      <td>50</td>
      <td>50</td>
      <td>0.86</td>
      <td>0.68</td>
    </tr>
    <tr>
      <th>Model 2</th>
      <td>GloVe.6B.50d</td>
      <td>Basic</td>
      <td>30</td>
      <td>50</td>
      <td>25</td>
      <td>0.82</td>
      <td>0.63</td>
    </tr>
    <tr>
      <th>Model 3</th>
      <td>GloVe.6B.100d</td>
      <td>Basic</td>
      <td>20</td>
      <td>100</td>
      <td>50</td>
      <td>0.94</td>
      <td>0.67</td>
    </tr>
    <tr>
      <th>Model 4</th>
      <td>GloVe.6B.100d</td>
      <td>Basic</td>
      <td>30</td>
      <td>100</td>
      <td>25</td>
      <td>0.86</td>
      <td>0.66</td>
    </tr>
    <tr>
      <th>Model 5</th>
      <td>GloVe.6B.100d</td>
      <td>Basic</td>
      <td>30</td>
      <td>100</td>
      <td>40</td>
      <td>0.89</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>Model 6</th>
      <td>GloVe.Twitter.100d</td>
      <td>Basic</td>
      <td>20</td>
      <td>100</td>
      <td>25</td>
      <td>0.93</td>
      <td>0.67</td>
    </tr>
    <tr>
      <th>Model 7</th>
      <td>GloVe.6B.300d</td>
      <td>LSTM</td>
      <td>20</td>
      <td>300</td>
      <td>50</td>
      <td>0.88</td>
      <td>0.76</td>
    </tr>
    <tr>
      <th>Model 8</th>
      <td>GloVe.6B.300d</td>
      <td>LSTM</td>
      <td>30</td>
      <td>300</td>
      <td>50</td>
      <td>1.00</td>
      <td>0.71</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[123]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Based on model performance, we recommend Model 7 and its Training &amp; Testing Accuracy Visualization is shown as below</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">results_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">logy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">&#39;ocean_r&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span> <span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span> <span class="s1">&#39;Train Iterations&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">():</span>
    <span class="n">label</span><span class="o">.</span><span class="n">set_rotation</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">label</span><span class="o">.</span><span class="n">set_size</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
        
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
