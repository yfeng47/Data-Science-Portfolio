{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "  trained using the backpropagation method.\n",
      "Version 0.2, 01/22/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      " \n",
      "This program learns to distinguish between five capital letters: X, M, H, A, and N\n",
      "It allows users to examine the hidden weights to identify learned features\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      " \n",
      "  The number of nodes at each level are:\n",
      "    Input: 5x5 (square array)\n",
      "    Hidden: 6\n",
      "    Output: 5 (five classes)\n",
      "[ 0.377  0.322 -0.117  0.972  0.894 -0.319]\n",
      "[-0.319  0.687 -0.394  0.161 -0.955]\n",
      " \n",
      "  Before training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.525 0.185 0.591 0.051 0.529 0.862]\n",
      " \n",
      " The output node activations are:\n",
      "[0.527 0.631 0.254 0.515 0.317]\n",
      " \n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[ 0.473 -0.631 -0.254 -0.515 -0.317]\n",
      "New SSE = 1.052650\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.781 0.132 0.889 0.042 0.547 0.802]\n",
      " \n",
      " The output node activations are:\n",
      "[0.505 0.602 0.264 0.623 0.367]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.505  0.398 -0.264 -0.623 -0.367]\n",
      "New SSE = 1.005051\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.694 0.353 0.922 0.056 0.535 0.901]\n",
      " \n",
      " The output node activations are:\n",
      "[0.518 0.573 0.275 0.578 0.333]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.518 -0.573  0.725 -0.578 -0.333]\n",
      "New SSE = 1.567192\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.754 0.332 0.887 0.074 0.259 0.756]\n",
      " \n",
      " The output node activations are:\n",
      "[0.54  0.618 0.332 0.646 0.307]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.54  -0.618 -0.332  0.354 -0.307]\n",
      "New SSE = 1.003412\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.666 0.099 0.889 0.025 0.321 0.729]\n",
      " \n",
      " The output node activations are:\n",
      "[0.493 0.645 0.325 0.661 0.312]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1.]\n",
      " \n",
      " The error values are:\n",
      "[-0.493 -0.645 -0.325 -0.661  0.688]\n",
      "New SSE = 1.674157\n",
      "Out of while loop at iteration  545\n",
      " \n",
      "  After training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.842 0.932 0.022 0.259 0.565 0.988]\n",
      " \n",
      " The output node activations are:\n",
      "[0.831 0.093 0.065 0.013 0.086]\n",
      " \n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[ 0.169 -0.093 -0.065 -0.013 -0.086]\n",
      "New SSE = 0.048749\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.588 0.074 0.625 0.685 0.254 0.915]\n",
      " \n",
      " The output node activations are:\n",
      "[0.129 0.741 0.131 0.126 0.046]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.129  0.259 -0.131 -0.126 -0.046]\n",
      "New SSE = 0.118986\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.169 0.877 0.898 0.836 0.109 0.923]\n",
      " \n",
      " The output node activations are:\n",
      "[0.085 0.194 0.742 0.139 0.006]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.085 -0.194  0.258 -0.139 -0.006]\n",
      "New SSE = 0.130817\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.877 0.46  0.948 0.746 0.052 0.074]\n",
      " \n",
      " The output node activations are:\n",
      "[0.034 0.146 0.127 0.782 0.143]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.034 -0.146 -0.127  0.218 -0.143]\n",
      "New SSE = 0.106657\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.986 0.142 0.62  0.108 0.964 0.064]\n",
      " \n",
      " The output node activations are:\n",
      "[0.078 0.121 0.006 0.152 0.832]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1.]\n",
      " \n",
      " The error values are:\n",
      "[-0.078 -0.121 -0.006 -0.152  0.168]\n",
      "New SSE = 0.072051\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "import random\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# So we can make a separate list from an initial one\n",
    "import copy\n",
    "\n",
    "# For pretty-printing the arrays\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# This is a tutorial program, designed for those who are learning Python, and specifically using \n",
    "#   Python for neural networks applications\n",
    "#\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to welcome the user and identify the code\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def welcome ():\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('******************************************************************************')\n",
    "    print()\n",
    "    print('Welcome to the Multilayer Perceptron Neural Network')\n",
    "    print('  trained using the backpropagation method.')\n",
    "    print('Version 0.2, 01/22/2017, A.J. Maren')\n",
    "    print('For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu')\n",
    "    print(' ') \n",
    "    print('This program learns to distinguish between five capital letters: X, M, H, A, and N')\n",
    "    print('It allows users to examine the hidden weights to identify learned features')\n",
    "    print()\n",
    "    print('******************************************************************************')\n",
    "    print()\n",
    "    return()\n",
    "\n",
    "        \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# A collection of worker-functions, designed to do specific small tasks\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "#------------------------------------------------------#    \n",
    "\n",
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
    "    return activation\n",
    "  \n",
    "\n",
    "#------------------------------------------------------# \n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "\n",
    "#------------------------------------------------------# \n",
    "def matrixDotProduct (matrx1,matrx2):\n",
    "    dotProduct = np.dot(matrx1,matrx2)\n",
    "    \n",
    "    return(dotProduct)    \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to obtain the neural network size specifications\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "\n",
    "# This procedure operates as a function, as it returns a single value (which really is a list of \n",
    "#    three values). It is called directly from 'main.'\n",
    "#        \n",
    "# This procedure allows the user to specify the size of the input (I), hidden (H), \n",
    "#    and output (O) layers.  \n",
    "# These values will be stored in a list, the arraySizeList. \n",
    "# This list will be used to specify the sizes of two different weight arrays:\n",
    "#   - wWeights; the Input-to-Hidden array, and\n",
    "#   - vWeights; the Hidden-to-Output array. \n",
    "# However, even though we're calling this procedure, we will still hard-code the array sizes for now.   \n",
    "\n",
    "    numInputNodes = 25\n",
    "    numHiddenNodes = 6\n",
    "    numOutputNodes = 5   \n",
    "    print(' ')\n",
    "    print('  The number of nodes at each level are:')\n",
    "    print('    Input: 5x5 (square array)')\n",
    "    print('    Hidden: 6')\n",
    "    print('    Output: 5 (five classes)')\n",
    "            \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def InitializeWeight ():\n",
    "\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "#    print weight\n",
    "           \n",
    "    return (weight)  \n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the node-to-node connection weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeWeightArray (weightArraySizeList):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "#        \n",
    "# This procedure takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be assigned random values here, and \n",
    "#   this array is passed back to the 'main' procedure. \n",
    "\n",
    "    \n",
    "    numBottomNodes = weightArraySizeList[0]\n",
    "    numUpperNodes = weightArraySizeList[1]\n",
    "\n",
    "# Initialize the weight variables with random weights    \n",
    "    weightArray = np.zeros((numUpperNodes,numBottomNodes))    # iniitalize the weight matrix with 0's\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numBottomNodes):  # number of columns in matrix 2\n",
    "            weightArray[row,col] = InitializeWeight ()\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the bias weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeBiasWeightArray (numBiasNodes):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "\n",
    "# Initialize the bias weight variables with random weights    \n",
    "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
    "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
    "        biasWeightArray[node] = InitializeWeight ()\n",
    "                  \n",
    "# Print the entire weights array. \n",
    "    print(biasWeightArray)\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to return a trainingDataList\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainSelectedAlphabetTrainingValues (trainingDataSetNum):\n",
    "    \n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainRandomAlphabetTrainingValues (numTrainingDataSets):\n",
    "\n",
    "   \n",
    "    # The training data list will have 11 values for the X-OR problem:\n",
    "    #   - First nine valuea will be the 5x5 pixel-grid representation of the letter\n",
    "    #       represented as a 1-D array (0 or 1 for each)\n",
    "    #   - Tenth value will be the output class (0 .. totalClasses - 1)\n",
    "    #   - Eleventh value will be the string associated with that class, e.g., 'X'\n",
    "    # We are starting with five letters in the training set: X, M, N, H, and A\n",
    "    # Thus there are five choices for training data, which we'll select on random basis\n",
    "      \n",
    "    trainingDataSetNum = random.randint(0, numTrainingDataSets)\n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)  \n",
    "\n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Perform a single feedforward pass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, \n",
    "biasHiddenWeightArray):\n",
    "\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
    "\n",
    "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
    "    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
    "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
    "\n",
    "#    print ' '\n",
    "#    print 'Back in ComputeSingleFeedforwardPass'\n",
    "#    print 'The activations for the hidden nodes are:'\n",
    "#    print '  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
    "\n",
    "                                                                                                    \n",
    "    return (hiddenArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to compute the output node activations, given the hidden node activations, the hidden-to\n",
    "#   output connection weights, and the output bias weights.\n",
    "# Function returns the array of output node activations.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, \n",
    "biasOutputWeightArray):\n",
    "\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
    "    outputArray = np.zeros(outputArrayLength)   \n",
    "\n",
    "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
    "    \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
    "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
    "                                                                                                   \n",
    "    return (outputArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to compute the output node activations and determine errors across the entire training\n",
    "#  data set, and print results.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
    "\n",
    "    selectedTrainingDataSet = 0\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]                               \n",
    "                              \n",
    "\n",
    "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "        inputDataList = [] \n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "\n",
    "        print(' ')\n",
    "        print('  Data Set Number', selectedTrainingDataSet, ' for letter ', trainingDataList[26] )\n",
    "\n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        print(' ')\n",
    "        print(' The hidden node activations are:')\n",
    "        print(hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "        print(' ')\n",
    "        print(' The output node activations are:')\n",
    "        print(outputArray )  \n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "        print( ' ')\n",
    "        print(' The desired output array values are: ')\n",
    "        print(desiredOutputArray ) \n",
    "       \n",
    "                        \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        print(' ')\n",
    "        print(' The error values are:')\n",
    "        print(errorArray)\n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "        print('New SSE = %.6f' % newSSE)\n",
    "         \n",
    "        selectedTrainingDataSet = selectedTrainingDataSet +1 \n",
    "        \n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "#   Backpropgation Section\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the hidden-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight v. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in the equations for the deltas in the connection weights    \n",
    "#    print ' '\n",
    "#    print ' The transfer function derivative is: '\n",
    "#    print transferFuncDerivArray\n",
    "                        \n",
    "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
    "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
    "        \n",
    "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes,\n",
    "        #    and the columns correspond to the number of hidden nodes,\n",
    "        #    which can be multiplied by the hidden node array (expressed as a column).\n",
    "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
    "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
    "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newVWeightArray\n",
    "\n",
    "#    PrintAndTraceBackpropagateOutputToHidden (alpha, nu, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newHiddenWeightArray)    \n",
    "                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "    return (newVWeightArray);     \n",
    "\n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "\n",
    "# Unpack the output array length\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
    "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
    "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
    "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
    "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
    "   \n",
    "#    print ' '\n",
    "#    print ' The previous biases for the output nodes are: '\n",
    "#    print biasOutputWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new biases for the output nodes are: '\n",
    "#    print newBiasOutputWeightArray\n",
    "                                                                                                                                                \n",
    "    return (newBiasOutputWeightArray);     \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the input-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the input-to-hidden wts w. \n",
    "# Core equation for the second part of backpropagation: \n",
    "# d(SSE)/dw(i,h) = -eta*alpha*F(h)(1-F(h))*Input(i)*sum(v(h,o)*Error(o))\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- w(i,h) is the connection weight w between the input node i and the hidden node h\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1 \n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# ---- NOTE: in this second step, the transfer function is applied to the output of the hidden node,\n",
    "# ------ so that F = F(h)\n",
    "# -- Hidden(h) = the output of hidden node h (used in computing the derivative of the transfer function). \n",
    "# -- Input(i) = the input at node i.\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight w. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    " \n",
    "# For the second step in backpropagation (computing deltas on the input-to-hidden weights)\n",
    "#   we need the transfer function derivative is applied to the output at the hidden node        \n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations       \n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    # initialize an array for the transfer function deriv \n",
    "      \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
    "        \n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
    "      \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "        \n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "             \n",
    "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
    "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
    "        \n",
    "    for row in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "\n",
    "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
    "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
    "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print wWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newWWeightArray    \n",
    "       \n",
    "                                                                    \n",
    "    return (newWWeightArray);     \n",
    "    \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "   \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence              \n",
    "\n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array    \n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array    \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  # initialize an array for the transfer function deriv \n",
    "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  # initialize an array for the partial derivative of the SSE\n",
    "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  # initialize an array for the deltas\n",
    "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  # initialize an array for the new hidden bias weights\n",
    "          \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
    "                  \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = (weightedErrorArray[hiddenNode] \n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode])\n",
    "            \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "\n",
    "# ===>>> AJM needs to double-check these equations in the comments area\n",
    "# ===>>> The code should be fine. \n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
    "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
    "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
    "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
    "  \n",
    "                                                                                                                                            \n",
    "    return (newBiasHiddenWeightArray); \n",
    "    \n",
    "            \n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Compute a feedforward pass in two steps\n",
    "#       - Randomly select a single training data set\n",
    "#       - Input-to-Hidden\n",
    "#       - Hidden-to-Output\n",
    "#       - Compute the error array\n",
    "#       - Compute the new Summed Squared Error (SSE)\n",
    "#   (5) Perform a single backpropagation training pass\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    welcome()\n",
    "    \n",
    "\n",
    "# Parameter definitions, to be replaced with user inputs\n",
    "    alpha = 1.0\n",
    "    eta = 0.5    \n",
    "    maxNumIterations = 5000    # temporarily set to 10 for testing\n",
    "    epsilon = 0.05\n",
    "    iteration = 0\n",
    "    SSE = 0.0\n",
    "    numTrainingDataSets = 4\n",
    "    \n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# Define the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Obtain the actual sizes for each layer of the network       \n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
    "    \n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers    \n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "                           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength        \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array\n",
    "    wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
    "    vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
    "\n",
    "# The bias weights are stored in a 1-D array         \n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "\n",
    "# Notice in the very beginning of the program, we have \n",
    "#   np.set_printoptions(precision=4) (sets number of dec. places in print)\n",
    "#     and 'np.set_printoptions(suppress=True)', which keeps it from printing in scientific format\n",
    "#   Debug print: \n",
    "#    print\n",
    "#    print 'The initial weights for this neural network are:'\n",
    "#    print '       Input-to-Hidden '\n",
    "#    print wWeightArray\n",
    "#    print '       Hidden-to-Output'\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print 'The initial bias weights for this neural network are:'\n",
    "#    print '        Hidden Bias = ', biasHiddenWeightArray                         \n",
    "#    print '        Output Bias = ', biasOutputWeightArray\n",
    "  \n",
    "\n",
    "          \n",
    "####################################################################################################\n",
    "# Before we start training, get a baseline set of outputs, errors, and SSE \n",
    "####################################################################################################                \n",
    "           \n",
    "    trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')                  \n",
    "    print(' ')\n",
    "    print('  Before training:')\n",
    "    \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray)                           \n",
    "                                             \n",
    "          \n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of randomly-selected training values for alpha-classification \n",
    "####################################################################################################                \n",
    "  \n",
    "  \n",
    "    while iteration < maxNumIterations:           \n",
    "\n",
    "# Increment the iteration count\n",
    "        iteration = iteration +1\n",
    "    \n",
    "# For any given pass, we re-initialize the training list\n",
    "        trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')\n",
    "\n",
    "        inputDataList = []                                \n",
    "                                                                                          \n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        trainingDataList = obtainRandomAlphabetTrainingValues (numTrainingDataSets) \n",
    "\n",
    "\n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "        \n",
    "#        print ' '\n",
    "#        print '  The input data list for letter', trainingDataList[26], ' is:'\n",
    "#        print inputDataList[0], inputDataList[1], inputDataList[2], inputDataList[3], inputDataList[4]\n",
    "#        print inputDataList[5], inputDataList[6], inputDataList[7], inputDataList[8], inputDataList[9]          \n",
    "#        print inputDataList[10], inputDataList[11], inputDataList[12], inputDataList[13], inputDataList[14]\n",
    "#        print inputDataList[15], inputDataList[16], inputDataList[17], inputDataList[18], inputDataList[19]        \n",
    "#        print inputDataList[20], inputDataList[21], inputDataList[22], inputDataList[23], inputDataList[24]\n",
    "#        print ' '\n",
    "    \n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "#        print ' '\n",
    "#        print ' The desired output array values are: '\n",
    "#        print desiredOutputArray  \n",
    "#        print ' '\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "####################################################################################################                \n",
    "                \n",
    "                           \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#        print ' '\n",
    "#        print ' The hidden node activations are:'\n",
    "#        print hiddenArray\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#        print ' '\n",
    "#        print ' The output node activations are:'\n",
    " #       print outputArray    \n",
    "\n",
    "#  Optional alternative code for later use:\n",
    "#  Assign the hidden and output values to specific different variables\n",
    "#    for node in range(hiddenArrayLength):    \n",
    "#        actualHiddenOutput[node] = actualAllNodesOutputList [node]\n",
    "    \n",
    "#    for node in range(outputArrayLength):    \n",
    "#        actualOutput[node] = actualAllNodesOutputList [hiddenArrayLength + node]\n",
    " \n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "# Determine the error between actual and desired outputs        \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print ' '\n",
    "#        print ' The error values are:'\n",
    "#        print errorArray   \n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print 'Initial SSE = %.6f' % newSSE\n",
    "#        SSE = newSSE\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Perform backpropagation\n",
    "####################################################################################################                \n",
    "                \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes    \n",
    "        newVWeightArray = BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray)\n",
    "        newBiasOutputWeightArray = BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray) \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes       \n",
    "        newWWeightArray = BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "\n",
    "        newBiasHiddenWeightArray = BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)  \n",
    "    \n",
    "                    \n",
    "# Assign new values to the weight matrices\n",
    "# Assign the old hidden-to-output weight array to be the same as what was returned from the BP weight update\n",
    "        vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "        biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
    "    \n",
    "# Assign the old input-to-hidden weight array to be the same as what was returned from the BP weight update\n",
    "        wWeightArray = newWWeightArray[:]  \n",
    "    \n",
    "        biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
    "    \n",
    "# Compute a forward pass, test the new SSE                                                                                \n",
    "                                                                                                                                    \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#    print ' '\n",
    "#    print ' The hidden node activations are:'\n",
    "#    print hiddenArray\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#    print ' '\n",
    "#    print ' The output node activations are:'\n",
    "#    print outputArray    \n",
    "\n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print ' '\n",
    "#        print ' The error values are:'\n",
    "#        print errorArray   \n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print 'Previous SSE = %.6f' % SSE\n",
    "#        print 'New SSE = %.6f' % newSSE \n",
    "    \n",
    "#        print ' '\n",
    "#        print 'Iteration number ', iteration\n",
    "#        iteration = iteration + 1\n",
    "\n",
    "        if newSSE < epsilon:\n",
    "\n",
    "            \n",
    "            break\n",
    "    print('Out of while loop at iteration ', iteration)\n",
    "    \n",
    "####################################################################################################\n",
    "# After training, get a new comparative set of outputs, errors, and SSE \n",
    "####################################################################################################                           \n",
    "\n",
    "    print(' ')\n",
    "    print('  After training:')                  \n",
    "                                                      \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray) \n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "                                              \n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "####################################################################################################                \n",
    "    \n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "####################################################################################################\n",
    "# End program\n",
    "#################################################################################################### \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
