{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:5000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:5000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "trained using the backpropagation method.\n",
      "Version 0.4, 03/05/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      "\n",
      "This program learns to distinguish between broad classes of capital letters\n",
      "It allows users to examine the hidden weights to identify learned features\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "\n",
      "  The number of nodes at each level are:\n",
      "    Input: 9x9 (square array)\n",
      "    Hidden:  6\n",
      "    Output:  9\n",
      "\n",
      " inputArrayLength =  81\n",
      " hiddenArrayLength =  6\n",
      " outputArrayLength =  9\n",
      "\n",
      "  Before training:\n",
      "\n",
      " the selected Training Data Set is  1\n",
      "\n",
      "  Data Set Number 1  for letter  A  with letter number  1\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.023 0.001 0.094 0.533 0.881 0.927]\n",
      "\n",
      " The output node activations are: \n",
      "[0.902 0.215 0.844 0.301 0.76  0.463 0.371 0.661 0.354]\n",
      "\n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[ 0.098 -0.215 -0.844 -0.301 -0.76  -0.463 -0.371 -0.661 -0.354]\n",
      "New SSE = 2.351987\n",
      "\n",
      " the selected Training Data Set is  2\n",
      "\n",
      "  Data Set Number 2  for letter  B  with letter number  2\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.07  0.044 0.044 0.984 0.828 0.03 ]\n",
      "\n",
      " The output node activations are: \n",
      "[0.812 0.181 0.67  0.406 0.881 0.531 0.567 0.804 0.442]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.812  0.819 -0.67  -0.406 -0.881 -0.531 -0.567 -0.804 -0.442]\n",
      "New SSE = 4.165662\n",
      "\n",
      " the selected Training Data Set is  3\n",
      "\n",
      "  Data Set Number 3  for letter  C  with letter number  3\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.533 0.885 0.011 0.996 0.23  0.003]\n",
      "\n",
      " The output node activations are: \n",
      "[0.68  0.092 0.719 0.355 0.831 0.482 0.552 0.492 0.385]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.68  -0.092  0.281 -0.355 -0.831 -0.482 -0.552 -0.492 -0.385]\n",
      "New SSE = 2.294129\n",
      "\n",
      " the selected Training Data Set is  4\n",
      "\n",
      "  Data Set Number 4  for letter  D  with letter number  4\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.379 0.136 0.05  0.919 0.282 0.017]\n",
      "\n",
      " The output node activations are: \n",
      "[0.771 0.139 0.621 0.449 0.837 0.542 0.459 0.65  0.405]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.771 -0.139 -0.621  0.551 -0.837 -0.542 -0.459 -0.65  -0.405]\n",
      "New SSE = 3.094984\n",
      "\n",
      " the selected Training Data Set is  5\n",
      "\n",
      "  Data Set Number 5  for letter  E  with letter number  5\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.452 0.396 0.023 0.997 0.121 0.026]\n",
      "\n",
      " The output node activations are: \n",
      "[0.738 0.113 0.641 0.451 0.819 0.538 0.465 0.57  0.367]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.738 -0.113 -0.641 -0.451  0.181 -0.538 -0.465 -0.57  -0.367]\n",
      "New SSE = 2.170779\n",
      "\n",
      " the selected Training Data Set is  6\n",
      "\n",
      "  Data Set Number 6  for letter  F  with letter number  6\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.468 0.037 0.153 0.959 0.024 0.231]\n",
      "\n",
      " The output node activations are: \n",
      "[0.821 0.121 0.63  0.51  0.804 0.552 0.339 0.553 0.346]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.821 -0.121 -0.63  -0.51   0.196 -0.552 -0.339 -0.553 -0.346]\n",
      "New SSE = 2.229041\n",
      "\n",
      " the selected Training Data Set is  7\n",
      "\n",
      "  Data Set Number 7  for letter  G  with letter number  7\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.369 0.8   0.013 0.949 0.671 0.002]\n",
      "\n",
      " The output node activations are: \n",
      "[0.713 0.115 0.752 0.296 0.869 0.457 0.628 0.624 0.442]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.713  0.885 -0.752 -0.296 -0.869 -0.457 -0.628 -0.624 -0.442]\n",
      "New SSE = 3.887774\n",
      "\n",
      " the selected Training Data Set is  8\n",
      "\n",
      "  Data Set Number 8  for letter  H  with letter number  8\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.142 0.001 0.811 0.441 0.212 0.277]\n",
      "\n",
      " The output node activations are: \n",
      "[0.887 0.231 0.752 0.52  0.835 0.509 0.256 0.433 0.52 ]\n",
      "\n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[ 0.113 -0.231 -0.752 -0.52  -0.835 -0.509 -0.256 -0.433 -0.52 ]\n",
      "New SSE = 2.381481\n",
      "\n",
      " the selected Training Data Set is  9\n",
      "\n",
      "  Data Set Number 9  for letter  I  with letter number  9\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.923 0.763 0.048 0.976 0.403 0.008]\n",
      "\n",
      " The output node activations are: \n",
      "[0.708 0.068 0.711 0.266 0.882 0.41  0.538 0.481 0.45 ]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.708 -0.068 -0.711 -0.266 -0.882  0.59  -0.538 -0.481 -0.45 ]\n",
      "New SSE = 2.932181\n",
      "\n",
      " the selected Training Data Set is  10\n",
      "\n",
      "  Data Set Number 10  for letter  J  with letter number  10\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.63  0.335 0.126 0.995 0.352 0.735]\n",
      "\n",
      " The output node activations are: \n",
      "[0.871 0.085 0.789 0.348 0.801 0.448 0.337 0.476 0.297]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.871 -0.085 -0.789 -0.348 -0.801  0.552 -0.337 -0.476 -0.297]\n",
      "New SSE = 2.884400\n",
      "\n",
      " the selected Training Data Set is  11\n",
      "\n",
      "  Data Set Number 11  for letter  J  with letter number  10\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.63  0.335 0.126 0.995 0.352 0.735]\n",
      "\n",
      " The output node activations are: \n",
      "[0.871 0.085 0.789 0.348 0.801 0.448 0.337 0.476 0.297]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.871 -0.085 -0.789 -0.348 -0.801  0.552 -0.337 -0.476 -0.297]\n",
      "New SSE = 2.884400\n",
      "\n",
      " the selected Training Data Set is  12\n",
      "\n",
      "  Data Set Number 12  for letter  L  with letter number  12\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.496 0.809 0.23  0.932 0.749 0.014]\n",
      "\n",
      " The output node activations are: \n",
      "[0.756 0.105 0.776 0.276 0.9   0.415 0.597 0.571 0.496]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.756 -0.105 -0.776 -0.276 -0.9   -0.415 -0.597  0.429 -0.496]\n",
      "New SSE = 3.030568\n",
      "\n",
      " the selected Training Data Set is  13\n",
      "\n",
      "  Data Set Number 13  for letter  M  with letter number  13\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.822 0.009 0.051 0.644 0.239 0.387]\n",
      "\n",
      " The output node activations are: \n",
      "[0.817 0.11  0.677 0.326 0.808 0.457 0.331 0.502 0.412]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.817 -0.11  -0.677 -0.326 -0.808 -0.457 -0.331 -0.502  0.588]\n",
      "New SSE = 2.814286\n",
      "\n",
      " the selected Training Data Set is  14\n",
      "\n",
      "  Data Set Number 14  for letter  N  with letter number  14\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.31  0.009 0.806 0.149 0.168 0.019]\n",
      "\n",
      " The output node activations are: \n",
      "[0.84  0.256 0.712 0.458 0.845 0.485 0.275 0.394 0.629]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.84  -0.256 -0.712 -0.458 -0.845 -0.485 -0.275 -0.394  0.371]\n",
      "New SSE = 2.805346\n",
      "\n",
      " the selected Training Data Set is  15\n",
      "\n",
      "  Data Set Number 15  for letter  O  with letter number  15\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.187 0.667 0.06  0.877 0.074 0.096]\n",
      "\n",
      " The output node activations are: \n",
      "[0.718 0.141 0.712 0.464 0.767 0.552 0.482 0.513 0.348]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.718 -0.141 -0.712  0.536 -0.767 -0.552 -0.482 -0.513 -0.348]\n",
      "New SSE = 2.838462\n",
      "\n",
      " the selected Training Data Set is  16\n",
      "\n",
      "  Data Set Number 16  for letter  P  with letter number  16\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.17  0.004 0.227 0.963 0.026 0.415]\n",
      "\n",
      " The output node activations are: \n",
      "[0.858 0.152 0.681 0.573 0.764 0.584 0.307 0.564 0.301]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.858  0.848 -0.681 -0.573 -0.764 -0.584 -0.307 -0.564 -0.301]\n",
      "New SSE = 3.675791\n",
      "\n",
      "\n",
      "hiddenNodeActivationsList L-{letter} C-{class}\n",
      "==============================================\n",
      "==============================================\n",
      "\n",
      "    L-A C-A   L-B C-B   L-C C-C   L-D C-O   L-E C-E   L-F C-E   L-G C-C  \\\n",
      "0  0.023077  0.069857  0.532784  0.378628  0.452020  0.468325  0.368662   \n",
      "1  0.000541  0.043503  0.884850  0.136230  0.396403  0.037198  0.800188   \n",
      "2  0.093716  0.044400  0.010605  0.050051  0.022890  0.152733  0.012501   \n",
      "3  0.532927  0.983732  0.995651  0.919124  0.997451  0.959443  0.949128   \n",
      "4  0.880719  0.827517  0.230464  0.282364  0.120806  0.023794  0.670761   \n",
      "5  0.926696  0.029814  0.003205  0.016534  0.026325  0.231461  0.001569   \n",
      "\n",
      "    L-H C-A   L-I C-I   L-J C-I   L-J C-I   L-L C-L   L-M C-M   L-N C-M  \\\n",
      "0  0.142472  0.922591  0.629570  0.629570  0.495711  0.822147  0.309769   \n",
      "1  0.001219  0.763438  0.334788  0.334788  0.809018  0.008678  0.009357   \n",
      "2  0.811260  0.048453  0.125820  0.125820  0.229591  0.050510  0.805526   \n",
      "3  0.441392  0.976283  0.995110  0.995110  0.932447  0.643727  0.148572   \n",
      "4  0.211834  0.403163  0.352292  0.352292  0.748574  0.238909  0.168201   \n",
      "5  0.276893  0.008269  0.734879  0.734879  0.013625  0.387126  0.018776   \n",
      "\n",
      "    L-O C-O   L-P C-B  \n",
      "0  0.186564  0.169676  \n",
      "1  0.666744  0.004201  \n",
      "2  0.060184  0.226861  \n",
      "3  0.877028  0.962923  \n",
      "4  0.073602  0.025987  \n",
      "5  0.095947  0.414769  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of while loop at iteration  2476\n",
      "\n",
      "  After training:\n",
      "\n",
      " the selected Training Data Set is  1\n",
      "\n",
      "  Data Set Number 1  for letter  A  with letter number  1\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.682 0.001 0.982 0.48  1.    0.999]\n",
      "\n",
      " The output node activations are: \n",
      "[0.918 0.106 0.005 0.002 0.023 0.004 0.005 0.031 0.057]\n",
      "\n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[ 0.082 -0.106 -0.005 -0.002 -0.023 -0.004 -0.005 -0.031 -0.057]\n",
      "New SSE = 0.022959\n",
      "\n",
      " the selected Training Data Set is  2\n",
      "\n",
      "  Data Set Number 2  for letter  B  with letter number  2\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.482 0.008 0.073 0.998 0.987 0.022]\n",
      "\n",
      " The output node activations are: \n",
      "[0.06  0.916 0.027 0.055 0.16  0.009 0.021 0.093 0.018]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.06   0.084 -0.027 -0.055 -0.16  -0.009 -0.021 -0.093 -0.018]\n",
      "New SSE = 0.049188\n",
      "\n",
      " the selected Training Data Set is  3\n",
      "\n",
      "  Data Set Number 3  for letter  C  with letter number  3\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.476 0.973 0.011 0.997 0.603 0.   ]\n",
      "\n",
      " The output node activations are: \n",
      "[0.002 0.051 0.128 0.019 0.447 0.19  0.016 0.092 0.006]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.002 -0.051  0.872 -0.019 -0.447 -0.19  -0.016 -0.092 -0.006]\n",
      "New SSE = 1.007866\n",
      "\n",
      " the selected Training Data Set is  4\n",
      "\n",
      "  Data Set Number 4  for letter  D  with letter number  4\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.952 0.001 0.255 0.978 0.004 0.001]\n",
      "\n",
      " The output node activations are: \n",
      "[0.006 0.113 0.013 0.86  0.051 0.021 0.007 0.003 0.049]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.006 -0.113 -0.013  0.14  -0.051 -0.021 -0.007 -0.003 -0.049]\n",
      "New SSE = 0.038068\n",
      "\n",
      " the selected Training Data Set is  5\n",
      "\n",
      "  Data Set Number 5  for letter  E  with letter number  5\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.939 0.989 0.025 1.    0.99  0.006]\n",
      "\n",
      " The output node activations are: \n",
      "[0.002 0.089 0.126 0.006 0.792 0.013 0.009 0.052 0.009]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.002 -0.089 -0.126 -0.006  0.208 -0.013 -0.009 -0.052 -0.009]\n",
      "New SSE = 0.070381\n",
      "\n",
      " the selected Training Data Set is  6\n",
      "\n",
      "  Data Set Number 6  for letter  F  with letter number  6\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.998 0.912 0.04  0.994 0.991 0.054]\n",
      "\n",
      " The output node activations are: \n",
      "[0.002 0.102 0.102 0.007 0.765 0.01  0.008 0.039 0.009]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.002 -0.102 -0.102 -0.007  0.235 -0.01  -0.008 -0.039 -0.009]\n",
      "New SSE = 0.077646\n",
      "\n",
      " the selected Training Data Set is  7\n",
      "\n",
      "  Data Set Number 7  for letter  G  with letter number  7\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.328 0.039 0.086 0.978 0.825 0.   ]\n",
      "\n",
      " The output node activations are: \n",
      "[0.058 0.868 0.029 0.079 0.105 0.025 0.026 0.113 0.018]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.058  0.132 -0.029 -0.079 -0.105 -0.025 -0.026 -0.113 -0.018]\n",
      "New SSE = 0.053372\n",
      "\n",
      " the selected Training Data Set is  8\n",
      "\n",
      "  Data Set Number 8  for letter  H  with letter number  8\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.962 0.    0.995 0.462 0.998 0.983]\n",
      "\n",
      " The output node activations are: \n",
      "[0.856 0.073 0.004 0.002 0.037 0.001 0.004 0.013 0.084]\n",
      "\n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[ 0.144 -0.073 -0.004 -0.002 -0.037 -0.001 -0.004 -0.013 -0.084]\n",
      "New SSE = 0.034615\n",
      "\n",
      " the selected Training Data Set is  9\n",
      "\n",
      "  Data Set Number 9  for letter  I  with letter number  9\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.038 0.987 0.017 0.994 0.014 0.   ]\n",
      "\n",
      " The output node activations are: \n",
      "[0.002 0.012 0.122 0.114 0.111 0.874 0.025 0.099 0.004]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.002 -0.012 -0.122 -0.114 -0.111  0.126 -0.025 -0.099 -0.004]\n",
      "New SSE = 0.066926\n",
      "\n",
      " the selected Training Data Set is  10\n",
      "\n",
      "  Data Set Number 10  for letter  J  with letter number  10\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.055 0.974 0.069 0.996 0.016 0.905]\n",
      "\n",
      " The output node activations are: \n",
      "[0.062 0.003 0.035 0.02  0.027 0.948 0.01  0.025 0.   ]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.062 -0.003 -0.035 -0.02  -0.027  0.052 -0.01  -0.025 -0.   ]\n",
      "New SSE = 0.009627\n",
      "\n",
      " the selected Training Data Set is  11\n",
      "\n",
      "  Data Set Number 11  for letter  J  with letter number  10\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.055 0.974 0.069 0.996 0.016 0.905]\n",
      "\n",
      " The output node activations are: \n",
      "[0.062 0.003 0.035 0.02  0.027 0.948 0.01  0.025 0.   ]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.062 -0.003 -0.035 -0.02  -0.027  0.052 -0.01  -0.025 -0.   ]\n",
      "New SSE = 0.009627\n",
      "\n",
      " the selected Training Data Set is  12\n",
      "\n",
      "  Data Set Number 12  for letter  L  with letter number  12\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.023 0.879 0.876 0.762 0.957 0.008]\n",
      "\n",
      " The output node activations are: \n",
      "[0.073 0.039 0.074 0.001 0.163 0.112 0.022 0.62  0.04 ]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.073 -0.039 -0.074 -0.001 -0.163 -0.112 -0.022  0.38  -0.04 ]\n",
      "New SSE = 0.198015\n",
      "\n",
      " the selected Training Data Set is  13\n",
      "\n",
      "  Data Set Number 13  for letter  M  with letter number  13\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.994 0.    0.99  0.008 0.422 0.044]\n",
      "\n",
      " The output node activations are: \n",
      "[0.113 0.023 0.023 0.083 0.082 0.002 0.013 0.024 0.893]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.113 -0.023 -0.023 -0.083 -0.082 -0.002 -0.013 -0.024  0.107]\n",
      "New SSE = 0.039721\n",
      "\n",
      " the selected Training Data Set is  14\n",
      "\n",
      "  Data Set Number 14  for letter  N  with letter number  14\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.976 0.    0.998 0.015 0.43  0.033]\n",
      "\n",
      " The output node activations are: \n",
      "[0.115 0.024 0.023 0.081 0.081 0.002 0.014 0.026 0.892]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.115 -0.024 -0.023 -0.081 -0.081 -0.002 -0.014 -0.026  0.108]\n",
      "New SSE = 0.039851\n",
      "\n",
      " the selected Training Data Set is  15\n",
      "\n",
      "  Data Set Number 15  for letter  O  with letter number  15\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.937 0.008 0.301 0.972 0.001 0.004]\n",
      "\n",
      " The output node activations are: \n",
      "[0.007 0.099 0.012 0.849 0.046 0.022 0.007 0.003 0.052]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.007 -0.099 -0.012  0.151 -0.046 -0.022 -0.007 -0.003 -0.052]\n",
      "New SSE = 0.038240\n",
      "\n",
      " the selected Training Data Set is  16\n",
      "\n",
      "  Data Set Number 16  for letter  P  with letter number  16\n",
      "\n",
      " The hidden node activations are: \n",
      "[0.998 0.066 0.038 0.996 0.971 0.22 ]\n",
      "\n",
      " The output node activations are: \n",
      "[0.032 0.769 0.02  0.067 0.284 0.002 0.009 0.014 0.015]\n",
      "\n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " ' The error values are:\n",
      "[-0.032  0.231 -0.02  -0.067 -0.284 -0.002 -0.009 -0.014 -0.015]\n",
      "New SSE = 0.140517\n",
      "\n",
      "\n",
      "hiddenNodeActivationsList L-{letter} C-{class}\n",
      "==============================================\n",
      "==============================================\n",
      "\n",
      "    L-A C-A   L-B C-B   L-C C-C   L-D C-O   L-E C-E   L-F C-E   L-G C-C  \\\n",
      "0  0.681861  0.482211  0.476344  0.951726  0.939373  0.998142  0.328430   \n",
      "1  0.000866  0.007834  0.973073  0.001289  0.988825  0.912108  0.038896   \n",
      "2  0.982024  0.072767  0.010716  0.254647  0.025131  0.039958  0.086452   \n",
      "3  0.480288  0.998432  0.997471  0.978478  0.999592  0.994307  0.978078   \n",
      "4  0.999714  0.986547  0.602829  0.004433  0.989838  0.991080  0.825289   \n",
      "5  0.999186  0.022013  0.000036  0.000822  0.005918  0.053519  0.000113   \n",
      "\n",
      "    L-H C-A   L-I C-I   L-J C-I   L-J C-I   L-L C-L   L-M C-M   L-N C-M  \\\n",
      "0  0.961875  0.037633  0.055302  0.055302  0.023396  0.993997  0.976150   \n",
      "1  0.000134  0.987342  0.973767  0.973767  0.878508  0.000209  0.000082   \n",
      "2  0.995300  0.017474  0.068678  0.068678  0.875938  0.990233  0.997858   \n",
      "3  0.461512  0.994078  0.995631  0.995631  0.762268  0.008033  0.014590   \n",
      "4  0.998083  0.014362  0.015759  0.015759  0.957026  0.421919  0.429542   \n",
      "5  0.983219  0.000481  0.904624  0.904624  0.007892  0.043541  0.033153   \n",
      "\n",
      "    L-O C-O   L-P C-B  \n",
      "0  0.936674  0.997689  \n",
      "1  0.008146  0.065702  \n",
      "2  0.301453  0.037615  \n",
      "3  0.972219  0.996345  \n",
      "4  0.000546  0.970640  \n",
      "5  0.003842  0.219749  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datafiles/GB1wWeightFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-823eafd668cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[1;31m####################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[1;31m####################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-823eafd668cc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[1;31m# Write the input-to-hidden connection weights to a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m     \u001b[0mwWeightFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datafiles/GB1wWeightFile'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwWeightList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datafiles/GB1wWeightFile'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "\n",
    "# ################################################################################################\n",
    "#\n",
    "#\n",
    "#  9x9 Module 4\n",
    "#\n",
    "#\n",
    "# ################################################################################################\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# So we can make a separate list from an initial one\n",
    "import copy\n",
    "\n",
    "# import packages for heatmap associated generation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    " \n",
    "\n",
    "# For pretty-printing the arrays\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# This is a tutorial program, designed for those who are learning Python, and specifically using \n",
    "#   Python for neural networks applications\n",
    "#\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Code Map: List of Procedures / Functions\n",
    "# - welcome\n",
    "#\n",
    "# == set of basic functions ==\n",
    "# - computeTransferFnctn\n",
    "# - computeTransferFnctnDeriv\n",
    "# - matrixDotProduct\n",
    "#\n",
    "# == identify crucial parameters (these can be changed by the user)\n",
    "#    obtainNeuralNetworkSizeSpecs\n",
    "#\n",
    "#    -- initializeWeight\n",
    "# - initializeWeightArray\n",
    "# - initializeBiasWeightArray\n",
    "#\n",
    "# == obtain the training data (two possible routes; user selection & random)\n",
    "# - obtainSelectedAlphabetTrainingValues\n",
    "# - obtainRandomAlphabetTrainingValues\n",
    "#\n",
    "# == the feedforward modules\n",
    "#   -- ComputeSingleFeedforwardPassFirstStep\n",
    "#   -- ComputeSingleFeedforwardPassSecondStep\n",
    "# - ComputeOutputsAcrossAllTrainingData\n",
    "#\n",
    "# == the backpropagation training modules\n",
    "# - backpropagateOutputToHidden\n",
    "# - backpropagateBiasOutputWeights\n",
    "# - backpropagateHiddenToInput\n",
    "# - backpropagateBiasHiddenWeights\n",
    "# - main\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to welcome the user and identify the code\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def welcome ():\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"******************************************************************************\")\n",
    "    print()\n",
    "    print(\"Welcome to the Multilayer Perceptron Neural Network\")\n",
    "    print(\"trained using the backpropagation method.\")\n",
    "    print(\"Version 0.4, 03/05/2017, A.J. Maren\")\n",
    "    print(\"For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\")\n",
    "    print() \n",
    "    print(\"This program learns to distinguish between broad classes of capital letters\")\n",
    "    print(\"It allows users to examine the hidden weights to identify learned features\")\n",
    "    print()\n",
    "    print(\"******************************************************************************\")\n",
    "    print()\n",
    "    return()\n",
    "\n",
    "        \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# A collection of worker-functions, designed to do specific small tasks\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "#------------------------------------------------------#    \n",
    "\n",
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
    "    return activation\n",
    "  \n",
    "\n",
    "#------------------------------------------------------# \n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "\n",
    "#------------------------------------------------------# \n",
    "def matrixDotProduct (matrx1,matrx2):\n",
    "    dotProduct = np.dot(matrx1,matrx2)\n",
    "    \n",
    "    return(dotProduct)    \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to obtain the neural network size specifications\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "\n",
    "# This procedure operates as a function, as it returns a single value (which really is a list of \n",
    "#    three values). It is called directly from 'main.'\n",
    "#        \n",
    "# This procedure allows the user to specify the size of the input (I), hidden (H), \n",
    "#    and output (O) layers.  \n",
    "# These values will be stored in a list, the arraySizeList. \n",
    "# This list will be used to specify the sizes of two different weight arrays:\n",
    "#   - wWeights; the Input-to-Hidden array, and\n",
    "#   - vWeights; the Hidden-to-Output array. \n",
    "# However, even though we're calling this procedure, we will still hard-code the array sizes for now.   \n",
    "\n",
    "    numInputNodes = 81\n",
    "    numHiddenNodes = 6\n",
    "    numOutputNodes = 9   \n",
    "    print()\n",
    "    print(\"  The number of nodes at each level are:\")\n",
    "    print(\"    Input: 9x9 (square array)\")\n",
    "    print(\"    Hidden: \", numHiddenNodes)\n",
    "    print(\"    Output: \", numOutputNodes)\n",
    "            \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def InitializeWeight ():\n",
    "\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "#    print weight\n",
    "           \n",
    "    return (weight)  \n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the node-to-node connection weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeWeightArray (weightArraySizeList):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "#        \n",
    "# This procedure takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be assigned random values here, and \n",
    "#   this array is passed back to the 'main' procedure. \n",
    "\n",
    "    \n",
    "    numLowerNodes = weightArraySizeList[0] \n",
    "    numUpperNodes = weightArraySizeList[1] \n",
    "    \n",
    "#    print ' '\n",
    "#    print ' inside procedure initializeWeightArray'\n",
    "#    print ' the number of lower nodes is', numLowerNodes\n",
    "#    print ' the number of upper nodes is', numUpperNodes    \n",
    "#\n",
    "# Initialize the weight variables with random weights    \n",
    "    weightArray = np.zeros((numUpperNodes,numLowerNodes))    # iniitalize the weight matrix with 0's\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numLowerNodes):  # number of columns in matrix 2\n",
    "            weightArray[row,col] = InitializeWeight ()\n",
    "            \n",
    "#    print weightArray                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the bias weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeBiasWeightArray (numBiasNodes):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "\n",
    "# Initialize the bias weight variables with random weights    \n",
    "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
    "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
    "        biasWeightArray[node] = InitializeWeight ()\n",
    "                  \n",
    "# Print the entire weights array. \n",
    "#    print biasWeightArray\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to return a trainingDataList\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainSelectedAlphabetTrainingValues (dataSet):\n",
    "    \n",
    "# Note: Nine possible output classes: 0 .. 8 trainingDataListXX [4]    \n",
    "    trainingDataListA0 =  (1,[0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0, 0,1,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1],1,'A',0,'A') # training data list 1 selected for the letter 'A'\n",
    "    trainingDataListB0 =  (2,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,0,0, 1,0,0,0,0,0,0,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,0],2,'B',1,'B') # training data list 2, letter 'E', courtesy AJM\n",
    "    trainingDataListC0 =  (3,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],3,'C',2,'C') # training data list 3, letter 'C', courtesy PKVR\n",
    "    trainingDataListD0 =  (4,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,1, 1,1,1,1,1,1,1,1,0],4,'D',3,'O') # training data list 4, letter 'D', courtesy TD\n",
    "    trainingDataListE0 =  (5,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],5,'E',4,'E') # training data list 5, letter 'E', courtesy BMcD \n",
    "    trainingDataListF0 =  (6,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0],6,'F',4,'E') # training data list 6, letter 'F', courtesy SK\n",
    "    trainingDataListG0 =  (7,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1],7,'G',1,'C')\n",
    "    trainingDataListH0 =  (8,[1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1],8,'H',0,'A') # training data list 8, letter 'H', courtesy JC\n",
    "    trainingDataListI0 =  (9,[0,0,1,1,1,1,1,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,1,1,1,1,1,0,0],9,'I',5,'I') # training data list 9, letter 'I', courtesy GR\n",
    "    trainingDataListJ0 = (10,[0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1,0, 0,0,1,0,0,0,1,0,0, 0,0,0,1,1,1,0,0,0],10,'J',5,'I') # training data list 10 selected for the letter 'L', courtesy JT\n",
    "    trainingDataListK0 = (11,[1,0,0,0,0,0,1,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,1,0,0,0,0, 1,0,0,1,0,0,0,0,0, 1,1,1,0,0,0,0,0,0, 1,0,0,1,0,0,0,0,0, 1,0,0,0,1,0,0,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,0,0,1,0,0],11,'K',6,'K') # training data list 11 selected for the letter 'K', courtesy EO      \n",
    "    trainingDataListL0 = (12,[1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],12,'L',7,'L') # training data list 12 selected for the letter 'L', courtesy PV\n",
    "    trainingDataListM0 = (13,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,1,1, 1,1,0,0,0,0,0,1,1, 1,0,1,0,0,0,1,0,1, 1,0,1,0,0,0,1,0,1, 1,0,0,1,0,1,0,0,1, 1,0,0,1,0,1,0,0,1, 1,1,0,0,1,0,0,0,1, 1,0,0,0,1,0,0,0,1],13,'M',8,'M') # training data list 13 selected for the letter 'M', courtesy GR            \n",
    "    trainingDataListN0 = (14,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,0,1, 1,0,1,0,0,0,0,0,1, 1,0,0,1,0,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,0,0,0,0,1,0,0,1, 1,0,0,0,0,0,1,0,1, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1],14,'N',8,'M') # training data list 14 selected for the letter 'N'\n",
    "    trainingDataListO0 = (15,[0,1,1,1,1,1,1,1,0, 1,1,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 0,1,1,1,1,1,1,1,0],15,'O',3,'O') # training data list 15, letter 'O', courtesy TD\n",
    "    trainingDataListP0 = (16,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0],16,'P',1, 'B') # training data list 16, letter 'P', courtesy MT \n",
    "\n",
    "#    trainingDataListQ0 = (17,[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1],17,'Q') # training data list 17, letter 'Q', courtesy RG (square corners)\n",
    "\n",
    "\n",
    "#    trainingDataListT0 = (20,[0,1,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0],20,'T') # training data list 20, letter 'T', courtesy JR\n",
    "\n",
    "\n",
    "#    trainingDataListW0 = (23, [1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0],23,'W') # training data list 23, letter 'W', courtesy KW\n",
    "#    trainingDataListX0 = (24,[1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1],24,'X') # training data list 24, letter 'X', courtesy JD\n",
    "#\n",
    "#    trainingDataListZ0 = (26,[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,],26,'Z') # training data list 26, letter 'Z', courtesy ZW\n",
    "                      \n",
    "    if dataSet == 1: trainingDataList = trainingDataListA0\n",
    "    if dataSet == 2: trainingDataList = trainingDataListB0 \n",
    "    if dataSet == 3: trainingDataList = trainingDataListC0\n",
    "    if dataSet == 4: trainingDataList = trainingDataListD0     \n",
    "    if dataSet == 5: trainingDataList = trainingDataListE0\n",
    "    if dataSet == 6: trainingDataList = trainingDataListF0 \n",
    "    if dataSet == 7: trainingDataList = trainingDataListG0 \n",
    "    if dataSet == 8: trainingDataList = trainingDataListH0\n",
    "    if dataSet == 9: trainingDataList = trainingDataListI0\n",
    "    if dataSet == 10: trainingDataList = trainingDataListJ0    \n",
    "\n",
    "    if dataSet == 11: trainingDataList = trainingDataListJ0 # needs to be replaced w/ K            \n",
    "\n",
    "    if dataSet == 12: trainingDataList = trainingDataListL0\n",
    "    if dataSet == 13: trainingDataList = trainingDataListM0\n",
    "    if dataSet == 14: trainingDataList = trainingDataListN0 \n",
    "    if dataSet == 15: trainingDataList = trainingDataListO0 \n",
    "    if dataSet == 16: trainingDataList = trainingDataListP0  \n",
    "\n",
    "#    if dataSet == 17: trainingDataList = trainingDataListQ0\n",
    "#     if dataSet == 18: trainingDataList = trainingDataListQ0 # needs to be replaced w/ R \n",
    "#     if dataSet == 19: trainingDataList = trainingDataListQ0 # needs to be replaced w/ S \n",
    "        \n",
    "#     if dataSet == 20: trainingDataList = trainingDataListT0                            \n",
    "    return (trainingDataList)      \n",
    "\n",
    "   \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainRandomAlphabetTrainingValues (numTrainingDataSets):\n",
    "\n",
    "   \n",
    "    # The training data list will have the  values for the X-OR problem:\n",
    "    #   - First 81 valuea will be the 9x9 pixel-grid representation of the letter\n",
    "    #       represented as a 1-D array (0 or 1 for each)\n",
    "    #   - 82nd value will be the output class (0 .. totalClasses - 1)\n",
    "    #   - 83rd value will be the string associated with that class, e.g., 'X'\n",
    "    # We are starting with five letters in the training set: X, M, N, H, and A\n",
    "    # Thus there are five choices for training data, which we'll select on random basis\n",
    "      \n",
    "    dataSet = random.randint(0, numTrainingDataSets)\n",
    "#    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,0,0,0,0,1, 0,1,0,0,0,0,0,1,0, 0,0,1,0,0,0,1,0,0, 0,0,0,1,0,1,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0, 0,1,0,0,0,0,0,1,0, 1,0,0,0,0,0,0,0,1, 0,'X')\n",
    "#    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,1,1, 1,1,0,0,0,0,0,1,1, 1,0,1,0,0,0,1,0,1, 1,0,1,0,0,0,1,0,1, 1,0,0,1,0,1,0,0,1, 1,0,0,1,0,1,0,0,1, 1,1,0,0,1,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,'M') \n",
    "#    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,0,1, 1,0,1,0,0,0,0,0,1, 1,0,0,1,0,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,0,0,0,0,1,0,0,1, 1,0,0,0,0,0,1,0,1, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1, 2,'N') \n",
    "#    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 3,'H') \n",
    "#    if trainingDataSetNum == 4: trainingDataList = (0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0, 0,1,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 4,'A')             \n",
    "\n",
    "\n",
    "    trainingDataListA0 =  (1,[0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0, 0,1,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1],1,'A',1,'A') # training data list 1 selected for the letter 'A'\n",
    "    trainingDataListB0 =  (2,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,0,0, 1,0,0,0,0,0,0,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,0],2,'B',2,'B') # training data list 2, letter 'E', courtesy AJM\n",
    "    trainingDataListC0 =  (3,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],3,'C',3,'C') # training data list 3, letter 'C', courtesy PKVR\n",
    "    trainingDataListD0 =  (4,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,1, 1,1,1,1,1,1,1,1,0],4,'D',4,'O') # training data list 4, letter 'D', courtesy TD\n",
    "    trainingDataListE0 =  (5,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],5,'E',5,'E') # training data list 5, letter 'E', courtesy BMcD \n",
    "    trainingDataListF0 =  (6,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0],6,'F',5,'E') # training data list 6, letter 'F', courtesy SK\n",
    "    trainingDataListG0 =  (7,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1],7,'G',3,'C')\n",
    "    trainingDataListH0 =  (8,[1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1],8,'H',1,'A') # training data list 8, letter 'H', courtesy JC\n",
    "    trainingDataListI0 =  (9,[0,0,1,1,1,1,1,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,1,1,1,1,1,0,0],9,'I',6,'I') # training data list 9, letter 'I', courtesy GR\n",
    "    trainingDataListJ0 = (10,[0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1,0, 0,0,1,0,0,0,1,0,0, 0,0,0,1,1,1,0,0,0],10,'J',6,'I') # training data list 10 selected for the letter 'L', courtesy JT\n",
    "    trainingDataListK0 = (11,[1,0,0,0,0,0,1,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,1,0,0,0,0, 1,0,0,1,0,0,0,0,0, 1,1,1,0,0,0,0,0,0, 1,0,0,1,0,0,0,0,0, 1,0,0,0,1,0,0,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,0,0,1,0,0],11,'K',7,'K') # training data list 11 selected for the letter 'K', courtesy EO      \n",
    "    trainingDataListL0 = (12,[1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1],12,'L',8,'L') # training data list 12 selected for the letter 'L', courtesy PV\n",
    "    trainingDataListM0 = (13,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,1,1, 1,1,0,0,0,0,0,1,1, 1,0,1,0,0,0,1,0,1, 1,0,1,0,0,0,1,0,1, 1,0,0,1,0,1,0,0,1, 1,0,0,1,0,1,0,0,1, 1,1,0,0,1,0,0,0,1, 1,0,0,0,1,0,0,0,1],13,'M',9,'M') # training data list 13 selected for the letter 'M', courtesy GR            \n",
    "    trainingDataListN0 = (14,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,0,1, 1,0,1,0,0,0,0,0,1, 1,0,0,1,0,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,0,0,0,0,1,0,0,1, 1,0,0,0,0,0,1,0,1, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1],14,'N',9,'M') # training data list 14 selected for the letter 'N'\n",
    "    trainingDataListO0 = (15,[0,1,1,1,1,1,1,1,0, 1,1,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 0,1,1,1,1,1,1,1,0],15,'O',4,'O') # training data list 15, letter 'O', courtesy TD\n",
    "    trainingDataListP0 = (16,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0],16,'P',2, 'B') # training data list 16, letter 'P', courtesy MT \n",
    "\n",
    "#    trainingDataListQ0 = (17,[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1],17,'Q') # training data list 17, letter 'Q', courtesy RG (square corners)\n",
    "\n",
    "\n",
    "#    trainingDataListT0 = (20,[0,1,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0],20,'T') # training data list 20, letter 'T', courtesy JR\n",
    "\n",
    "\n",
    "#    trainingDataListW0 = (23, [1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0],23,'W') # training data list 23, letter 'W', courtesy KW\n",
    "#    trainingDataListX0 = (24,[1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1],24,'X') # training data list 24, letter 'X', courtesy JD\n",
    "#\n",
    "#    trainingDataListZ0 = (26,[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,],26,'Z') # training data list 26, letter 'Z', courtesy ZW\n",
    "                      \n",
    "    if dataSet == 1: trainingDataList = trainingDataListA0\n",
    "    if dataSet == 2: trainingDataList = trainingDataListB0 \n",
    "    if dataSet == 3: trainingDataList = trainingDataListC0\n",
    "    if dataSet == 4: trainingDataList = trainingDataListD0     \n",
    "    if dataSet == 5: trainingDataList = trainingDataListE0\n",
    "    if dataSet == 6: trainingDataList = trainingDataListF0 \n",
    "    if dataSet == 7: trainingDataList = trainingDataListG0 \n",
    "    if dataSet == 8: trainingDataList = trainingDataListH0\n",
    "    if dataSet == 9: trainingDataList = trainingDataListI0\n",
    "    if dataSet == 10: trainingDataList = trainingDataListJ0    \n",
    "\n",
    "    if dataSet == 11: trainingDataList = trainingDataListJ0 # needs to be replaced w/ K            \n",
    "\n",
    "    if dataSet == 12: trainingDataList = trainingDataListL0\n",
    "    if dataSet == 13: trainingDataList = trainingDataListM0\n",
    "    if dataSet == 14: trainingDataList = trainingDataListN0 \n",
    "    if dataSet == 15: trainingDataList = trainingDataListO0 \n",
    "    if dataSet == 16: trainingDataList = trainingDataListP0  \n",
    "\n",
    "#    if dataSet == 17: trainingDataList = trainingDataListQ0\n",
    "#     if dataSet == 18: trainingDataList = trainingDataListQ0 # needs to be replaced w/ R \n",
    "#     if dataSet == 19: trainingDataList = trainingDataListQ0 # needs to be replaced w/ S \n",
    "        \n",
    "#     if dataSet == 20: trainingDataList = trainingDataListT0                                            \n",
    "                                                                                        \n",
    "    return (trainingDataList)  \n",
    "           \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Perform a single feedforward pass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray):\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
    "\n",
    "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
    "    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
    "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
    "\n",
    "#    print ' '\n",
    "#    print 'Back in ComputeSingleFeedforwardPass'\n",
    "#    print 'The activations for the hidden nodes are:'\n",
    "#    print '  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
    "\n",
    "                                                                                                    \n",
    "    return (hiddenArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to compute the output node activations, given the hidden node activations, the hidden-to\n",
    "#   output connection weights, and the output bias weights.\n",
    "# Function returns the array of output node activations.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray):\n",
    "    \n",
    "# initialize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
    "    outputArray = np.zeros(outputArrayLength)   \n",
    "\n",
    "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
    "    \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
    "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
    "                                                                                                   \n",
    "    return (outputArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to compute the output node activations and determine errors across the entire training\n",
    "#  data set, and print results.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeOutputsAcrossAllTrainingData (alpha, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
    "\n",
    "    selectedTrainingDataSet = 1                              \n",
    "    \n",
    "    # ##################################################################\n",
    "    # code added 01/31/19\n",
    "    # lists to hold activations\n",
    "    # ##################################################################\n",
    "    hiddenNodeActivationsList = []\n",
    "    outputNodeActivationsList = []\n",
    "\n",
    "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "        print()\n",
    "        print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "\n",
    "        #Note: the trainingDataList is a list comprising several values:\n",
    "        #    - the 0th is the list number \n",
    "        #    - the 1st is the actual list of the input training values\n",
    "        #    - etc. \n",
    "\n",
    "        trainingDataInputList = trainingDataList[1]      \n",
    "        \n",
    "        inputDataList = [] \n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataInputList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "\n",
    "        letterNum = trainingDataList[2]\n",
    "        letterChar = trainingDataList[3]  \n",
    "        print()\n",
    "        print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
    "\n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        print()\n",
    "        print(\" The hidden node activations are: \")\n",
    "        print(hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "        print()\n",
    "        print(\" The output node activations are: \")\n",
    "        print(outputArray) \n",
    "        \n",
    "        # ########################\n",
    "        # code added 01/31/19\n",
    "        # append activations\n",
    "        # ########################\n",
    "        hiddenNodeActivationsList.append([letterChar, letterNum, trainingDataList[4], '\\n'.join([letterChar, trainingDataList[5]]), hiddenArray])\n",
    "        outputNodeActivationsList.append([letterChar, letterNum, trainingDataList[4], '\\n'.join([letterChar, trainingDataList[5]]), outputArray])\n",
    " \n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[4]                  # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "        print()\n",
    "        print(\" The desired output array values are: \")\n",
    "        print(desiredOutputArray)  \n",
    "       \n",
    "                        \n",
    "        # Determine the error between actual and desired outputs\n",
    "        # Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        print()\n",
    "        print(\" ' The error values are:\")\n",
    "        print(errorArray)   \n",
    "        \n",
    "        # Print the Summed Squared Error  \n",
    "        print(\"New SSE = %.6f\" % newSSE) \n",
    "         \n",
    "        selectedTrainingDataSet = selectedTrainingDataSet +1 \n",
    "    \n",
    "    # ##################################################################\n",
    "    # code added 01/31/19\n",
    "    # visualize activations from output layer in heat map\n",
    "    # ##################################################################\n",
    "    print(\"\\n\\nhiddenNodeActivationsList L-{letter} C-{class}\")\n",
    "    print(\"==============================================\")\n",
    "    print(\"==============================================\\n\")\n",
    "    df = pd.DataFrame([lst[4] for lst in hiddenNodeActivationsList])\n",
    "    df = df.transpose()\n",
    "    df.columns = (x[3] for x in hiddenNodeActivationsList)\n",
    "    df.columns = 'L-' + df.columns.str[0:1] + ' C-' + df.columns.str[2:3]\n",
    "    print(df)\n",
    "    \n",
    "    # hidden nodes heatmap\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    sns.set(rc={'figure.figsize':(10.0,4.0)})\n",
    "    ax = sns.heatmap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white')\n",
    "    ax.set_title('Hidden Node Activations\\nL-{letter} C-{class}')\n",
    "    ax.set_ylabel('Node')\n",
    "    ax.set_xlabel('L-{Letter} C-{Class}')\n",
    "    plt.show()\n",
    "    \n",
    "    # hidden nodes clustermap with row and column dendrogram\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    ax = sns.clustermap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white')\n",
    "    ax.fig.suptitle('Hidden Node Activations\\nClustered by Node and Letter\\nL-{letter} C-{class}')\n",
    "    plt.show()\n",
    "\n",
    "    # hidden nodes clustermap with column dendrogram\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    ax = sns.clustermap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white',row_cluster=False)\n",
    "    ax.fig.suptitle('Hidden Node Activations\\nClustered Letter\\nL-{letter} C-{class}')\n",
    "    plt.show()\n",
    " \n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "#   Backpropagation Section\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the hidden-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def backpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight v. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in the equations for the deltas in the connection weights    \n",
    "#    print ' '\n",
    "#    print ' The transfer function derivative is: '\n",
    "#    print transferFuncDerivArray\n",
    "                        \n",
    "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
    "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
    "        \n",
    "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes,\n",
    "        #    and the columns correspond to the number of hidden nodes,\n",
    "        #    which can be multiplied by the hidden node array (expressed as a column).\n",
    "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
    "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
    "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newVWeightArray\n",
    "\n",
    "#    PrintAndTraceBackpropagateOutputToHidden (alpha, nu, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newHiddenWeightArray)    \n",
    "                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "    return (newVWeightArray);     \n",
    "\n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def backpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "\n",
    "# Unpack the output array length\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
    "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
    "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
    "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
    "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
    "   \n",
    "#    print ' '\n",
    "#    print ' The previous biases for the output nodes are: '\n",
    "#    print biasOutputWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new biases for the output nodes are: '\n",
    "#    print newBiasOutputWeightArray\n",
    "                                                                                                                                                \n",
    "    return (newBiasOutputWeightArray);     \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the input-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def backpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the input-to-hidden wts w. \n",
    "# Core equation for the second part of backpropagation: \n",
    "# d(SSE)/dw(i,h) = -eta*alpha*F(h)(1-F(h))*Input(i)*sum(v(h,o)*Error(o))\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- w(i,h) is the connection weight w between the input node i and the hidden node h\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1 \n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# ---- NOTE: in this second step, the transfer function is applied to the output of the hidden node,\n",
    "# ------ so that F = F(h)\n",
    "# -- Hidden(h) = the output of hidden node h (used in computing the derivative of the transfer function). \n",
    "# -- Input(i) = the input at node i.\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight w. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    " \n",
    "# For the second step in backpropagation (computing deltas on the input-to-hidden weights)\n",
    "#   we need the transfer function derivative is applied to the output at the hidden node        \n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations       \n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    # initialize an array for the transfer function deriv \n",
    "      \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
    "        \n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
    "      \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "        \n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "             \n",
    "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
    "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
    "        \n",
    "    for row in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "\n",
    "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
    "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
    "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print wWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newWWeightArray    \n",
    "       \n",
    "                                                                    \n",
    "    return (newWWeightArray);     \n",
    "    \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def backpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "   \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence              \n",
    "\n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array    \n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array    \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  # initialize an array for the transfer function deriv \n",
    "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  # initialize an array for the partial derivative of the SSE\n",
    "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  # initialize an array for the deltas\n",
    "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  # initialize an array for the new hidden bias weights\n",
    "          \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
    "                  \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = (weightedErrorArray[hiddenNode]\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode])\n",
    "            \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "\n",
    "# ===>>> AJM needs to double-check these equations in the comments area\n",
    "# ===>>> The code should be fine. \n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
    "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
    "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
    "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
    "  \n",
    "              \n",
    "    return (newBiasHiddenWeightArray); \n",
    "\n",
    "\n",
    "             \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to print out a letter, given the number of the letter code\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def printLetter (trainingDataList):    \n",
    "            \n",
    "    pixelArray = trainingDataList[1]\n",
    "    print(' ')\n",
    "    gridWidth = 9\n",
    "    gridHeight = 9\n",
    "    iterAcrossRow = 0\n",
    "    iterOverAllRows = 0\n",
    "    while iterOverAllRows <gridHeight:\n",
    "        while iterAcrossRow < gridWidth:\n",
    "            arrayElement = pixelArray [iterAcrossRow+iterOverAllRows*gridWidth]\n",
    "            if arrayElement <0.9: \n",
    "                printElement = ' '\n",
    "            else: \n",
    "                printElement = 'X'\n",
    "            print(printElement, end='') \n",
    "            iterAcrossRow = iterAcrossRow+1\n",
    "        print(' ')\n",
    "        iterOverAllRows = iterOverAllRows + 1\n",
    "        iterAcrossRow = 0 #re-initialize so the row-print can begin again\n",
    "    print('The data set is for the letter', trainingDataList[3], ', which is alphabet number ', trainingDataList[2])\n",
    "    if trainingDataList[0] > 25:\n",
    "        print('This is a variant pattern for letter ', trainingDataList[3]) \n",
    "    \n",
    "    return             \n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################    \n",
    "        \n",
    "            \n",
    "                    \n",
    "            \n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Compute a feedforward pass in two steps\n",
    "#       - Randomly select a single training data set\n",
    "#       - Input-to-Hidden\n",
    "#       - Hidden-to-Output\n",
    "#       - Compute the error array\n",
    "#       - Compute the new Summed Squared Error (SSE)\n",
    "#   (5) Perform a single backpropagation training pass\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "# Define the global variables        \n",
    "    global inputArrayLength\n",
    "    global hiddenArrayLength\n",
    "    global outputArrayLength\n",
    "    global gridWidth\n",
    "    global gridHeight\n",
    "    global eGH # expandedGridHeight, defined in function expandLetterBoundaries \n",
    "    global eGW # expandedGridWidth defined in function expandLetterBoundaries \n",
    "    global mask1    \n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    welcome()\n",
    "\n",
    "    \n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# Define the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Obtain the actual sizes for each layer of the network       \n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
    "    \n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers\n",
    "# Note: A word on Python encoding ... the actually length of the array, in each of these three cases, \n",
    "#       will be xArrayLength. For example, the inputArrayLength for the 9x9 pixel array is 81. \n",
    "#       These values are passed to various procedures. They start filling in actual array values,\n",
    "#       where the array values start their count at element 0. However, when filling them in using a\n",
    "#       \"for node in range[limit]\" statement, the \"for\" loop fills from 0 up to limit-1. Thus, the\n",
    "#       original xArrayLength size is preserved.   \n",
    "    inputArrayLength = arraySizeList [0] \n",
    "    hiddenArrayLength = arraySizeList [1] \n",
    "    outputArrayLength = arraySizeList [2] \n",
    "    \n",
    "    print()\n",
    "    print(\" inputArrayLength = \", inputArrayLength)\n",
    "    print(\" hiddenArrayLength = \", hiddenArrayLength)\n",
    "    print(\" outputArrayLength = \", outputArrayLength)        \n",
    "\n",
    "\n",
    "# Parameter definitions for backpropagation, to be replaced with user inputs\n",
    "    alpha = 1.0\n",
    "    eta = 0.5    \n",
    "    maxNumIterations = 5000    # temporarily set to 10 for testing\n",
    "    epsilon = 0.01\n",
    "    iteration = 0\n",
    "    SSE = 0.0\n",
    "    numTrainingDataSets = 16\n",
    "\n",
    "                           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength        \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array\n",
    "\n",
    "    wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
    "  \n",
    "    vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
    "\n",
    "# The bias weights are stored in a 1-D array         \n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "    \n",
    "          \n",
    "####################################################################################################\n",
    "# Starting the backpropagation work\n",
    "####################################################################################################     \n",
    "\n",
    "\n",
    "\n",
    "# Notice in the very beginning of the program, we have \n",
    "#   np.set_printoptions(precision=4) (sets number of dec. places in print)\n",
    "#     and 'np.set_printoptions(suppress=True)', which keeps it from printing in scientific format\n",
    "#   Debug print: \n",
    "#    print\n",
    "#    print 'The initial weights for this neural network are:'\n",
    "#    print '       Input-to-Hidden '\n",
    "#    print wWeightArray\n",
    "#    print '       Hidden-to-Output'\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print 'The initial bias weights for this neural network are:'\n",
    "#    print '        Hidden Bias = ', biasHiddenWeightArray                         \n",
    "#    print '        Output Bias = ', biasOutputWeightArray\n",
    "  \n",
    "\n",
    "          \n",
    "####################################################################################################\n",
    "# Before we start training, get a baseline set of outputs, errors, and SSE \n",
    "####################################################################################################                \n",
    "                            \n",
    "    print()\n",
    "    print(\"  Before training:\")\n",
    "    \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray)                           \n",
    "                                             \n",
    "          \n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of randomly-selected training values for alpha-classification \n",
    "####################################################################################################                \n",
    "   \n",
    "    while iteration < maxNumIterations:           \n",
    "\n",
    "# Increment the iteration count\n",
    "        iteration = iteration +1\n",
    "\n",
    "####################################################################################################\n",
    "# While training - STEP 1: Obtain a set of training data; inputs and desired outputs\n",
    "####################################################################################################     \n",
    "            \n",
    "# For any given pass, we re-initialize the training list\n",
    "        trainingDataList = (0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0, ' ')                                 \n",
    "                                                                                          \n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        dataSet = random.randint(1, numTrainingDataSets)\n",
    "\n",
    "# We return the list from the function, with values placed inside the list.           \n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (dataSet)  \n",
    "                \n",
    "# Optional print/debug\n",
    "#        printLetter(trainingDataList)        \n",
    "          \n",
    "                                                                                                                                                    \n",
    "####################################################################################################\n",
    "# While training - STEP 2: Create an input array based on the input training data list\n",
    "####################################################################################################     \n",
    "\n",
    "        inputDataList = []  \n",
    "        inputDataArray =  np.zeros(inputArrayLength)\n",
    "        \n",
    "# The trainning inputs are drawn from the first element (starting count at 0) in the training data list\n",
    "\n",
    "        thisTrainingDataList = list()                                                                            \n",
    "        thisTrainingDataList = trainingDataList[1]\n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = thisTrainingDataList[node]\n",
    "            inputDataList.append(trainingData)\n",
    "            inputDataArray[node] = trainingData\n",
    "        \n",
    "        \n",
    "# The desired outputs are drawn from the fourth element (starting count at 0) in the training data list\n",
    "#   This represents the \"big shape class\" which we are training towards in GB1\n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[4]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "\n",
    "                 \n",
    "          \n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "####################################################################################################                \n",
    "                \n",
    "                           \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataArray, wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray,vWeightArray, biasOutputWeightArray)\n",
    "       \n",
    "\n",
    "#  Optional alternative code for later use:\n",
    "#  Assign the hidden and output values to specific different variables\n",
    "#    for node in range(hiddenArrayLength):    \n",
    "#        actualHiddenOutput[node] = actualAllNodesOutputList [node]\n",
    "    \n",
    "#    for node in range(outputArrayLength):    \n",
    "#        actualOutput[node] = actualAllNodesOutputList [hiddenArrayLength + node]\n",
    " \n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "# Determine the error between actual and desired outputs        \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "\n",
    "          \n",
    "####################################################################################################\n",
    "# Perform backpropagation\n",
    "####################################################################################################                \n",
    "                \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes    \n",
    "        newVWeightArray = backpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray)\n",
    "        newBiasOutputWeightArray = backpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray) \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes       \n",
    "        newWWeightArray = backpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "\n",
    "        newBiasHiddenWeightArray = backpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)  \n",
    "    \n",
    "                    \n",
    "# Assign new values to the weight matrices\n",
    "# Assign the old hidden-to-output weight array to be the same as what was returned from the BP weight update\n",
    "        vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "        biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
    "    \n",
    "# Assign the old input-to-hidden weight array to be the same as what was returned from the BP weight update\n",
    "        wWeightArray = newWWeightArray[:]  \n",
    "    \n",
    "        biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
    "    \n",
    "# Compute a forward pass, test the new SSE                                                                                \n",
    "                                                                                                                                    \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "\n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        if newSSE < epsilon:            \n",
    "            break\n",
    "    print(\"Out of while loop at iteration \", iteration) \n",
    "    \n",
    "####################################################################################################\n",
    "# After training, get a new comparative set of outputs, errors, and SSE \n",
    "####################################################################################################                           \n",
    "\n",
    "    print()\n",
    "    print(\"  After training:\")                  \n",
    "                                                      \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray) \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# After computing final SSEs, etc., store all the weights to files (four different files)\n",
    "####################################################################################################    \n",
    "\n",
    "# Create lists of the connection weights; one for the input-to-hidden, another for the hidden-to-output\n",
    "    wWeightList = list()\n",
    "    numUpperNodes = hiddenArrayLength \n",
    "    numLowerNodes = inputArrayLength\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numLowerNodes):  # number of columns in matrix 2\n",
    "            localWeight = wWeightArray[row,col] \n",
    "            wWeightList.append(localWeight)    \n",
    "\n",
    "#    print wWeightList\n",
    "\n",
    "# Write the input-to-hidden connection weights to a file\n",
    "\n",
    "    wWeightFile = open('datafiles/GB1wWeightFile', 'w') \n",
    "\n",
    "    for item in wWeightList:\n",
    "        wWeightFile.write(\"%s\\n\" % item)  \n",
    "    wWeightFile.close()\n",
    "     \n",
    "# do the same for the hidden to output weights\n",
    " \n",
    "    vWeightList = list()\n",
    "    numUpperNodes = outputArrayLength\n",
    "    numLowerNodes = hiddenArrayLength\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numLowerNodes):  # number of columns in matrix 2\n",
    "            localWeight = vWeightArray[row,col] \n",
    "            vWeightList.append(localWeight)    \n",
    "\n",
    "#    print vWeightList\n",
    "\n",
    "# Write the hidden-to-output connection weights to a file\n",
    "\n",
    "    vWeightFile = open('datafiles/GB1vWeightFile', 'w') \n",
    "\n",
    "    for item in vWeightList:\n",
    "        vWeightFile.write(\"%s\\n\" % item)  \n",
    " \n",
    "    vWeightFile.close() \n",
    "\n",
    "# Do the same thing for the two sets of bias weights    \n",
    "        \n",
    "# Start with the hidden node bias weights\n",
    "    \n",
    "# Create a list storing the array values for the hidden node bias weights\n",
    "    wBiasWeightList = list()\n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden bias nodes\n",
    "        localBiasWeight = biasHiddenWeightArray[node] \n",
    "        wBiasWeightList.append(localBiasWeight)                \n",
    "                \n",
    "# Write the hidden node bias weights to a file\n",
    "\n",
    "    wBiasWeightFile = open('datafiles/GB1wBiasWeightFile', 'w') \n",
    "\n",
    "    for item in wBiasWeightList:\n",
    "        wBiasWeightFile.write(\"%s\\n\" % item)  \n",
    "    wBiasWeightFile.close()                            \n",
    "                                        \n",
    "\n",
    "# Repeat the process with the output node bias weights\n",
    "    \n",
    "# Create a list storing the array values for the output node bias weights\n",
    "    vBiasWeightList = list()  \n",
    "    for node in range(outputArrayLength):  #  Number of output nodes\n",
    "        localBiasWeight = biasOutputWeightArray[node] \n",
    "        vBiasWeightList.append(localBiasWeight)                                                      \n",
    "                                                                          \n",
    "# Write the output node bias weights to a file\n",
    "\n",
    "    vBiasWeightFile = open('datafiles/GB1vBiasWeightFile', 'w') \n",
    "\n",
    "    for item in vBiasWeightList:\n",
    "        vBiasWeightFile.write(\"%s\\n\" % item)  \n",
    "    vBiasWeightFile.close()  \n",
    "                        \n",
    "    print(\" Completed training and storing connection weights to files\")\n",
    "\n",
    "#    print ' for the last training data set:'\n",
    "#    printLetter(trainingDataList)\n",
    "    \n",
    "\n",
    "\n",
    "                                             \n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "####################################################################################################                \n",
    "    \n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "####################################################################################################\n",
    "# End program\n",
    "#################################################################################################### \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
