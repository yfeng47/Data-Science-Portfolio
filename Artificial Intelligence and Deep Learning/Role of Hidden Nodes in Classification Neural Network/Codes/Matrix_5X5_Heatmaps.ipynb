{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "  trained using the backpropagation method.\n",
      "Version 0.2, 01/22/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      " \n",
      "This program learns to distinguish between five capital letters: X, M, H, A, and N\n",
      "It allows users to examine the hidden weights to identify learned features\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      " \n",
      "  The number of nodes at each level are:\n",
      "    Input: 5x5 (square array)\n",
      "    Hidden: 6\n",
      "    Output: 5 (five classes)\n",
      "[-0.511  0.397 -0.24   0.409 -0.854 -0.346]\n",
      "[ 0.432  0.815  0.896 -0.729 -0.823]\n",
      " \n",
      "  Before training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.381 0.903 0.476 0.754 0.593 0.719]\n",
      " \n",
      " The output node activations are:\n",
      "[0.288 0.71  0.749 0.242 0.401]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.288 -0.71  -0.749 -0.242 -0.401]\n",
      "New SSE = 1.366623\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.84  0.794 0.773 0.553 0.814 0.219]\n",
      " \n",
      " The output node activations are:\n",
      "[0.336 0.673 0.74  0.23  0.477]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.336 -0.673 -0.74  -0.23  -0.477]\n",
      "New SSE = 1.394047\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.572 0.923 0.485 0.509 0.625 0.295]\n",
      " \n",
      " The output node activations are:\n",
      "[0.359 0.672 0.775 0.236 0.444]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.359 -0.672 -0.775 -0.236 -0.444]\n",
      "New SSE = 1.434305\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.803 0.644 0.709 0.517 0.37  0.073]\n",
      " \n",
      " The output node activations are:\n",
      "[0.452 0.739 0.692 0.207 0.538]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.452 -0.739 -0.692 -0.207 -0.538]\n",
      "New SSE = 1.560907\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.736 0.743 0.903 0.829 0.735 0.047]\n",
      " \n",
      " The output node activations are:\n",
      "[0.36  0.671 0.621 0.254 0.533]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.36  -0.671 -0.621 -0.254 -0.533]\n",
      "New SSE = 1.312760\n",
      "\n",
      "\n",
      "hiddenNodeActivationsList L-{letter} C-{class}\n",
      "==============================================\n",
      "==============================================\n",
      "\n",
      "    L-X C-X   L-M C-M   L-N C-N   L-H C-H   L-A C-A\n",
      "0  0.380850  0.840459  0.571917  0.803185  0.736330\n",
      "1  0.903023  0.793720  0.922661  0.644303  0.743497\n",
      "2  0.475515  0.772789  0.485348  0.709492  0.902739\n",
      "3  0.754159  0.553424  0.508848  0.517470  0.829447\n",
      "4  0.592527  0.814225  0.625069  0.369877  0.734716\n",
      "5  0.719037  0.219166  0.294579  0.073123  0.046807\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of while loop at iteration  474\n",
      " \n",
      "  After training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.055 0.959 0.919 0.561 0.12  0.973]\n",
      " \n",
      " The output node activations are:\n",
      "[0.844 0.097 0.117 0.011 0.163]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.844 -0.097 -0.117 -0.011 -0.163]\n",
      "New SSE = 0.761446\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.836 0.223 0.523 0.336 0.962 0.903]\n",
      " \n",
      " The output node activations are:\n",
      "[0.041 0.638 0.18  0.112 0.09 ]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.041 -0.638 -0.18  -0.112 -0.09 ]\n",
      "New SSE = 0.462169\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.185 0.832 0.063 0.093 0.948 0.922]\n",
      " \n",
      " The output node activations are:\n",
      "[0.141 0.133 0.821 0.072 0.02 ]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.141 -0.133 -0.821 -0.072 -0.02 ]\n",
      "New SSE = 0.717324\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.167 0.111 0.104 0.846 0.97  0.125]\n",
      " \n",
      " The output node activations are:\n",
      "[0.05  0.127 0.109 0.755 0.175]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.05  -0.127 -0.109 -0.755 -0.175]\n",
      "New SSE = 0.631111\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.87  0.891 0.965 0.969 0.862 0.044]\n",
      " \n",
      " The output node activations are:\n",
      "[0.075 0.078 0.013 0.106 0.839]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.075 -0.078 -0.013 -0.106 -0.839]\n",
      "New SSE = 0.726531\n",
      "\n",
      "\n",
      "hiddenNodeActivationsList L-{letter} C-{class}\n",
      "==============================================\n",
      "==============================================\n",
      "\n",
      "    L-X C-X   L-M C-M   L-N C-N   L-H C-H   L-A C-A\n",
      "0  0.055267  0.836419  0.185337  0.166855  0.870174\n",
      "1  0.959469  0.223205  0.831990  0.111469  0.890932\n",
      "2  0.918931  0.522608  0.063494  0.103894  0.965146\n",
      "3  0.561205  0.336323  0.093397  0.846444  0.968931\n",
      "4  0.119735  0.961563  0.947915  0.969914  0.861903\n",
      "5  0.973280  0.903212  0.921664  0.125149  0.043939\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "\n",
    "# ################################################################################################\n",
    "#\n",
    "#\n",
    "#  5x5 Module 3\n",
    "#\n",
    "#\n",
    "# ################################################################################################\n",
    "\n",
    "import random\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# So we can make a separate list from an initial one\n",
    "import copy\n",
    "\n",
    "# import packages for heatmap associated generation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# For pretty-printing the arrays\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# This is a tutorial program, designed for those who are learning Python, and specifically using \n",
    "#   Python for neural networks applications\n",
    "#\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to welcome the user and identify the code\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def welcome ():\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('******************************************************************************')\n",
    "    print()\n",
    "    print('Welcome to the Multilayer Perceptron Neural Network')\n",
    "    print('  trained using the backpropagation method.')\n",
    "    print('Version 0.2, 01/22/2017, A.J. Maren')\n",
    "    print('For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu')\n",
    "    print(' ') \n",
    "    print('This program learns to distinguish between five capital letters: X, M, H, A, and N')\n",
    "    print('It allows users to examine the hidden weights to identify learned features')\n",
    "    print()\n",
    "    print('******************************************************************************')\n",
    "    print()\n",
    "    return()\n",
    "\n",
    "        \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# A collection of worker-functions, designed to do specific small tasks\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "#------------------------------------------------------#    \n",
    "\n",
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
    "    return activation\n",
    "  \n",
    "\n",
    "#------------------------------------------------------# \n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "\n",
    "#------------------------------------------------------# \n",
    "def matrixDotProduct (matrx1,matrx2):\n",
    "    dotProduct = np.dot(matrx1,matrx2)\n",
    "    \n",
    "    return(dotProduct)    \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to obtain the neural network size specifications\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "\n",
    "# This procedure operates as a function, as it returns a single value (which really is a list of \n",
    "#    three values). It is called directly from 'main.'\n",
    "#        \n",
    "# This procedure allows the user to specify the size of the input (I), hidden (H), \n",
    "#    and output (O) layers.  \n",
    "# These values will be stored in a list, the arraySizeList. \n",
    "# This list will be used to specify the sizes of two different weight arrays:\n",
    "#   - wWeights; the Input-to-Hidden array, and\n",
    "#   - vWeights; the Hidden-to-Output array. \n",
    "# However, even though we're calling this procedure, we will still hard-code the array sizes for now.   \n",
    "\n",
    "    numInputNodes = 25\n",
    "    numHiddenNodes = 6\n",
    "    numOutputNodes = 5   \n",
    "    print(' ')\n",
    "    print('  The number of nodes at each level are:')\n",
    "    print('    Input: 5x5 (square array)')\n",
    "    print('    Hidden: 6')\n",
    "    print('    Output: 5 (five classes)')\n",
    "            \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def InitializeWeight ():\n",
    "\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "#    print weight\n",
    "           \n",
    "    return (weight)  \n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the node-to-node connection weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeWeightArray (weightArraySizeList):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "#        \n",
    "# This procedure takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be assigned random values here, and \n",
    "#   this array is passed back to the 'main' procedure. \n",
    "\n",
    "    \n",
    "    numBottomNodes = weightArraySizeList[0]\n",
    "    numUpperNodes = weightArraySizeList[1]\n",
    "\n",
    "# Initialize the weight variables with random weights    \n",
    "    weightArray = np.zeros((numUpperNodes,numBottomNodes))    # iniitalize the weight matrix with 0's\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numBottomNodes):  # number of columns in matrix 2\n",
    "            weightArray[row,col] = InitializeWeight ()\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the bias weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeBiasWeightArray (numBiasNodes):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "\n",
    "# Initialize the bias weight variables with random weights    \n",
    "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
    "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
    "        biasWeightArray[node] = InitializeWeight ()\n",
    "                  \n",
    "# Print the entire weights array. \n",
    "    print(biasWeightArray)\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to return a trainingDataList\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainSelectedAlphabetTrainingValues (trainingDataSetNum):\n",
    "    \n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainRandomAlphabetTrainingValues (numTrainingDataSets):\n",
    "\n",
    "   \n",
    "    # The training data list will have 11 values for the X-OR problem:\n",
    "    #   - First nine valuea will be the 5x5 pixel-grid representation of the letter\n",
    "    #       represented as a 1-D array (0 or 1 for each)\n",
    "    #   - Tenth value will be the output class (0 .. totalClasses - 1)\n",
    "    #   - Eleventh value will be the string associated with that class, e.g., 'X'\n",
    "    # We are starting with five letters in the training set: X, M, N, H, and A\n",
    "    # Thus there are five choices for training data, which we'll select on random basis\n",
    "      \n",
    "    trainingDataSetNum = random.randint(0, numTrainingDataSets)\n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)  \n",
    "\n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Perform a single feedforward pass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, \n",
    "biasHiddenWeightArray):\n",
    "\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
    "\n",
    "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
    "    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
    "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
    "\n",
    "#    print ' '\n",
    "#    print 'Back in ComputeSingleFeedforwardPass'\n",
    "#    print 'The activations for the hidden nodes are:'\n",
    "#    print '  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
    "\n",
    "                                                                                                    \n",
    "    return (hiddenArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to compute the output node activations, given the hidden node activations, the hidden-to\n",
    "#   output connection weights, and the output bias weights.\n",
    "# Function returns the array of output node activations.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, \n",
    "biasOutputWeightArray):\n",
    "\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
    "    outputArray = np.zeros(outputArrayLength)   \n",
    "\n",
    "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
    "    \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
    "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
    "                                                                                                   \n",
    "    return (outputArray);\n",
    "  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to compute the output node activations and determine errors across the entire training\n",
    "#  data set, and print results.\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "def ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
    "\n",
    "    selectedTrainingDataSet = 0\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]                               \n",
    "\n",
    "    # HM\n",
    "    hiddenNodeActivationsList = []\n",
    "    outputNodeActivationsList = []\n",
    "                              \n",
    "\n",
    "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "        \n",
    "        inputDataList = [] \n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "\n",
    "        \n",
    "        letterNum = trainingDataList[25]\n",
    "        letterChar = trainingDataList[26]  \n",
    "        \n",
    "        print(' ')\n",
    "        print('  Data Set Number', selectedTrainingDataSet, ' for letter ', trainingDataList[26] )\n",
    "\n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        print(' ')\n",
    "        print(' The hidden node activations are:')\n",
    "        print(hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "        print(' ')\n",
    "        print(' The output node activations are:')\n",
    "        print(outputArray )  \n",
    "\n",
    "        # ########################\n",
    "        # code added 01/31/19\n",
    "        # append activations\n",
    "        # ########################\n",
    "        hiddenNodeActivationsList.append([letterChar, letterNum, trainingDataList[26], '\\n'.join([letterChar, trainingDataList[26]]), hiddenArray])\n",
    "        outputNodeActivationsList.append([letterChar, letterNum, trainingDataList[26], '\\n'.join([letterChar, trainingDataList[26]]), outputArray])\n",
    " \n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        # desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        # desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "        print( ' ')\n",
    "        print(' The desired output array values are: ')\n",
    "        print(desiredOutputArray ) \n",
    "       \n",
    "                        \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        print(' ')\n",
    "        print(' The error values are:')\n",
    "        print(errorArray)\n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "        print('New SSE = %.6f' % newSSE)\n",
    "         \n",
    "        selectedTrainingDataSet = selectedTrainingDataSet +1 \n",
    "        \n",
    "\n",
    "    # ##################################################################\n",
    "    # code added 01/31/19\n",
    "    # visualize activations from output layer in heat map\n",
    "    # ##################################################################\n",
    "    print(\"\\n\\nhiddenNodeActivationsList L-{letter} C-{class}\")\n",
    "    print(\"==============================================\")\n",
    "    print(\"==============================================\\n\")\n",
    "    df = pd.DataFrame([lst[4] for lst in hiddenNodeActivationsList])\n",
    "    df = df.transpose()\n",
    "    df.columns = (x[3] for x in hiddenNodeActivationsList)\n",
    "    df.columns = 'L-' + df.columns.str[0:1] + ' C-' + df.columns.str[2:3]\n",
    "    print(df)\n",
    "    \n",
    "    # hidden nodes heatmap\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    sns.set(rc={'figure.figsize':(10.0,4.0)})\n",
    "    ax = sns.heatmap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white')\n",
    "    ax.set_title('Hidden Node Activations\\nL-{letter} C-{class}')\n",
    "    ax.set_ylabel('Node')\n",
    "    ax.set_xlabel('L-{Letter} C-{Class}')\n",
    "    plt.show()\n",
    "    \n",
    "    # hidden nodes clustermap with row and column dendrogram\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    ax = sns.clustermap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white')\n",
    "    ax.fig.suptitle('Hidden Node Activations\\nClustered by Node and Letter\\nL-{letter} C-{class}')\n",
    "    plt.show()\n",
    "\n",
    "    # hidden nodes clustermap with column dendrogram\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    ax = sns.clustermap(df,cmap='YlOrRd',annot=False,linewidths=0.1,linecolor='white',row_cluster=False)\n",
    "    ax.fig.suptitle('Hidden Node Activations\\nClustered Letter\\nL-{letter} C-{class}')\n",
    "    plt.show()\n",
    " \n",
    "                        \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "#   Backpropgation Section\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the hidden-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight v. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in the equations for the deltas in the connection weights    \n",
    "#    print ' '\n",
    "#    print ' The transfer function derivative is: '\n",
    "#    print transferFuncDerivArray\n",
    "                        \n",
    "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
    "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
    "        \n",
    "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes,\n",
    "        #    and the columns correspond to the number of hidden nodes,\n",
    "        #    which can be multiplied by the hidden node array (expressed as a column).\n",
    "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
    "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
    "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newVWeightArray\n",
    "\n",
    "#    PrintAndTraceBackpropagateOutputToHidden (alpha, nu, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newHiddenWeightArray)    \n",
    "                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "    return (newVWeightArray);     \n",
    "\n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-output connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "\n",
    "# Unpack the output array length\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
    "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
    "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
    "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
    "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
    "   \n",
    "#    print ' '\n",
    "#    print ' The previous biases for the output nodes are: '\n",
    "#    print biasOutputWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new biases for the output nodes are: '\n",
    "#    print newBiasOutputWeightArray\n",
    "                                                                                                                                                \n",
    "    return (newBiasOutputWeightArray);     \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the input-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the input-to-hidden wts w. \n",
    "# Core equation for the second part of backpropagation: \n",
    "# d(SSE)/dw(i,h) = -eta*alpha*F(h)(1-F(h))*Input(i)*sum(v(h,o)*Error(o))\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- w(i,h) is the connection weight w between the input node i and the hidden node h\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1 \n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# ---- NOTE: in this second step, the transfer function is applied to the output of the hidden node,\n",
    "# ------ so that F = F(h)\n",
    "# -- Hidden(h) = the output of hidden node h (used in computing the derivative of the transfer function). \n",
    "# -- Input(i) = the input at node i.\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight w. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    " \n",
    "# For the second step in backpropagation (computing deltas on the input-to-hidden weights)\n",
    "#   we need the transfer function derivative is applied to the output at the hidden node        \n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations       \n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    # initialize an array for the transfer function deriv \n",
    "      \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
    "        \n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
    "      \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "        \n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "             \n",
    "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
    "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
    "        \n",
    "    for row in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "\n",
    "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
    "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
    "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print ' '\n",
    "#    print ' The previous hidden-to-output connection weights are: '\n",
    "#    print wWeightArray\n",
    "#    print ' '\n",
    "#    print ' The new hidden-to-output connection weights are: '\n",
    "#    print newWWeightArray    \n",
    "       \n",
    "                                                                    \n",
    "    return (newWWeightArray);     \n",
    "    \n",
    "            \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Backpropagate weight changes onto the bias-to-hidden connection weights\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "   \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence              \n",
    "\n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array    \n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array    \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  # initialize an array for the transfer function deriv \n",
    "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  # initialize an array for the partial derivative of the SSE\n",
    "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  # initialize an array for the deltas\n",
    "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  # initialize an array for the new hidden bias weights\n",
    "          \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
    "                  \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = (weightedErrorArray[hiddenNode] \n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode])\n",
    "            \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "\n",
    "# ===>>> AJM needs to double-check these equations in the comments area\n",
    "# ===>>> The code should be fine. \n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
    "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
    "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
    "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
    "  \n",
    "                                                                                                                                            \n",
    "    return (newBiasHiddenWeightArray); \n",
    "    \n",
    "            \n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Compute a feedforward pass in two steps\n",
    "#       - Randomly select a single training data set\n",
    "#       - Input-to-Hidden\n",
    "#       - Hidden-to-Output\n",
    "#       - Compute the error array\n",
    "#       - Compute the new Summed Squared Error (SSE)\n",
    "#   (5) Perform a single backpropagation training pass\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    welcome()\n",
    "    \n",
    "\n",
    "# Parameter definitions, to be replaced with user inputs\n",
    "    alpha = 1.0\n",
    "    eta = 0.5    \n",
    "    maxNumIterations = 5000    # temporarily set to 10 for testing\n",
    "    epsilon = 0.05\n",
    "    iteration = 0\n",
    "    SSE = 0.0\n",
    "    numTrainingDataSets = 4\n",
    "    \n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# Define the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Obtain the actual sizes for each layer of the network       \n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
    "    \n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers    \n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "                           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength        \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array\n",
    "    wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
    "    vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
    "\n",
    "# The bias weights are stored in a 1-D array         \n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "\n",
    "# Notice in the very beginning of the program, we have \n",
    "#   np.set_printoptions(precision=4) (sets number of dec. places in print)\n",
    "#     and 'np.set_printoptions(suppress=True)', which keeps it from printing in scientific format\n",
    "#   Debug print: \n",
    "#    print\n",
    "#    print 'The initial weights for this neural network are:'\n",
    "#    print '       Input-to-Hidden '\n",
    "#    print wWeightArray\n",
    "#    print '       Hidden-to-Output'\n",
    "#    print vWeightArray\n",
    "#    print ' '\n",
    "#    print 'The initial bias weights for this neural network are:'\n",
    "#    print '        Hidden Bias = ', biasHiddenWeightArray                         \n",
    "#    print '        Output Bias = ', biasOutputWeightArray\n",
    "  \n",
    "\n",
    "          \n",
    "####################################################################################################\n",
    "# Before we start training, get a baseline set of outputs, errors, and SSE \n",
    "####################################################################################################                \n",
    "           \n",
    "    trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')                  \n",
    "    print(' ')\n",
    "    print('  Before training:')\n",
    "    \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray)                           \n",
    "                                             \n",
    "          \n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of randomly-selected training values for alpha-classification \n",
    "####################################################################################################                \n",
    "  \n",
    "  \n",
    "    while iteration < maxNumIterations:           \n",
    "\n",
    "# Increment the iteration count\n",
    "        iteration = iteration +1\n",
    "    \n",
    "# For any given pass, we re-initialize the training list\n",
    "        trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')\n",
    "\n",
    "        inputDataList = []                                \n",
    "                                                                                          \n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        trainingDataList = obtainRandomAlphabetTrainingValues (numTrainingDataSets) \n",
    "\n",
    "\n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "        \n",
    "#        print ' '\n",
    "#        print '  The input data list for letter', trainingDataList[26], ' is:'\n",
    "#        print inputDataList[0], inputDataList[1], inputDataList[2], inputDataList[3], inputDataList[4]\n",
    "#        print inputDataList[5], inputDataList[6], inputDataList[7], inputDataList[8], inputDataList[9]          \n",
    "#        print inputDataList[10], inputDataList[11], inputDataList[12], inputDataList[13], inputDataList[14]\n",
    "#        print inputDataList[15], inputDataList[16], inputDataList[17], inputDataList[18], inputDataList[19]        \n",
    "#        print inputDataList[20], inputDataList[21], inputDataList[22], inputDataList[23], inputDataList[24]\n",
    "#        print ' '\n",
    "    \n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "#        print ' '\n",
    "#        print ' The desired output array values are: '\n",
    "#        print desiredOutputArray  \n",
    "#        print ' '\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "####################################################################################################                \n",
    "                \n",
    "                           \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#        print ' '\n",
    "#        print ' The hidden node activations are:'\n",
    "#        print hiddenArray\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#        print ' '\n",
    "#        print ' The output node activations are:'\n",
    " #       print outputArray    \n",
    "\n",
    "#  Optional alternative code for later use:\n",
    "#  Assign the hidden and output values to specific different variables\n",
    "#    for node in range(hiddenArrayLength):    \n",
    "#        actualHiddenOutput[node] = actualAllNodesOutputList [node]\n",
    "    \n",
    "#    for node in range(outputArrayLength):    \n",
    "#        actualOutput[node] = actualAllNodesOutputList [hiddenArrayLength + node]\n",
    " \n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "# Determine the error between actual and desired outputs        \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print ' '\n",
    "#        print ' The error values are:'\n",
    "#        print errorArray   \n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print 'Initial SSE = %.6f' % newSSE\n",
    "#        SSE = newSSE\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Perform backpropagation\n",
    "####################################################################################################                \n",
    "                \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes    \n",
    "        newVWeightArray = BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray)\n",
    "        newBiasOutputWeightArray = BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray) \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes       \n",
    "        newWWeightArray = BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "\n",
    "        newBiasHiddenWeightArray = BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)  \n",
    "    \n",
    "                    \n",
    "# Assign new values to the weight matrices\n",
    "# Assign the old hidden-to-output weight array to be the same as what was returned from the BP weight update\n",
    "        vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "        biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
    "    \n",
    "# Assign the old input-to-hidden weight array to be the same as what was returned from the BP weight update\n",
    "        wWeightArray = newWWeightArray[:]  \n",
    "    \n",
    "        biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
    "    \n",
    "# Compute a forward pass, test the new SSE                                                                                \n",
    "                                                                                                                                    \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#    print ' '\n",
    "#    print ' The hidden node activations are:'\n",
    "#    print hiddenArray\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#    print ' '\n",
    "#    print ' The output node activations are:'\n",
    "#    print outputArray    \n",
    "\n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print ' '\n",
    "#        print ' The error values are:'\n",
    "#        print errorArray   \n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print 'Previous SSE = %.6f' % SSE\n",
    "#        print 'New SSE = %.6f' % newSSE \n",
    "    \n",
    "#        print ' '\n",
    "#        print 'Iteration number ', iteration\n",
    "#        iteration = iteration + 1\n",
    "\n",
    "        if newSSE < epsilon:\n",
    "\n",
    "            \n",
    "            break\n",
    "    print('Out of while loop at iteration ', iteration)\n",
    "    \n",
    "####################################################################################################\n",
    "# After training, get a new comparative set of outputs, errors, and SSE \n",
    "####################################################################################################                           \n",
    "\n",
    "    print(' ')\n",
    "    print('  After training:')                  \n",
    "                                                      \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray) \n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "                                              \n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "####################################################################################################                \n",
    "    \n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "####################################################################################################\n",
    "# End program\n",
    "#################################################################################################### \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
