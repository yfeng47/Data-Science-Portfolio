{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling the main() procedure from the program\n",
      "Starting main() function\n",
      "\n",
      "Calling welcome() function\n",
      "\n",
      "starting the welcome() function\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "  trained using the backpropagation method.\n",
      "Version 1.0, 03/25/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "ending the welcome() function and returning to main\n",
      "\n",
      "setting alpha, summedInput, maxNumIterations, and eta\n",
      "Setting the debug_xxx_off statements to be false, so will print extra\n",
      "\n",
      "calling the obtainNeuralNetworkSizeSpecs() function\n",
      "Starting the obtainNeuralNetworkSizeSpecs() function\n",
      "\n",
      "This network is set up to run the X-OR problem.\n",
      "The numbers of nodes in the input, hidden, and output layers have been set to 2 each.\n",
      "The number of input nodes is  2\n",
      "The number of hidden nodes is  2\n",
      "The number of output nodes is  2\n",
      "Ending the obtainNeuralNetworkSizeSpecs() function\n",
      "\n",
      "Flow-of-control trace: Back in main\n",
      "I = number of nodes in input layer is 2\n",
      "H = number of nodes in hidden layer is 2\n",
      "O = number of nodes in output layer is 2\n",
      "\n",
      "Calling initializeWeightArray for input-to-hidden weights\n",
      "starting the initializeWeightArray() function\n",
      "initialize wt00 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.9614660034311402\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt01 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.3299879381488191\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt02 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.7220530552135145\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt03 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.7145719564144686\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "  Inside initializeWeightArray\n",
      "\n",
      "    The weights just initialized are: \n",
      "      weight00 = 0.9615,\n",
      "      weight01 = 0.3300,\n",
      "      weight10 = 0.7221,\n",
      "      weight11 = -0.7146,\n",
      "\n",
      "    The weightArray just established is:  [[ 0.961466    0.72205306]\n",
      " [ 0.32998794 -0.71457196]]\n",
      "\n",
      "    Within this array: \n",
      "       weight00 = 0.9615    weight10 = 0.7221\n",
      "       weight01 = 0.3300    weight11 = -0.7146\n",
      "  Returning to calling procedure\n",
      "\n",
      "ending the initializeWeightArray() function and returning to main\n",
      "\n",
      "\n",
      "Calling initializeWeightArray for Hidden-to-Output weights\n",
      "starting the initializeWeightArray() function\n",
      "initialize wt00 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.9832664312132853\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt01 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.2708044365289026\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt02 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.31138744294041865\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt03 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.985659820424303\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "  Inside initializeWeightArray\n",
      "\n",
      "    The weights just initialized are: \n",
      "      weight00 = 0.9833,\n",
      "      weight01 = 0.2708,\n",
      "      weight10 = 0.3114,\n",
      "      weight11 = -0.9857,\n",
      "\n",
      "    The weightArray just established is:  [[ 0.98326643  0.31138744]\n",
      " [ 0.27080444 -0.98565982]]\n",
      "\n",
      "    Within this array: \n",
      "       weight00 = 0.9833    weight10 = 0.3114\n",
      "       weight01 = 0.2708    weight11 = -0.9857\n",
      "  Returning to calling procedure\n",
      "\n",
      "ending the initializeWeightArray() function and returning to main\n",
      "\n",
      "\n",
      "calling initializeBiasWeightArray() function for Hidden nodes\n",
      "\n",
      "starting initializeBiasWeightArray() function\n",
      "initialize biasWeight0 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.9605503027090583\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize biasWeight1 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.17356776663688755\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "biasWeight0 =  -0.9605503027090583\n",
      "biasWeight1 =  0.17356776663688755\n",
      "The biasWeightArray =  [-0.9605503   0.17356777]\n",
      "ending initializeBiasWeightArray() function\n",
      "returning to main\n",
      "\n",
      "\n",
      "calling initializeBiasWeightArray() function for Output nodes\n",
      "\n",
      "starting initializeBiasWeightArray() function\n",
      "initialize biasWeight0 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.4278812583239726\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize biasWeight1 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.020262027320203657\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "biasWeight0 =  -0.4278812583239726\n",
      "biasWeight1 =  0.020262027320203657\n",
      "The biasWeightArray =  [-0.42788126  0.02026203]\n",
      "ending initializeBiasWeightArray() function\n",
      "returning to main\n",
      "\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "       Input-to-Hidden                            Hidden-to-Output\n",
      "  w(0,0) = 0.9615   w(1,0) = 0.7221         v(0,0) = 0.9833   v(1,0) = 0.3114\n",
      "  w(0,1) = 0.3300   w(1,1) = -0.7146         v(0,1) = 0.2708   v(1,1) = -0.9857\n",
      "\n",
      "       Bias at Hidden Layer                          Bias at Output Layer\n",
      "       b(hidden,0) = -0.9606                           b(output,0) = -0.4279\n",
      "       b(hidden,1) = 0.1736                           b(output,1) = 0.0203\n",
      "\n",
      "Establishing parameters - epsilon, iteration counter, SSE_InitialTotal\n",
      "\n",
      "Initialize SSE_InitialArray to zeros\n",
      "calling the computeSSE_Values() function to initialize the SSE_InitialArray\n",
      "starting the computeSSE_Values() function\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.2767680286346613\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is -0.9606\n",
      "    The activation (applied transfer function) for that neuron is 0.2768\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5432833341993574\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is 0.1736\n",
      "    The activation (applied transfer function) for that neuron is 0.5433\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.2768 Hidden1 = 0.5433\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5033567149929414\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.2767680286346613 ,  0.5432833341993574\n",
      "    The summed neuron input is 0.0134\n",
      "    The activation (applied transfer function) for that neuron is 0.5034\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.3916741298736473\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.2767680286346613 ,  0.5432833341993574\n",
      "    The summed neuron input is -0.4403\n",
      "    The activation (applied transfer function) for that neuron is 0.3917\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5034 Output1 = 0.3917\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.2767680286346613\n",
      "the hiddenActivation1 =  0.5432833341993574\n",
      "the outputActivation0 =  0.5033567149929414\n",
      "the outputActivation1 =  0.3916741298736473\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  In computeSSE_Values\n",
      "\n",
      "  Actual Node Outputs for (0,0) training set:\n",
      "      input0 =  0    input1 =  0\n",
      "      actualOutput0 = 0.5034   actualOutput1 = 0.3917\n",
      "      error0 =        -0.5034   error1 =        0.6083\n",
      "   Initial SSE for (0,0) = 0.6234\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for zeroth data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for first data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4406567136291536\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is -0.2385\n",
      "    The activation (applied transfer function) for that neuron is 0.4407\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.36795401306426\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is -0.5410\n",
      "    The activation (applied transfer function) for that neuron is 0.3680\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.4407 Hidden1 = 0.3680\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5299585603348342\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.4406567136291536 ,  0.36795401306426\n",
      "    The summed neuron input is 0.1200\n",
      "    The activation (applied transfer function) for that neuron is 0.5300\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4444592309702055\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.4406567136291536 ,  0.36795401306426\n",
      "    The summed neuron input is -0.2231\n",
      "    The activation (applied transfer function) for that neuron is 0.4445\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5300 Output1 = 0.4445\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.4406567136291536\n",
      "the hiddenActivation1 =  0.36795401306426\n",
      "the outputActivation0 =  0.5299585603348342\n",
      "the outputActivation1 =  0.4444592309702055\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (0,1) training set:\n",
      "     input0 =  0    input1 =  1\n",
      "     actualOutput0 = 0.5300   actualOutput1 = 0.4445\n",
      "     error0 =        0.4700   error1 =        -0.4445\n",
      "  Initial SSE for (0,1) = 0.4185\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for first data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for second data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5002289251645241\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.0009\n",
      "    The activation (applied transfer function) for that neuron is 0.5002\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.6232945704584058\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.5036\n",
      "    The activation (applied transfer function) for that neuron is 0.6233\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.5002 Hidden1 = 0.6233\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5641601127418778\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is 0.2581\n",
      "    The activation (applied transfer function) for that neuron is 0.5642\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.3873108347247935\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is -0.4586\n",
      "    The activation (applied transfer function) for that neuron is 0.3873\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5642 Output1 = 0.3873\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.5002289251645241\n",
      "the hiddenActivation1 =  0.6232945704584058\n",
      "the outputActivation0 =  0.5641601127418778\n",
      "the outputActivation1 =  0.3873108347247935\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (1,0) training set:\n",
      "     input0 =  1    input1 =  0\n",
      "     actualOutput0 = 0.5642   actualOutput1 = 0.3873\n",
      "     error0 =        0.4358   error1 =        -0.3873\n",
      "  Initial SSE for (1,0) = 0.3400\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for second data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for third data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.6732604220617612\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is 0.7230\n",
      "    The activation (applied transfer function) for that neuron is 0.6733\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4474408214808319\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is -0.2110\n",
      "    The activation (applied transfer function) for that neuron is 0.4474\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.6733 Hidden1 = 0.4474\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.592290078477283\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.6732604220617612 ,  0.4474408214808319\n",
      "    The summed neuron input is 0.3734\n",
      "    The activation (applied transfer function) for that neuron is 0.5923\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4406706999185958\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.6732604220617612 ,  0.4474408214808319\n",
      "    The summed neuron input is -0.2384\n",
      "    The activation (applied transfer function) for that neuron is 0.4407\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5923 Output1 = 0.4407\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.6732604220617612\n",
      "the hiddenActivation1 =  0.4474408214808319\n",
      "the outputActivation0 =  0.592290078477283\n",
      "the outputActivation1 =  0.4406706999185958\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (1,1) training set:\n",
      "     input0 =  1    input1 =  1\n",
      "     actualOutput0 = 0.5923   actualOutput1 = 0.4407\n",
      "     error0 =        -0.5923   error1 =        0.5593\n",
      "  Initial SSE for (1,1) = 0.6637\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for third data set\n",
      "\n",
      "Initializing array of SSE values\n",
      "the SSE_InitialArray[0] =  0.6234283467934694 the SSE_InitialArray[1] =  0.41848296299712817\n",
      "the SSE_InitialArray[2] =  0.3399660900203889 the SSE_InitialArray[3] =  0.6636568029921794\n",
      "\n",
      "  The initial total of the SSEs is 2.0455\n",
      "\n",
      "ending the computeSSE_Values() function and returning to main\n",
      "\n",
      "The SSE_Array =  [0.6234283467934694, 0.41848296299712817, 0.3399660900203889, 0.6636568029921794, 2.045534202803166]\n",
      "The SSE_InitialTotal =  2.045534202803166\n",
      "\n",
      "In main, SSE computations completed, Total of all SSEs = 2.0455\n",
      "  For input nodes (0,0), SSE_Array[0] = 0.6234\n",
      "  For input nodes (0,1), SSE_Array[1] = 0.4185\n",
      "  For input nodes (1,0), SSE_Array[2] = 0.3400\n",
      "  For input nodes (1,1), SSE_Array[3] = 0.6637\n",
      "\n",
      "\n",
      "About to enter the while loop for  4  iterations\n",
      "\n",
      "\n",
      "starting at top of while loop for first time\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (1, 0, 1, 0, 2)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  0\n",
      "\n",
      "Randomly selected training data set number  2\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  1             Input1 =  0\n",
      " Desired Output0 =  1    Desired Output1 =  0\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5002289251645241\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.0009\n",
      "    The activation (applied transfer function) for that neuron is 0.5002\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  For hiddenActivation1 from input0, input1 =  1 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.6232945704584058\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.5036\n",
      "    The activation (applied transfer function) for that neuron is 0.6233\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.5002 Hidden1 = 0.6233\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5641601127418778\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is 0.2581\n",
      "    The activation (applied transfer function) for that neuron is 0.5642\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.3873108347247935\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is -0.4586\n",
      "    The activation (applied transfer function) for that neuron is 0.3873\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5642 Output1 = 0.3873\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.5002289251645241\n",
      "the hiddenActivation1 =  0.6232945704584058\n",
      "the outputActivation0 =  0.5641601127418778\n",
      "the outputActivation1 =  0.3873108347247935\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 1 0\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.5002\n",
      "    actualHiddenOutput1 = 0.6233\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.5642\n",
      "    actualOutput1 = 0.3873'\n",
      "  Initial SSE (before backpropagation) = 0.339966\n",
      "  Corresponding SSE (from initial SSE determination) = 0.339966\n",
      "\n",
      "SSE_Array[0] =  0.6234283467934694 SSE_Array[1] =  0.41848296299712817\n",
      "SSE_Array[2] =  0.3399660900203889 SSE_Array[3] =  0.6636568029921794\n",
      "SSE_Array[4] =  2.045534202803166\n",
      "    For node 0: Desired Output =  1  New Output = 0.5642\n",
      "    For node 1: Desired Output =  0  New Output = 0.3873\n",
      "              Error(0) = 0.4358,           Error(1) = -0.3873\n",
      "     Squared Error (0) = 0.1900,   Squared Error(1) = 0.1500\n",
      "\n",
      "  The sum of these squared errors (SSE) for training set  2  is 2.0455\n",
      "Will now add 1 to the iteration counter, and go back to the top of the while loop\n",
      "\n",
      "starting at the top of the while loop again\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (1, 0, 1, 0, 2)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  1\n",
      "\n",
      "Randomly selected training data set number  2\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  1             Input1 =  0\n",
      " Desired Output0 =  1    Desired Output1 =  0\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5002289251645241\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.0009\n",
      "    The activation (applied transfer function) for that neuron is 0.5002\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.6232945704584058\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is 0.5036\n",
      "    The activation (applied transfer function) for that neuron is 0.6233\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.5002 Hidden1 = 0.6233\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5641601127418778\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is 0.2581\n",
      "    The activation (applied transfer function) for that neuron is 0.5642\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.3873108347247935\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.5002289251645241 ,  0.6232945704584058\n",
      "    The summed neuron input is -0.4586\n",
      "    The activation (applied transfer function) for that neuron is 0.3873\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5642 Output1 = 0.3873\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.5002289251645241\n",
      "the hiddenActivation1 =  0.6232945704584058\n",
      "the outputActivation0 =  0.5641601127418778\n",
      "the outputActivation1 =  0.3873108347247935\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 1 0\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.5002\n",
      "    actualHiddenOutput1 = 0.6233\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.5642\n",
      "    actualOutput1 = 0.3873'\n",
      "  Initial SSE (before backpropagation) = 0.339966\n",
      "  Corresponding SSE (from initial SSE determination) = 0.339966\n",
      "\n",
      "SSE_Array[0] =  0.6234283467934694 SSE_Array[1] =  0.41848296299712817\n",
      "SSE_Array[2] =  0.3399660900203889 SSE_Array[3] =  0.6636568029921794\n",
      "SSE_Array[4] =  2.045534202803166\n",
      "    For node 0: Desired Output =  1  New Output = 0.5642\n",
      "    For node 1: Desired Output =  0  New Output = 0.3873\n",
      "              Error(0) = 0.4358,           Error(1) = -0.3873\n",
      "     Squared Error (0) = 0.1900,   Squared Error(1) = 0.1500\n",
      "\n",
      "  The sum of these squared errors (SSE) for training set  2  is 2.0455\n",
      "Will now add 1 to the iteration counter, and go back to the top of the while loop\n",
      "\n",
      "starting at the top of the while loop again\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (1, 1, 0, 1, 3)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  2\n",
      "\n",
      "Randomly selected training data set number  3\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  1             Input1 =  1\n",
      " Desired Output0 =  0    Desired Output1 =  1\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.6732604220617612\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is 0.7230\n",
      "    The activation (applied transfer function) for that neuron is 0.6733\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4474408214808319\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is -0.2110\n",
      "    The activation (applied transfer function) for that neuron is 0.4474\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.6733 Hidden1 = 0.4474\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.592290078477283\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.6732604220617612 ,  0.4474408214808319\n",
      "    The summed neuron input is 0.3734\n",
      "    The activation (applied transfer function) for that neuron is 0.5923\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4406706999185958\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.6732604220617612 ,  0.4474408214808319\n",
      "    The summed neuron input is -0.2384\n",
      "    The activation (applied transfer function) for that neuron is 0.4407\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5923 Output1 = 0.4407\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.6732604220617612\n",
      "the hiddenActivation1 =  0.4474408214808319\n",
      "the outputActivation0 =  0.592290078477283\n",
      "the outputActivation1 =  0.4406706999185958\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 1 1\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.6733\n",
      "    actualHiddenOutput1 = 0.4474\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.5923\n",
      "    actualOutput1 = 0.4407'\n",
      "  Initial SSE (before backpropagation) = 0.663657\n",
      "  Corresponding SSE (from initial SSE determination) = 0.663657\n",
      "\n",
      "SSE_Array[0] =  0.6234283467934694 SSE_Array[1] =  0.41848296299712817\n",
      "SSE_Array[2] =  0.3399660900203889 SSE_Array[3] =  0.6636568029921794\n",
      "SSE_Array[4] =  2.045534202803166\n",
      "    For node 0: Desired Output =  0  New Output = 0.5923\n",
      "    For node 1: Desired Output =  1  New Output = 0.4407\n",
      "              Error(0) = -0.5923,           Error(1) = 0.5593\n",
      "     Squared Error (0) = 0.3508,   Squared Error(1) = 0.3128\n",
      "\n",
      "  The sum of these squared errors (SSE) for training set  3  is 2.0455\n",
      "Will now add 1 to the iteration counter, and go back to the top of the while loop\n",
      "\n",
      "starting at the top of the while loop again\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (0, 1, 1, 0, 1)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  3\n",
      "\n",
      "Randomly selected training data set number  1\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  0             Input1 =  1\n",
      " Desired Output0 =  1    Desired Output1 =  0\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4406567136291536\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is -0.2385\n",
      "    The activation (applied transfer function) for that neuron is 0.4407\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.36795401306426\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is -0.5410\n",
      "    The activation (applied transfer function) for that neuron is 0.3680\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.4407 Hidden1 = 0.3680\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.5299585603348342\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.4406567136291536 ,  0.36795401306426\n",
      "    The summed neuron input is 0.1200\n",
      "    The activation (applied transfer function) for that neuron is 0.5300\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "starting the computeTransferFnction() function\n",
      "The activation is  0.4444592309702055\n",
      "ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.4406567136291536 ,  0.36795401306426\n",
      "    The summed neuron input is -0.2231\n",
      "    The activation (applied transfer function) for that neuron is 0.4445\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.5300 Output1 = 0.4445\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.4406567136291536\n",
      "the hiddenActivation1 =  0.36795401306426\n",
      "the outputActivation0 =  0.5299585603348342\n",
      "the outputActivation1 =  0.4444592309702055\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 0 1\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.4407\n",
      "    actualHiddenOutput1 = 0.3680\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.5300\n",
      "    actualOutput1 = 0.4445'\n",
      "  Initial SSE (before backpropagation) = 0.418483\n",
      "  Corresponding SSE (from initial SSE determination) = 0.418483\n",
      "\n",
      "SSE_Array[0] =  0.6234283467934694 SSE_Array[1] =  0.41848296299712817\n",
      "SSE_Array[2] =  0.3399660900203889 SSE_Array[3] =  0.6636568029921794\n",
      "SSE_Array[4] =  2.045534202803166\n",
      "    For node 0: Desired Output =  1  New Output = 0.5300\n",
      "    For node 1: Desired Output =  0  New Output = 0.4445\n",
      "              Error(0) = 0.4700,           Error(1) = -0.4445\n",
      "     Squared Error (0) = 0.2209,   Squared Error(1) = 0.1975\n",
      "\n",
      "  The sum of these squared errors (SSE) for training set  1  is 2.0455\n",
      "Will now add 1 to the iteration counter, and go back to the top of the while loop\n",
      "\n",
      "Out of while loop after  4  iterations\n",
      " \n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden                       Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      " \n",
      "The final weights for this neural network are:\n",
      "     Input-to-Hidden                       Hidden-to-Output\n",
      "w(0,0) = 0.961   w(0,1) = 0.722         v(0,0) = 0.983   v(0,1) = 0.311\n",
      "w(1,0) = 0.330   w(1,1) = -0.715         v(1,0) = 0.271   v(1,1) = -0.986\n",
      " \n",
      "The SSE values at the beginning of training were: \n",
      "  SSE_Initial[0] = 0.6234\n",
      "  SSE_Initial[1] = 0.4185\n",
      "  SSE_Initial[2] = 0.3400\n",
      "  SSE_Initial[3] = 0.6637\n",
      " \n",
      "The total of the SSE values at the beginning of training is 2.0455\n",
      " \n",
      "The SSE values at the end of training were: \n",
      "  SSE[0] = 0.6234\n",
      "  SSE[1] = 0.4185\n",
      "  SSE[2] = 0.3400\n",
      "  SSE[3] = 0.6637\n",
      " \n",
      "The total of the SSE values at the end of training is 2.0455\n",
      "\n",
      "ending main() function\n"
     ]
    }
   ],
   "source": [
    "# MSDS_458_simple_MLP_no_learning_RT_edits.py\n",
    "# this is Dr. Maren's codeset \"simple_MLP-fixed-size_Python-3-pt-6_tutorial_code_no-learning_debug_2017-03-25.py\" edited\n",
    "\n",
    "\n",
    "# This is the Simple MLP no backprop code. I have removed all backprop code.\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "import random\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# debug near line 583 - deltas did not get the mirror image fix\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Procedure to welcome the user and identify the code\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "# This procedure is called from the 'main' program.\n",
    "# Notice that it has an empty parameter list. \n",
    "# Procedures require a parameter list, but it can be empty. \n",
    "\n",
    "def welcome ():\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('starting the welcome() function')\n",
    "    print(\"******************************************************************************\")\n",
    "    print()\n",
    "    print(\"Welcome to the Multilayer Perceptron Neural Network\")\n",
    "    print(\"  trained using the backpropagation method.\")\n",
    "    print(\"Version 1.0, 03/25/2017, A.J. Maren\")\n",
    "    print(\"For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\")\n",
    "    print()\n",
    "    print(  \"******************************************************************************\")\n",
    "    print()\n",
    "    print('ending the welcome() function and returning to main')\n",
    "    print()\n",
    "    return()\n",
    "\n",
    "# The above 'return()' statement takes us back to the 'main' module. There have been no parameters\n",
    "#   defined in this procedure, there is nothing new to return, so the list is still empty.         \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# A collection of worker-functions, designed to do specific small tasks\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "#------------------------------------------------------#    \n",
    "\n",
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    print()\n",
    "    print('starting the computeTransferFnction() function')\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput))\n",
    "    print('The activation is ', activation)\n",
    "    print('ending the computeTransferFnction() function and returning the activation' )\n",
    "    print()\n",
    "    return activation   \n",
    "\n",
    "#------------------------------------------------------# \n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to obtain the neural network size specifications\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "    print('Starting the obtainNeuralNetworkSizeSpecs() function')\n",
    "\n",
    "# This is a function, not a procedure, because it returns a single value (which really is a list of \n",
    "#    three values). It is called directly from 'main.'\n",
    "#        \n",
    "# This function specifies the size of the input (I), hidden (H), \n",
    "#    and output (O) layers.  \n",
    "# In a more general version of a neural network, this function should ask the user for the numbers\n",
    "#    of nodes at each layer of the network. However, our goal is to start with a very simple program. \n",
    "\n",
    "# Notice that the three values are being stored in a list, the arraySizeList. \n",
    "# This list will be used (by two different functions) to specify the sizes of two different weight arrays:\n",
    "#   - wWeights; the Input-to-Hidden array, and\n",
    "#   - vWeights; the Hidden-to-Output array. \n",
    "\n",
    "# The following is the kind of code that you would use if you wanted to actually give the network \n",
    "#    sizes for each layer yourself. They are commented out in this program, but you could convert them\n",
    "#    to work and obtain your inputs. \n",
    "\n",
    "# Commented-out code: \n",
    "#    print 'Define the numbers of input, hidden, and output nodes'          \n",
    "#    x = input('Enter the number of input nodes here: ')\n",
    "#    numInputNodes = int(x)\n",
    "#    print 'The number of input nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of input nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests\n",
    "                \n",
    "#    x = input('Enter the number of hidden nodes here: ')\n",
    "    # Notice that we are re-using the variable x; it can be reused because it is just catching the \n",
    "    #    user's input and then the real variable of interest is defined using it. \n",
    "    # Notice that by specifying 'int(x)', we make sure that whatever we catch from the user is \n",
    "    #    stored as an integer, not as a float or a string variable.     \n",
    "#    numHiddenNodes = int(x)\n",
    "#    print 'The number of hidden nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of hidden nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests   \n",
    "        \n",
    "#    x = input('Enter the number of output nodes here: ')\n",
    "#    numOutputNodes = int(x)\n",
    "#    print 'The number of output nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of output nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests    \n",
    "\n",
    "    numInputNodes = 2\n",
    "    numHiddenNodes = 2\n",
    "    numOutputNodes = 2   \n",
    "    print()\n",
    "    print(\"This network is set up to run the X-OR problem.\")\n",
    "    print(\"The numbers of nodes in the input, hidden, and output layers have been set to 2 each.\") \n",
    "            \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    print('The number of input nodes is ', numInputNodes)\n",
    "    print('The number of hidden nodes is ', numHiddenNodes)\n",
    "    print('The number of output nodes is ', numOutputNodes)\n",
    "    \n",
    "    print('Ending the obtainNeuralNetworkSizeSpecs() function')\n",
    "    print()\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList)  \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def InitializeWeight ():\n",
    "    \n",
    "    print('starting the InitializeWeight() function')\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "    print('weight = ', weight)\n",
    "    print('ending the InitializWeight() function')\n",
    "    print()\n",
    "    \n",
    "    return (weight)  \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the connection weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeWeightArray (weightArraySizeList, debugInitializeOff=True):\n",
    "    print('starting the initializeWeightArray() function')\n",
    "\n",
    "# This function is also called directly from 'main.'\n",
    "#        \n",
    "# This function takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be given randomly-assigned values, so that we can trace the creation and \n",
    "#   transfer of this array back to the 'main' procedure. \n",
    "  \n",
    "# These connection weight values will be stored in an array, the weightArray. \n",
    "\n",
    "# This code segment is a function, not a procedure, because it returns a single value (which is actually an array). \n",
    "#     The returned value is the new connection weight matrix. \n",
    "#     Right now, for simplicity and test purposes, the weightArray is set to a single value. \n",
    "\n",
    "# Note my variable-naming convention: \n",
    "#    - If it is an array, I call it variableNameArray\n",
    "#    - If it is a list, I call it variableNameList\n",
    "#    - It's a lot like calling your dog Roger \"rogerDog\"\n",
    "#        and your cat Fluffy \"fluffyCat\"\n",
    "#        but until we're better at telling cats from dogs, this helps. \n",
    "      \n",
    "    numBottomNodes = weightArraySizeList[0]\n",
    "    numUpperNodes = weightArraySizeList[1]\n",
    "                \n",
    "\n",
    "# Initialize the weight variables with random weights\n",
    "    print('initialize wt00 by calling initializeWeight() function')\n",
    "    wt00=InitializeWeight ()\n",
    "    print('initialize wt01 by calling initializeWeight() function')\n",
    "    wt01=InitializeWeight ()\n",
    "    print('initialize wt02 by calling initializeWeight() function')\n",
    "    wt10=InitializeWeight ()\n",
    "    print('initialize wt03 by calling initializeWeight() function')\n",
    "    wt11=InitializeWeight ()    \n",
    "    weightArray=np.array([[wt00,wt10],[wt01,wt11]])\n",
    "    \n",
    "# Debug mode: if debug is set to False, then we DO NOT do the prints\n",
    "    if not debugInitializeOff:\n",
    "# Print the weights\n",
    "        print ()\n",
    "        print (\"  Inside initializeWeightArray\")\n",
    "# The following two prints are commented out because we have fixed values for \n",
    "#    the numbers of nodes in each layer for this version of the program        \n",
    "#        print '    The number of lower nodes is', numBottomNodes \n",
    "#        print '    The number of upper nodes is', numUpperNodes  \n",
    "        print ()\n",
    "        print (\"    The weights just initialized are: \")\n",
    "        print (\"      weight00 = %.4f,\"  % wt00)\n",
    "        print (\"      weight01 = %.4f,\"  % wt01)\n",
    "        print (\"      weight10 = %.4f,\"  % wt10)\n",
    "        print (\"      weight11 = %.4f,\"  % wt11)\n",
    "\n",
    "\n",
    "# Remember, this is a tutorial program.\n",
    "#    The following print statements are designed to help you understand how Python\n",
    "#      stores values in an array, and how it prints these out. \n",
    "\n",
    "# The weight array is set up so that it lists the rows, and within the rows, the columns:\n",
    "#    wt00   wt10\n",
    "#    wt01   wt11\n",
    "\n",
    "# The sum of weighted terms should be carried out as follows: \n",
    "#      Matrix Formula     =>   Actual Multiplication     Results\n",
    "#   [wt 00  wt01] * [node0] = wt00*node0 + wt10*node1 = sum-weighted-nodes-to-higher-node0       \n",
    "#   [wt 10  wt11] * [node1] = wt10*node0 + wt11*node1 = sum-weighted-nodes-to-higher-node1\n",
    "\n",
    "# Notice that the weight positions are being labeled according to how Python numbers elements in an array\n",
    "#    ... so the first one is in position [0,0].\n",
    "\n",
    "# Notice that the position of the weights in the weightArray is not as would be expected:\n",
    "#  Row 0, Col 0: wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray [0,0]\n",
    "#  Row 0, Col 1: wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray [0,1]\n",
    "#  Row 1, Col 0: wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray [1,0]\n",
    "#  Row 1, Col 1: wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray [1,1]\n",
    "# Notice that wt01 & wt10 are reversed from what we'd expect \n",
    "\n",
    "# Debug mode: if debug is set to False, then we DO NOT do the prints\n",
    "    if not debugInitializeOff:\n",
    "# Print the entire weights array. \n",
    "        print ()\n",
    "        print (\"    The weightArray just established is: \", weightArray)\n",
    "        print () \n",
    "        print (\"    Within this array: \") \n",
    "        print (\"       weight00 = %.4f    weight10 = %.4f\"  % (weightArray[0,0], weightArray[0,1]) )\n",
    "        print (\"       weight01 = %.4f    weight11 = %.4f\"  % (weightArray[1,0], weightArray[1,1]) )   \n",
    "        print (\"  Returning to calling procedure\") \n",
    "        print()\n",
    "    \n",
    "    print('ending the initializeWeightArray() function and returning to main')\n",
    "    print()\n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------# \n",
    "\n",
    "# Just a few notes to myself ... on array numbering an printing ... \n",
    "\n",
    "# To print a specific element in the weight array; \n",
    "#   recall that Python arrays start with\n",
    "#   numbering the first element as 0-position;\n",
    "#   so index for an array with two elements goes from 0..1\n",
    "\n",
    "# The following prints out the weight connecting the first input node (node 0)\n",
    "#   to the second hidden node (node number 1, as they are numbered 0 .. 1)\n",
    "#    print wWeightArray[0,1]\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to initialize the bias weight arrays\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def initializeBiasWeightArray (weightArray1DSize):\n",
    "    print('starting initializeBiasWeightArray() function')\n",
    "\n",
    "# This procedure is also called directly from 'main.'       \n",
    "# This procedure takes in a single parameters; the number of nodes in a given layer. \n",
    "\n",
    "\n",
    "# This will be more useful when we move to array operations; right now, it is a placeholder step        \n",
    "    numBiasNodes = weightArray1DSize\n",
    "                              \n",
    "# Hard-coding bias values for two nodes; this code really SHOULD be upgraded to deal with \n",
    "#   the number of nodes specified by weightArray1DSize; see subsequent versions for that upgrade. \n",
    "\n",
    "# Initialize the bias weight variables with random weights\n",
    "    print('initialize biasWeight0 by calling initializeWeight() function')\n",
    "    biasWeight0=InitializeWeight ()\n",
    "    print('initialize biasWeight1 by calling initializeWeight() function')\n",
    "    biasWeight1=InitializeWeight ()\n",
    "\n",
    "# Store the two bias weights in a 1-D array            \n",
    "    biasWeightArray=np.array([biasWeight0,biasWeight1])\n",
    "\n",
    "# Notice that the weight positions are being labeled according to how Python numbers elements in a 1-D array\n",
    "#    ... so the first one is in position [0].\n",
    "\n",
    "# Print the entire weights array.\n",
    "    print()\n",
    "    print('biasWeight0 = ', biasWeight0)\n",
    "    print('biasWeight1 = ', biasWeight1)\n",
    "    print('The biasWeightArray = ', biasWeightArray)\n",
    "\n",
    "    print('ending initializeBiasWeightArray() function')\n",
    "    print('returning to main')\n",
    "    print()\n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)  \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Function to obtain a randomly-selected training data set list, which will contain the two input \n",
    "#   values (each either 0 or 1), and two output values (see list below), and a third value which is \n",
    "#   the number of the training data set, in the range of (0..3). (There are a total of four training \n",
    "#   data sets.) \n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def obtainRandomXORTrainingValues ():\n",
    "    \n",
    "    print('starting obtainRandomXORTrainingValues() function')\n",
    "\n",
    "   \n",
    "# The training data list will have four values for the X-OR problem:\n",
    "#   - First two valuea will be the two inputs (0 or 1 for each)\n",
    "#   - Second two values will be the two outputs (0 or 1 for each)\n",
    "# There are only four combinations of 0 and 1 for the input data\n",
    "# Thus there are four choices for training data, which we'll select on random basis\n",
    "    \n",
    "# The fifth element in the list is the NUMBER of the training set; setNumber = 1..3\n",
    "# The setNumber is used to assign the Summed Squared Error (SSE) with the appropriate\n",
    "#   training set\n",
    "# This is because we need ALL the SSE's to get below a certain minimum before we stop\n",
    "#   training the network\n",
    "    \n",
    "    trainingDataSetNum = random.randint(1, 4)\n",
    "    if trainingDataSetNum >1.1: # The selection is for training lists between 2 & 4\n",
    "        if trainingDataSetNum > 2.1: # The selection is for training lists between 3 & 4\n",
    "            if trainingDataSetNum > 3.1: # The selection is for training list 4\n",
    "                trainingDataList = (1,1,0,1,3) # training data list 4 selected\n",
    "            else: trainingDataList = (1,0,1,0,2) # training data list 3 selected   \n",
    "        else: trainingDataList = (0,1,1,0,1) # training data list 2 selected     \n",
    "    else: trainingDataList = (0,0,0,1,0) # training data list 1 selected \n",
    "\n",
    "    print('The trainingDataList = ', trainingDataList)\n",
    "    print('ending obtainRandomXORTrainingValues() function and returning to main')\n",
    "    return (trainingDataList)  \n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Compute neuron activation - this is the summed weighted inputs after passing through transfer fnctn\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def computeSingleNeuronActivation(alpha, wt0, wt1, input0, input1, bias, debugComputeSingleNeuronActivationOff):\n",
    "    print()\n",
    "    print('starting computeSingleNeuronActivation() function')\n",
    "    \n",
    "# Obtain the inputs into the neuron; this is the sum of weights times inputs\n",
    "    summedNeuronInput = wt0*input0+wt1*input1+bias\n",
    "\n",
    "# Pass the summedNeuronActivation and the transfer function parameter alpha into the transfer function\n",
    "    print('compute the activation by calling computeTransferFnctn()')\n",
    "    activation = computeTransferFnctn(summedNeuronInput, alpha)\n",
    "\n",
    "    if not debugComputeSingleNeuronActivationOff:        \n",
    "        print()\n",
    "        print(\"  In computeSingleNeuronActivation with input0, input 1 given as: \", input0, \", \", input1)\n",
    "        print(\"    The summed neuron input is %.4f\" % summedNeuronInput)   \n",
    "        print(\"    The activation (applied transfer function) for that neuron is %.4f\" % activation) \n",
    "        \n",
    "    print('ending computeSingleNeuronActivation() function and returning activation')\n",
    "    print()\n",
    "    return activation;\n",
    "   \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Perform a single feedforward pass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray, \n",
    "biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff):\n",
    "\n",
    "# Some prints to verify that the input data and the weight arrays have been transferred:\n",
    "    print()\n",
    "    print('Starting ComputeSingleFeedforwardPass() function')\n",
    "    input0 = inputDataList[0]\n",
    "    input1 = inputDataList[1]      \n",
    "    print( 'The inputs transferred in are: ')\n",
    "    print('Input0 = ', input0)\n",
    "    print('Input1 = ', input1)     \n",
    "    print()\n",
    "    print('The initial weights for this neural network are:')\n",
    "    print('     Input-to-Hidden             Hidden-to-Output')\n",
    "    # need to correct this code \n",
    "    # switching the w(0,1) value in below statement from wWeightArray[0,1], to [1,0]\n",
    "    # switching the v(0,1) value in below statement from vWeightArray[0,1], to [1,0]\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (wWeightArray[0,0], \n",
    "    wWeightArray[0,1], vWeightArray[0,0], vWeightArray [0,1]))\n",
    "    # need to correct this code\n",
    "    # switching the w(1,0) value in below statement from wWeightArray[1,0], to [0,1]\n",
    "    # switching the v(1,0) value in below statement from vWeightArray[1,0], to [0,1]    \n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n",
    "    wWeightArray[1,1], vWeightArray[1,0], vWeightArray [1,1]))   \n",
    "#    actualOutputList = (0,0) \n",
    "\n",
    "# Recall from InitializeWeightArray: \n",
    "# Notice that the position of the weights in the weightArray is not as would be expected:\n",
    "#  Row 0, Col 1: wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray [0,0]\n",
    "#  Row 0, Col 1: wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray [0,1]\n",
    "#  Row 0, Col 1: wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray [1,0]\n",
    "#  Row 0, Col 1: wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray [1,1]\n",
    "# Notice that wt01 & wt10 are reversed from what we'd expect \n",
    "    \n",
    "            \n",
    "# Assign the input-to-hidden weights to specific variables\n",
    "    wWt00 = wWeightArray[0,0]\n",
    "    wWt10 = wWeightArray[0,1]\n",
    "    wWt01 = wWeightArray[1,0]       \n",
    "    wWt11 = wWeightArray[1,1]\n",
    "    \n",
    "# Assign the hidden-to-output weights to specific variables\n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt10 = vWeightArray[0,1]\n",
    "    vWt01 = vWeightArray[1,0]       \n",
    "    vWt11 = vWeightArray[1,1]    \n",
    "    \n",
    "    biasHidden0 = biasHiddenWeightArray[0]\n",
    "    biasHidden1 = biasHiddenWeightArray[1]\n",
    "    biasOutput0 = biasOutputWeightArray[0]\n",
    "    biasOutput1 = biasOutputWeightArray[1]\n",
    "    \n",
    "# Obtain the activations of the hidden nodes  \n",
    "    if not debugComputeSingleFeedforwardPassOff:\n",
    "        debugComputeSingleNeuronActivationOff = False\n",
    "    else: \n",
    "        debugComputeSingleNeuronActivationOff = True\n",
    "        \n",
    "    if not debugComputeSingleNeuronActivationOff:\n",
    "        print()\n",
    "        print(\"  For hiddenActivation0 from input0, input1 = \", input0, \", \", input1)\n",
    "    \n",
    "    print()\n",
    "    print('computing hiddenActivation0 by calling computeSingleNeuronActivation')\n",
    "    hiddenActivation0 = computeSingleNeuronActivation(alpha, wWt00, wWt10, input0, input1, biasHidden0,\n",
    "    debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    if not debugComputeSingleNeuronActivationOff:\n",
    "        print()\n",
    "        print(\"  For hiddenActivation1 from input0, input1 = \", input0, \", \", input1 )\n",
    "        \n",
    "    print('computing hiddenActivation1 by calling computeSingleNeuronActivation')\n",
    "    hiddenActivation1 = computeSingleNeuronActivation(alpha, wWt01, wWt11, input0, input1, biasHidden1,\n",
    "    debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    if not debugComputeSingleFeedforwardPassOff: \n",
    "        print()\n",
    "        print(\"  In computeSingleFeedforwardPass: \")\n",
    "        print(\"  Input node values: \", input0, \", \", input1)\n",
    "        print(\"  The activations for the hidden nodes are:\")\n",
    "        print(\"    Hidden0 = %.4f\" % hiddenActivation0, \"Hidden1 = %.4f\" % hiddenActivation1)\n",
    "        print()\n",
    "\n",
    "\n",
    "# Obtain the activations of the output nodes \n",
    "    print('computing the activation of output node0 by calling computeSingleNeuronActivation')\n",
    "    outputActivation0 = computeSingleNeuronActivation(alpha, vWt00, vWt10, hiddenActivation0, \n",
    "        hiddenActivation1, biasOutput0, debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    print('computing the activation of output node1 by calling computeSingleNeuronActivation')\n",
    "    outputActivation1 = computeSingleNeuronActivation(alpha, vWt01, vWt11, hiddenActivation0, \n",
    "        hiddenActivation1, biasOutput1, debugComputeSingleNeuronActivationOff)\n",
    "    if not debugComputeSingleFeedforwardPassOff: \n",
    "        print()\n",
    "        print(\"  Computing the output neuron activations\") \n",
    "        print()        \n",
    "        print(\"  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\")\n",
    "        print(\"  The activations for the output nodes are:\")\n",
    "        print(\"    Output0 = %.4f\" % outputActivation0, \"Output1 = %.4f\" % outputActivation1)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('creating the actualAllNodesOutputList')            \n",
    "    actualAllNodesOutputList = (hiddenActivation0, hiddenActivation1, outputActivation0, outputActivation1)\n",
    "    print('the hiddenActivation0 = ', hiddenActivation0)\n",
    "    print('the hiddenActivation1 = ', hiddenActivation1)\n",
    "    print('the outputActivation0 = ', outputActivation0)\n",
    "    print('the outputActivation1 = ', outputActivation1)\n",
    "    print()\n",
    "    print('ending ComputeSingleFeedforwardPass() function and returning to main')\n",
    "    \n",
    "    return (actualAllNodesOutputList);\n",
    "  \n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Determine initial Summed Squared Error (SSE) and Total Summed Squared Error (TotalSSE) as an\n",
    "#   array, SSE_Array (SSE[0], SSE[1], SSE[2], SSE[3], Total_SSE), return array to calling procedure\n",
    "# Note: This function returns the SSE_Array\n",
    "# Note: This function USES computeSingleFeedforwardPass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "def computeSSE_Values (alpha, SSE_InitialArray, wWeightArray, vWeightArray, biasHiddenWeightArray, \n",
    "biasOutputWeightArray, debugSSE_InitialComputationOff): \n",
    "    \n",
    "    print('starting the computeSSE_Values() function')\n",
    "\n",
    "    if not debugSSE_InitialComputationOff:\n",
    "        debugComputeSingleFeedforwardPassOff = False\n",
    "    else: \n",
    "        debugComputeSingleFeedforwardPassOff = True\n",
    "              \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set')\n",
    "    inputDataList = (0, 0)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "        biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 0.0 - actualOutput0\n",
    "    error1 = 1.0 - actualOutput1\n",
    "    SSE_InitialArray[0] = error0**2 + error1**2\n",
    "\n",
    "# debug print for function:\n",
    "    if not debugSSE_InitialComputationOff:\n",
    "        print()\n",
    "        print(\"  In computeSSE_Values\")\n",
    "\n",
    "# debug print for (0,0):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (0,0) training set:\")\n",
    "        print(\"      input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"      actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"      error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"   Initial SSE for (0,0) = %.4f\" % SSE_InitialArray[0])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for zeroth data set')\n",
    "        \n",
    "\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for first data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for first data set')\n",
    "    inputDataList = (0, 1)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 1.0 - actualOutput0\n",
    "    error1 = 0.0 - actualOutput1\n",
    "    SSE_InitialArray[1] = error0**2 + error1**2\n",
    "\n",
    "# debug print for (0,1):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (0,1) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (0,1) = %.4f\" % SSE_InitialArray[1])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for first data set')   \n",
    "                                                        \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for second data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for second data set')\n",
    "    inputDataList = (1, 0)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray, \n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 1.0 - actualOutput0\n",
    "    error1 = 0.0 - actualOutput1\n",
    "    SSE_InitialArray[2] = error0**2 + error1**2\n",
    "    \n",
    "# debug print for (1,0):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (1,0) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (1,0) = %.4f\" % SSE_InitialArray[2] )   \n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for second data set')\n",
    "    \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for third data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for third data set')\n",
    "    inputDataList = (1, 1)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 0.0 - actualOutput0\n",
    "    error1 = 1.0 - actualOutput1\n",
    "    SSE_InitialArray[3] = error0**2 + error1**2\n",
    "\n",
    "# debug print for (1,1):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (1,1) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (1,1) = %.4f\" % SSE_InitialArray[3])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for third data set')\n",
    "    \n",
    "# Initialize an array of SSE values\n",
    "    print()\n",
    "    print('Initializing array of SSE values')\n",
    "    SSE_InitialTotal = SSE_InitialArray[0] + SSE_InitialArray[1] +SSE_InitialArray[2] + SSE_InitialArray[3]\n",
    "    print('the SSE_InitialArray[0] = ', SSE_InitialArray[0], 'the SSE_InitialArray[1] = ', SSE_InitialArray[1])\n",
    "    print('the SSE_InitialArray[2] = ', SSE_InitialArray[2], 'the SSE_InitialArray[3] = ', SSE_InitialArray[3])\n",
    "\n",
    "# debug print for SSE_InitialTotal:\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        print()\n",
    "        print(\"  The initial total of the SSEs is %.4f\" %SSE_InitialTotal)\n",
    "\n",
    "    SSE_InitialArray[4] = SSE_InitialTotal\n",
    "    print()\n",
    "    print('ending the computeSSE_Values() function and returning to main')\n",
    "    print()\n",
    "    \n",
    "    return SSE_InitialArray\n",
    "\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Determine the initial Sum Squared Error (SSE) for each training pair, and also the total SSE\n",
    "# \n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    print('Starting main() function')\n",
    "    print()\n",
    "    print('Calling welcome() function')\n",
    "\n",
    "    welcome()\n",
    "    \n",
    "\n",
    "# Parameter definitions, to be replaced with user inputs\n",
    "    print('setting alpha, summedInput, maxNumIterations, and eta')\n",
    "    alpha = 1.0             # parameter governing steepness of sigmoid transfer function\n",
    "    summedInput = 1\n",
    "    maxNumIterations = 4    # You can adjust this parameter; 10,000 typically gives good results when training. \n",
    "#   To debug, or to see detailed step-by-step results, change maxNumIterations to a very small number\n",
    "#     e.g., maxNumIterations = 10000\n",
    "#   And then set the debug parameters below to be \"False\"      \n",
    "    eta = 0.5               # training rate     \n",
    "    \n",
    "# Set default values for debug parameters\n",
    "#    We are setting the debug parameters to be \"Off\" (debugxxxOff = True)\n",
    "#    This means that we will NOT see most of the print statements\n",
    "#    If we want to see a lot of interim print statements, change either or both of the \n",
    "#      debugxxxOff parameters to be False, e.g., \"debugInitializeOff = False\"\n",
    "    print('Setting the debug_xxx_off statements to be false, so will print extra')\n",
    "    debugCallInitializeOff = False\n",
    "    debugInitializeOff = False    \n",
    "\n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# This defines the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Notice that I'm using the same variable name, 'arraySizeList' both here in main and in the \n",
    "#    called procedure, 'obtainNeuralNetworkSizeSpecs.' \n",
    "# I don't have to use the same name; the procedure returns a list and I'm assigning it HERE \n",
    "#    to the list named arraySizeList in THIS 'main' procedure. \n",
    "# I could use different names. \n",
    "# I'm keeping the same name so that it is easier for us to connect what happens in the called procedure\n",
    "#    'obtainNeuralNetworkSizeSpecs' with this procedure, 'main.' \n",
    "       \n",
    "# Obtain the array sizes (arraySizeList) by calling the appropriate function\n",
    "    print()\n",
    "    print('calling the obtainNeuralNetworkSizeSpecs() function')\n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs()\n",
    "\n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers    \n",
    "    inputArrayLength = arraySizeList[0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# In addition to connection weights, we also use bias weights:\n",
    "#   - one bias term for each of the hidden nodes\n",
    "#   - one bias term for each of the output nodes\n",
    "#   This means that a 1-D array of bias weights for the hidden nodes will have the same dimension as \n",
    "#      the hidden array length, and\n",
    "#   also a 1-D array of bias weights for the output nodes will have the same dimension as\n",
    "#      the output array length. \n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength     \n",
    "\n",
    "# I have all sorts of debug statements left in this, so you can trace the code moving into and out of\n",
    "#   various procedures and functions.    \n",
    "            \n",
    "    print('Flow-of-control trace: Back in main')\n",
    "    print('I = number of nodes in input layer is', inputArrayLength)\n",
    "    print('H = number of nodes in hidden layer is', hiddenArrayLength)         \n",
    "    print('O = number of nodes in output layer is', outputArrayLength)                             \n",
    "\n",
    "\n",
    "# Initialize the training list\n",
    "# Note: The training list has, in order, the two input nodes, the two output nodes (this is a two-output\n",
    "#   version of the X-OR problem), and the data set number (0..3), meaning that each data set is numbered. \n",
    "#   This helps in going through the entire data set once the initial weights are established to get a \n",
    "#   total sum (across all data sets) of the Summed Squared Error, or SSE.\n",
    "\n",
    "# Thus, I am initializing the training data list with FIVE values; \n",
    "#  -- Zeroth and first values: two inputs into the input layer\n",
    "#  -- Second and third values: desired values for the two different nodes in the output layer\n",
    "#  -- Fourth value: the NUMBER of the training data set (which runs from 0 .. 3, for four different data sets)\n",
    "# All of these are being given an initial value of \"0\"; we will get actual values\n",
    "#     when we call a function to return an actual (randomly-selected) training data set. \n",
    "    trainingDataList = (0,0,0,0,0)\n",
    "           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "# We have a single function to initialize weights in a connection weight matrix (2-D array).\n",
    "#   This function needs to know the sizes (lengths) of the lower and the upper sets of nodes.\n",
    "#   These form the [row, column] size specifications for the returned weight matrices (2-D arrays). \n",
    "#   We will store these sizes in each of two different lists. \n",
    "\n",
    "# Specify the sizes for the input-to-hidden connection weight matrix (2-D array)\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "# Specify the sizes for the hidden-to-output connection weight matrix (2-D array)    \n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array    \n",
    "\n",
    "# Debug parameter for examining results within initializeWeightArray is currently set to False\n",
    "\n",
    "    if not debugCallInitializeOff:\n",
    "        print()\n",
    "        print(\"Calling initializeWeightArray for input-to-hidden weights\")\n",
    "\n",
    "# Obtain the actual (randomly-initialized) values for the input-to-hidden connection weight matrix.\n",
    "\n",
    "    wWeightArray = initializeWeightArray(wWeightArraySizeList, debugInitializeOff)\n",
    "\n",
    "    if not debugCallInitializeOff:\n",
    "        print()\n",
    "        print(\"Calling initializeWeightArray for Hidden-to-Output weights\")    \n",
    "\n",
    "# Obtain the actual (randomly-initialized) values for the hidden-to-output connection weight matrix.    \n",
    "    vWeightArray = initializeWeightArray(vWeightArraySizeList, debugInitializeOff)\n",
    "\n",
    "# Now, we similarly need to obtain randomly-initialized values for the two sets of bias weights. \n",
    "#    Each set of bias weights is stored in its respective 1-D array \n",
    "#    Recall that we have previously initialized the SIZE for each of these 1-D arrays.  \n",
    "\n",
    "    print()\n",
    "    print('calling initializeBiasWeightArray() function for Hidden nodes')\n",
    "    print()\n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    print()\n",
    "    print('calling initializeBiasWeightArray() function for Output nodes')\n",
    "    print()\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "\n",
    "# In a typical program, we would start changing the values of all the connection and bias weights.\n",
    "#    Here, we want to keep track of our initial values, so we can see how much they change. \n",
    "#    To do this, we create separate weightArray matrices, and separate biasArrays, and set them equal to\n",
    "#    the initial values that we've just obtained. \n",
    "\n",
    "    initialWWeightArray = wWeightArray[:]\n",
    "    initialVWeightArray = vWeightArray[:]\n",
    "    initialBiasHiddenWeightArray = biasHiddenWeightArray[:]   \n",
    "    initialBiasOutputWeightArray = biasOutputWeightArray[:] \n",
    "    \n",
    "    print()\n",
    "    print(\"The initial weights for this neural network are:\")\n",
    "    print(\"       Input-to-Hidden                            Hidden-to-Output\")\n",
    "    print(\"  w(0,0) = %.4f   w(1,0) = %.4f         v(0,0) = %.4f   v(1,0) = %.4f\" % (initialWWeightArray[0,0], \n",
    "        initialWWeightArray[0,1], initialVWeightArray[0,0], initialVWeightArray[0,1]))\n",
    "    print(\"  w(0,1) = %.4f   w(1,1) = %.4f         v(0,1) = %.4f   v(1,1) = %.4f\" % (initialWWeightArray[1,0], \n",
    "        initialWWeightArray[1,1], initialVWeightArray[1,0], initialVWeightArray[1,1])) \n",
    "    print()\n",
    "    print(\"       Bias at Hidden Layer                          Bias at Output Layer\")\n",
    "    print(\"       b(hidden,0) = %.4f                           b(output,0) = %.4f\" % (biasHiddenWeightArray[0],\n",
    "        biasOutputWeightArray[0] ) )                 \n",
    "    print(\"       b(hidden,1) = %.4f                           b(output,1) = %.4f\" % (biasHiddenWeightArray[1],\n",
    "        biasOutputWeightArray[1] )  )\n",
    "  \n",
    "# Establish some parameters just before we start training\n",
    "    print()\n",
    "    print('Establishing parameters - epsilon, iteration counter, SSE_InitialTotal')\n",
    "    print()\n",
    "    epsilon = 0.2 # epsilon determines when we are done training; \n",
    "                  # for each presentation of a training data set, we get a new value for the summed squared error (SSE)\n",
    "                  # and we will terminate the run when any ONE of these SSEs is < epsilon;\n",
    "                  # Note that this is a very crude stopping criterion, we can refine it in later versions. \n",
    "                  # must b\n",
    "    iteration = 0 # This counts the number of iterations that we've made through the training cycle.\n",
    "    SSE_InitialTotal = 0.0 # We initially set the SSE to be zero before any training pass; we accumulate inputs\n",
    "                           # into the SSE once we have a set of weights, and push the input data through the network,\n",
    "                           # generating a set of outputs. \n",
    "                           # We compare the generated outputs (actuals) with the desired, obtain errors, square them, \n",
    "                           # and sum across all the outputs. This gives our SSE for that particular data set, for that \n",
    "                           # particular training pass. \n",
    "                           # If the SSE is low enough (< epsilon), we stop training. \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Next step - Get an initial value for the Total Summed Squared Error (Total_SSE)\n",
    "#   The function will return an array of SSE values, SSE_Array[0] ... SSE_Array[3] are the initial SSEs\n",
    "#   for training sets 0..3; SSE_Array[4] is the sum of the SSEs. \n",
    "####################################################################################################                \n",
    "        \n",
    "                        \n",
    "# Initialize an array of SSE values\n",
    "# The first four SSE values are the SSE's for specific input/output pairs; \n",
    "#   the fifth is the sum of all the SSE's.\n",
    "    print('Initialize SSE_InitialArray to zeros')\n",
    "    SSE_InitialArray = [0,0,0,0,0]\n",
    "    \n",
    "# Before starting the training run, compute the initial SSE Total \n",
    "#   (sum across SSEs for each training data set) \n",
    "    debugSSE_InitialComputationOff = False\n",
    "\n",
    "    print('calling the computeSSE_Values() function to initialize the SSE_InitialArray')\n",
    "    SSE_InitialArray = computeSSE_Values(alpha, SSE_InitialArray, wWeightArray, vWeightArray, \n",
    "                                          biasHiddenWeightArray, biasOutputWeightArray, debugSSE_InitialComputationOff)    \n",
    "\n",
    "# Start the SSE_Array at the same values as the Initial SSE Array\n",
    "    SSE_Array = SSE_InitialArray[:] \n",
    "    SSE_InitialTotal = SSE_Array[4] \n",
    "    print('The SSE_Array = ', SSE_Array)\n",
    "    print('The SSE_InitialTotal = ', SSE_InitialTotal)\n",
    "    \n",
    "# Optionally, print a summary of the initial SSE Total (sum across SSEs for each training data set) \n",
    "#   and the specific SSE values \n",
    "# Set a local debug print parameter \n",
    "    debugSSE_InitialComputationReportOff = False    \n",
    "\n",
    "    if not debugSSE_InitialComputationReportOff:\n",
    "        print()\n",
    "        print(\"In main, SSE computations completed, Total of all SSEs = %.4f\" % SSE_Array[4])\n",
    "        print(\"  For input nodes (0,0), SSE_Array[0] = %.4f\" % SSE_Array[0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        print(\"  For input nodes (0,1), SSE_Array[1] = %.4f\" % SSE_Array[1]) \n",
    "        print(\"  For input nodes (1,0), SSE_Array[2] = %.4f\" % SSE_Array[2]) \n",
    "        print(\"  For input nodes (1,1), SSE_Array[3] = %.4f\" % SSE_Array[3]) \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"About to enter the while loop for \", maxNumIterations, \" iterations\")\n",
    "    print() \n",
    "    \n",
    "# start the while loop\n",
    "\n",
    "    while iteration < maxNumIterations:\n",
    "        if iteration == 0:\n",
    "            print()\n",
    "            print('starting at top of while loop for first time')\n",
    "        else:\n",
    "            print()\n",
    "            print('starting at the top of the while loop again')\n",
    "           \n",
    "\n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of input values for the X-OR problem; two integers - can be 0 or 1\n",
    "####################################################################################################                \n",
    "\n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        print()\n",
    "        print('create random trainingDataList by calling obtainRandomXORTrainingValues() function')\n",
    "        trainingDataList = obtainRandomXORTrainingValues () \n",
    "        input0 = trainingDataList[0]\n",
    "        input1 = trainingDataList[1] \n",
    "        desiredOutput0 = trainingDataList[2]\n",
    "        desiredOutput1 = trainingDataList[3]\n",
    "        setNumber = trainingDataList[4] # obtain the number (0 ... 3) of the training data set.       \n",
    "        print()\n",
    "        print(\"Iteration number \", iteration)\n",
    "        print()\n",
    "        print(\"Randomly selected training data set number \", trainingDataList[4]) \n",
    "        print(\"The inputs and desired outputs for the X-OR problem from this data set are:\")\n",
    "        print(\"          Input0 = \", input0,         \"            Input1 = \", input1 )  \n",
    "        print(\" Desired Output0 = \", desiredOutput0, \"   Desired Output1 = \", desiredOutput1 )   \n",
    "        print()\n",
    "         \n",
    "\n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass\n",
    "####################################################################################################                \n",
    " \n",
    "# Initialize the error list\n",
    "        errorList = (0,0)\n",
    "    \n",
    "# Initialize the actualOutput list\n",
    "#    Remember, we've hard-coded the number of hidden nodes and output nodes in this version;\n",
    "#      numHiddenNodes = 2; numOutputNodes = 2. \n",
    "#    We want to see the ACTUAL VALUES (\"activations\") of both the hidden AND the output nodes;\n",
    "#      this is just to satisfy our own interest. \n",
    "#    In just a few lines down, we will use the function \"ComputeSingleFeedforwardPass\" to get us all \n",
    "#      of those activations. \n",
    "        actualAllNodesOutputList = (0,0,0,0)     \n",
    "\n",
    "# Create the inputData list      \n",
    "        inputDataList = (input0, input1)         \n",
    "    \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "        debugComputeSingleFeedforwardPassOff = False\n",
    "        print('calling ComputeSingleFeedforwardPass to obtain actual outputs for training set')\n",
    "        actualAllNodesOutputList = ComputeSingleFeedforwardPass(alpha, inputDataList, \n",
    "            wWeightArray, vWeightArray, biasHiddenWeightArray, biasOutputWeightArray,debugComputeSingleFeedforwardPassOff)\n",
    "\n",
    "# Assign the hidden and output values to specific different variables\n",
    "        actualHiddenOutput0 = actualAllNodesOutputList [0] \n",
    "        actualHiddenOutput1 = actualAllNodesOutputList [1] \n",
    "        actualOutput0 = actualAllNodesOutputList [2]\n",
    "        actualOutput1 = actualAllNodesOutputList [3] \n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        error0 = desiredOutput0 - actualOutput0\n",
    "        error1 = desiredOutput1 - actualOutput1\n",
    "        errorList = (error0, error1)\n",
    "    \n",
    "# Compute the Summed Squared Error, or SSE\n",
    "        SSE0 = error0**2\n",
    "        SSE1 =  error1**2\n",
    "        SSEInitial = SSE0 + SSE1\n",
    "        \n",
    "                           \n",
    "        debugMainComputeForwardPassOutputsOff = False\n",
    "        \n",
    "# Debug print: the actual outputs from the two output neurons\n",
    "        if not debugMainComputeForwardPassOutputsOff:\n",
    "            print()\n",
    "            print(\"In main; have just completed a feedfoward pass with training set inputs\", input0, input1)\n",
    "            print(\"  The activations (actual outputs) for the two hidden neurons are:\")\n",
    "            print(\"    actualHiddenOutput0 = %.4f\" % actualHiddenOutput0)\n",
    "            print(\"    actualHiddenOutput1 = %.4f\" % actualHiddenOutput1)   \n",
    "            print(\"  The activations (actual outputs) for the two output neurons are:\")\n",
    "            print(\"    actualOutput0 = %.4f\" % actualOutput0)\n",
    "            print(\"    actualOutput1 = %.4f'\" % actualOutput1 )\n",
    "            print(\"  Initial SSE (before backpropagation) = %.6f\" % SSEInitial)\n",
    "            print(\"  Corresponding SSE (from initial SSE determination) = %.6f\" % SSE_Array[setNumber])    \n",
    "  \n",
    "\n",
    "# Assign the SSE to the SSE for the appropriate training set\n",
    "        SSE_Array[setNumber] = SSEInitial\n",
    "\n",
    "\n",
    "# Compute the new sum of SSEs (across all the different training sets)\n",
    "#   ... this will be different because we've changed one of the SSE's\n",
    "        print()\n",
    "        print(\"SSE_Array[0] = \", SSE_Array[0], \"SSE_Array[1] = \", SSE_Array[1])\n",
    "        print(\"SSE_Array[2] = \", SSE_Array[2], \"SSE_Array[3] = \", SSE_Array[3])\n",
    "        print(\"SSE_Array[4] = \", SSE_Array[4])\n",
    "        \n",
    "        newSSE_Total = SSE_Array[0] + SSE_Array[1] +SSE_Array[2] + SSE_Array[3]\n",
    "        \n",
    "        print(\"    For node 0: Desired Output = \",desiredOutput0,  \" New Output = %.4f\" % actualOutput0 )\n",
    "        print(\"    For node 1: Desired Output = \",desiredOutput1,  \" New Output = %.4f\" % actualOutput1 ) \n",
    "        print(\"              Error(0) = %.4f,           Error(1) = %.4f\" %(error0, error1))\n",
    "        print(\"     Squared Error (0) = %.4f,   Squared Error(1) = %.4f\" %(SSE0, SSE1))  \n",
    "        \n",
    "# Assign the new SSE to the final place in the SSE array\n",
    "        SSE_Array[4] = newSSE_Total\n",
    "        print()\n",
    "        print(\"  The sum of these squared errors (SSE) for training set \", trainingDataList[4], \" is %.4f\" %newSSE_Total )  \n",
    "        \n",
    "        print('Will now add 1 to the iteration counter, and go back to the top of the while loop')\n",
    "        iteration = iteration + 1\n",
    "\n",
    "        if newSSE_Total < epsilon:\n",
    "            print('newSSE_Total is now less than epsilon')\n",
    "\n",
    "            break\n",
    "            \n",
    "    print()\n",
    "    print(\"Out of while loop after \", maxNumIterations, \" iterations\")     \n",
    "\n",
    "\n",
    "    print(' ')\n",
    "    print('The initial weights for this neural network are:')\n",
    "    print('     Input-to-Hidden                       Hidden-to-Output')\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (initialWWeightArray[0,0], \n",
    "        initialWWeightArray[0,1], initialVWeightArray[0,0], initialVWeightArray[0,1]))\n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (initialWWeightArray[1,0], \n",
    "        initialWWeightArray[1,1], initialVWeightArray[1,0], initialVWeightArray[1,1]))        \n",
    "\n",
    "                                                                                    \n",
    "    print(' ')\n",
    "    print('The final weights for this neural network are:')\n",
    "    print('     Input-to-Hidden                       Hidden-to-Output')\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (wWeightArray[0,0], \n",
    "        wWeightArray[0,1], vWeightArray[0,0], vWeightArray[0,1]))\n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n",
    "        wWeightArray[1,1], vWeightArray[1,0], vWeightArray[1,1]))        \n",
    "                                                                                    \n",
    "   \n",
    "# Print the SSE's at the beginning of training\n",
    "    print(' ')\n",
    "    print('The SSE values at the beginning of training were: ')\n",
    "    print('  SSE_Initial[0] = %.4f' % SSE_InitialArray[0])\n",
    "    print('  SSE_Initial[1] = %.4f' % SSE_InitialArray[1])\n",
    "    print('  SSE_Initial[2] = %.4f' % SSE_InitialArray[2])\n",
    "    print('  SSE_Initial[3] = %.4f' % SSE_InitialArray[3])   \n",
    "    print(' ')\n",
    "    print('The total of the SSE values at the beginning of training is %.4f' % SSE_InitialTotal) \n",
    "\n",
    "\n",
    "# Print the SSE's at the end of training\n",
    "    print(' ')\n",
    "    print('The SSE values at the end of training were: ')\n",
    "    print('  SSE[0] = %.4f' % SSE_Array[0])\n",
    "    print('  SSE[1] = %.4f' % SSE_Array[1])\n",
    "    print('  SSE[2] = %.4f' % SSE_Array[2])\n",
    "    print('  SSE[3] = %.4f' % SSE_Array[3])    \n",
    "    print(' ')\n",
    "    print('The total of the SSE values at the end of training is %.4f' % SSE_Array[4])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "# Print comparison of previous and new outputs if using backprop            \n",
    "#    print(' ') \n",
    "#    print('Values for the new outputs compared with previous, given only a partial backpropagation training:')\n",
    "#    print('     Old:', '   ', 'New:', '   ', 'nu*Delta:')\n",
    "#    print('Output 0:  Desired = ', desiredOutput0, 'Old actual =  %.4f' % actualOutput0, 'Newactual  %.4f' % newOutput0)\n",
    "#    print('Output 1:  Desired = ', desiredOutput1, 'Old actual =  %.4f' % actualOutput1, 'Newactual  %.4f' % newOutput1)                                                                        \n",
    "    \n",
    "    print()\n",
    "    print('ending main() function')   \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "#################################################################################################### \n",
    "# This command is what executes the main() function\n",
    "\n",
    "print('Calling the main() procedure from the program')\n",
    "\n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "####################################################################################################\n",
    "# End program\n",
    "#################################################################################################### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "sseArray = np.array([3.989, 3.3658, 2.8527, 2.8325, 2.1682, 2.1545, 2.1353, 2.0982, 2.0658, 2.0920])\n",
    "hidden00 = np.array([-.0267, -.0248, .0189, -.0260, .0176, -.0246, -.0244, -.0252, -.0238, .0187])\n",
    "hidden01 = np.array([.0266, .0266, -.0110, .0262, -.0117, .0266, .0267, .0258, .0268, -.0128])\n",
    "hidden10 = np.array([-.0247, -.0405, .0330, -.0249, .0168, -.0403, -.0398, -.0251, -.0390, .0180])\n",
    "hidden11 = np.array([.0247, .0434, -.0191, .0251, -.0111, .0436, .0437, .0257, .0438, -.0123])\n",
    "\n",
    "desire0 = np.array([0,0,1,0,1,0,0,0,0,1])\n",
    "desire1 = np.array([1,1,0,1,0,1,1,1,1,0])\n",
    "\n",
    "output0 = np.array([.6123, .5464, .5506, .6043, .6131, .5429, .5348, .5888, .5210, .5943])\n",
    "output1 = np.array([.2906, .3068, .3048, .2980, .2942, .3140, .3217, .3114, .3349, .3129])\n",
    "\n",
    "epochs = range(1, len(sseArray) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
