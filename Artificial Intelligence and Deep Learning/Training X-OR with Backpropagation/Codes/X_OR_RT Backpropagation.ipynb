{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSDS_458_X_OR_RT_commented.py\n",
    "# based on MSDS_458_simple_MLP_no_learning_RT_edits.py with backprop functions added back in\n",
    "# and more comments to follow code flow\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "import random\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# debug near line 583 - deltas did not get the mirror image fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This procedure is called from the 'main' program.\n",
    "# Notice that it has an empty parameter list. \n",
    "# Procedures require a parameter list, but it can be empty. \n",
    "\n",
    "def welcome ():\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('starting the welcome() function')\n",
    "    print(\"******************************************************************************\")\n",
    "    print()\n",
    "    print(\"Welcome to the Multilayer Perceptron Neural Network\")\n",
    "    print(\"  trained using the backpropagation method.\")\n",
    "    print(\"Version 1.0, 03/25/2017, A.J. Maren\")\n",
    "    print(\"For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\")\n",
    "    print()\n",
    "    print(  \"******************************************************************************\")\n",
    "    print()\n",
    "    print('ending the welcome() function and returning to main')\n",
    "    print()\n",
    "    return()\n",
    "\n",
    "# The above 'return()' statement takes us back to the 'main' module. There have been no parameters\n",
    "#   defined in this procedure, there is nothing new to return, so the list is still empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    print()\n",
    "    print('Starting the computeTransferFnction() function')\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput))\n",
    "    print('The activation is ', activation)\n",
    "    print('Ending the computeTransferFnction() function and returning the activation' )\n",
    "    print()\n",
    "    return activation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    print()\n",
    "    print('Starting the computeTransferFnctnDeriv() function')\n",
    "    print('This computes the derivate of the transfer function')\n",
    "    print('Ending the computeTransferFnctnDeriv() function') \n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "    print('Starting the obtainNeuralNetworkSizeSpecs() function')\n",
    "\n",
    "# This is a function, not a procedure, because it returns a single value (which really is a list of \n",
    "#    three values). It is called directly from 'main.'\n",
    "#        \n",
    "# This function specifies the size of the input (I), hidden (H), \n",
    "#    and output (O) layers.  \n",
    "# In a more general version of a neural network, this function should ask the user for the numbers\n",
    "#    of nodes at each layer of the network. However, our goal is to start with a very simple program. \n",
    "\n",
    "# Notice that the three values are being stored in a list, the arraySizeList. \n",
    "# This list will be used (by two different functions) to specify the sizes of two different weight arrays:\n",
    "#   - wWeights; the Input-to-Hidden array, and\n",
    "#   - vWeights; the Hidden-to-Output array. \n",
    "\n",
    "# The following is the kind of code that you would use if you wanted to actually give the network \n",
    "#    sizes for each layer yourself. They are commented out in this program, but you could convert them\n",
    "#    to work and obtain your inputs. \n",
    "# Commented-out code: \n",
    "#    print 'Define the numbers of input, hidden, and output nodes'          \n",
    "#    x = input('Enter the number of input nodes here: ')\n",
    "#    numInputNodes = int(x)\n",
    "#    print 'The number of input nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of input nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests\n",
    "                \n",
    "#    x = input('Enter the number of hidden nodes here: ')\n",
    "    # Notice that we are re-using the variable x; it can be reused because it is just catching the \n",
    "    #    user's input and then the real variable of interest is defined using it. \n",
    "    # Notice that by specifying 'int(x)', we make sure that whatever we catch from the user is \n",
    "    #    stored as an integer, not as a float or a string variable.     \n",
    "#    numHiddenNodes = int(x)\n",
    "#    print 'The number of hidden nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of hidden nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests   \n",
    "        \n",
    "#    x = input('Enter the number of output nodes here: ')\n",
    "#    numOutputNodes = int(x)\n",
    "#    print 'The number of output nodes you have entered is', numInputNodes \n",
    "#    print 'Thank you; the number of output nodes that will be used is temporarilly hard-coded to 2.' \n",
    "#    print #to get a line space between requests    \n",
    "\n",
    "    numInputNodes = 2\n",
    "    numHiddenNodes = 2\n",
    "    numOutputNodes = 2   \n",
    "    print()\n",
    "    print(\"This network is set up to run the X-OR problem.\")\n",
    "    print(\"The numbers of nodes in the input, hidden, and output layers have been set to 2 each.\") \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    print('The number of input nodes is ', numInputNodes)\n",
    "    print('The number of hidden nodes is ', numHiddenNodes)\n",
    "    print('The number of output nodes is ', numOutputNodes)\n",
    "    \n",
    "    print('Ending the obtainNeuralNetworkSizeSpecs() function')\n",
    "    print()\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeight ():\n",
    "    \n",
    "    print('starting the InitializeWeight() function')\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "    print('weight = ', weight)\n",
    "    print('ending the InitializWeight() function')\n",
    "    print()\n",
    "    \n",
    "    return (weight)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeightArray (weightArraySizeList, debugInitializeOff=True):\n",
    "    print('starting the initializeWeightArray() function')\n",
    "\n",
    "# This function is also called directly from 'main.'\n",
    "#        \n",
    "# This function takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be given randomly-assigned values, so that we can trace the creation and \n",
    "#   transfer of this array back to the 'main' procedure. \n",
    "  \n",
    "# These connection weight values will be stored in an array, the weightArray. \n",
    "\n",
    "# This code segment is a function, not a procedure, because it returns a single value (which is actually an array). \n",
    "#     The returned value is the new connection weight matrix. \n",
    "#     Right now, for simplicity and test purposes, the weightArray is set to a single value. \n",
    "\n",
    "# Note my variable-naming convention: \n",
    "#    - If it is an array, I call it variableNameArray\n",
    "#    - If it is a list, I call it variableNameList\n",
    "#    - It's a lot like calling your dog Roger \"rogerDog\"\n",
    "#        and your cat Fluffy \"fluffyCat\"\n",
    "#        but until we're better at telling cats from dogs, this helps. \n",
    "      \n",
    "    numBottomNodes = weightArraySizeList[0]\n",
    "    numUpperNodes = weightArraySizeList[1]\n",
    "                \n",
    "\n",
    "# Initialize the weight variables with random weights\n",
    "    print('initialize wt00 by calling initializeWeight() function')\n",
    "    wt00=InitializeWeight ()\n",
    "    print('initialize wt01 by calling initializeWeight() function')\n",
    "    wt01=InitializeWeight ()\n",
    "    print('initialize wt02 by calling initializeWeight() function')\n",
    "    wt10=InitializeWeight ()\n",
    "    print('initialize wt03 by calling initializeWeight() function')\n",
    "    wt11=InitializeWeight ()    \n",
    "    weightArray=np.array([[wt00,wt10],[wt01,wt11]])\n",
    "    \n",
    "# Debug mode: if debug is set to False, then we DO NOT do the prints\n",
    "    if not debugInitializeOff:\n",
    "# Print the weights\n",
    "        print ()\n",
    "        print (\"  Inside initializeWeightArray\")\n",
    "# The following two prints are commented out because we have fixed values for \n",
    "#    the numbers of nodes in each layer for this version of the program        \n",
    "#        print '    The number of lower nodes is', numBottomNodes \n",
    "#        print '    The number of upper nodes is', numUpperNodes  \n",
    "        print ()\n",
    "        print (\"    The weights just initialized are: \")\n",
    "        print (\"      weight00 = %.4f,\"  % wt00)\n",
    "        print (\"      weight01 = %.4f,\"  % wt01)\n",
    "        print (\"      weight10 = %.4f,\"  % wt10)\n",
    "        print (\"      weight11 = %.4f,\"  % wt11)\n",
    "\n",
    "\n",
    "# Remember, this is a tutorial program.\n",
    "#    The following print statements are designed to help you understand how Python\n",
    "#      stores values in an array, and how it prints these out. \n",
    "\n",
    "# The weight array is set up so that it lists the rows, and within the rows, the columns:\n",
    "#    wt00   wt10\n",
    "#    wt01   wt11\n",
    "\n",
    "# The sum of weighted terms should be carried out as follows: \n",
    "#      Matrix Formula     =>   Actual Multiplication     Results\n",
    "#   [wt 00  wt01] * [node0] = wt00*node0 + wt10*node1 = sum-weighted-nodes-to-higher-node0       \n",
    "#   [wt 10  wt11] * [node1] = wt10*node0 + wt11*node1 = sum-weighted-nodes-to-higher-node1\n",
    "\n",
    "# Notice that the weight positions are being labeled according to how Python numbers elements in an array\n",
    "#    ... so the first one is in position [0,0].\n",
    "\n",
    "# Notice that the position of the weights in the weightArray is not as would be expected:\n",
    "#  Row 0, Col 0: wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray [0,0]\n",
    "#  Row 0, Col 1: wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray [0,1]\n",
    "#  Row 1, Col 0: wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray [1,0]\n",
    "#  Row 1, Col 1: wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray [1,1]\n",
    "# Notice that wt01 & wt10 are reversed from what we'd expect \n",
    "\n",
    "# Debug mode: if debug is set to False, then we DO NOT do the prints\n",
    "    if not debugInitializeOff:\n",
    "# Print the entire weights array. \n",
    "        print ()\n",
    "        print (\"    The weightArray just established is: \", weightArray)\n",
    "        print () \n",
    "        print (\"    Within this array: \") \n",
    "        print (\"       weight00 = %.4f    weight10 = %.4f\"  % (weightArray[0,0], weightArray[0,1]) )\n",
    "        print (\"       weight01 = %.4f    weight11 = %.4f\"  % (weightArray[1,0], weightArray[1,1]) )   \n",
    "        print (\"  Returning to calling procedure\") \n",
    "        print()\n",
    "    \n",
    "    print('ending the initializeWeightArray() function and returning to main')\n",
    "    print()\n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeBiasWeightArray (weightArray1DSize):\n",
    "    print('starting initializeBiasWeightArray() function')\n",
    "\n",
    "# This procedure is also called directly from 'main.'       \n",
    "# This procedure takes in the two parameters, the number of nodes on the bottom (of any two layers), \n",
    "#   and the number of nodes in the layer just above it. \n",
    "#   It will use these two sizes to create a weight array.\n",
    "# The weights will initially be given assigned values here, so that we can trace the creation and \n",
    "#   transfer of this array back to the 'main' procedure. \n",
    "# Later, we will randomly define initial weight values.   \n",
    "# These values will be stored in an array, the weightArray. \n",
    "# This procedure is being called as a function; the returned value is the new connection weight matrix. \n",
    "# Right now, for simplicity and test purposes, the weightArray is set to a single value. \n",
    "\n",
    "# Note my variable-naming convention: \n",
    "#    - If it is an array, I call it variableNameArray\n",
    "#    - If it is a list, I call it variableNameList\n",
    "#    - It's a lot like calling your dog Roger \"rogerDog\"\n",
    "#        and your cat Fluffy \"fluffyCat\"\n",
    "#        but until we're better at telling cats from dogs, this helps. \n",
    "\n",
    "\n",
    "# This will be more useful when we move to array operations; right now, it is a placeholder step        \n",
    "    numBiasNodes = weightArray1DSize\n",
    "                           \n",
    "# Hard-coding bias values for two nodes; this code really SHOULD be upgraded to deal with \n",
    "#   the number of nodes specified by weightArray1DSize; see subsequent versions for that upgrade. \n",
    "\n",
    "    # print('The number of lower nodes is', numBottomNodes) \n",
    "    # print('The number of upper nodes is', numUpperNodes) \n",
    "    # print(' ') #to get a line space\n",
    "\n",
    "# Initialize the bias weight variables with random weights\n",
    "    print('initialize biasWeight0 by calling initializeWeight() function')\n",
    "    biasWeight0=InitializeWeight ()\n",
    "    print('initialize biasWeight1 by calling initializeWeight() function')\n",
    "    biasWeight1=InitializeWeight ()\n",
    "\n",
    "# Store the two bias weights in a 1-D array            \n",
    "    biasWeightArray=np.array([biasWeight0,biasWeight1])\n",
    "\n",
    "# Notice that the weight positions are being labeled according to how Python numbers elements in a 1-D array\n",
    "#    ... so the first one is in position [0].\n",
    "\n",
    "# Print the entire weights array.\n",
    "    print()\n",
    "    print('biasWeight0 = ', biasWeight0)\n",
    "    print('biasWeight1 = ', biasWeight1)\n",
    "    print('The biasWeightArray = ', biasWeightArray)\n",
    "\n",
    "    print('ending initializeBiasWeightArray() function')\n",
    "    print('returning to main')\n",
    "    print()\n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainRandomXORTrainingValues ():\n",
    "    \n",
    "    print('starting obtainRandomXORTrainingValues() function')\n",
    "\n",
    "   \n",
    "# The training data list will have four values for the X-OR problem:\n",
    "#   - First two values will be the two inputs (0 or 1 for each)\n",
    "#   - Second two values will be the two outputs (0 or 1 for each)\n",
    "# There are only four combinations of 0 and 1 for the input data\n",
    "# Thus there are four choices for training data, which we'll select on random basis\n",
    "    \n",
    "# The fifth element in the list is the NUMBER of the training set; setNumber = 1..3\n",
    "# The setNumber is used to assign the Summed Squared Error (SSE) with the appropriate\n",
    "#   training set\n",
    "# This is because we need ALL the SSE's to get below a certain minimum before we stop\n",
    "#   training the network\n",
    "    \n",
    "    trainingDataSetNum = random.randint(1, 4)\n",
    "    if trainingDataSetNum >1.1: # The selection is for training lists between 2 & 4\n",
    "        if trainingDataSetNum > 2.1: # The selection is for training lists between 3 & 4\n",
    "            if trainingDataSetNum > 3.1: # The selection is for training list 4\n",
    "                trainingDataList = (1,1,0,1,3) # training data list 4 selected\n",
    "            else: trainingDataList = (1,0,1,0,2) # training data list 3 selected   \n",
    "        else: trainingDataList = (0,1,1,0,1) # training data list 2 selected     \n",
    "    else: trainingDataList = (0,0,0,1,0) # training data list 1 selected \n",
    "\n",
    "    print('The trainingDataList = ', trainingDataList)\n",
    "    print('ending obtainRandomXORTrainingValues() function and returning to main')\n",
    "    return (trainingDataList)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSingleNeuronActivation(alpha, wt0, wt1, input0, input1, bias, debugComputeSingleNeuronActivationOff):\n",
    "    print()\n",
    "    print('starting computeSingleNeuronActivation() function')\n",
    "    \n",
    "# Obtain the inputs into the neuron; this is the sum of weights times inputs\n",
    "    summedNeuronInput = wt0*input0+wt1*input1+bias\n",
    "\n",
    "# Pass the summedNeuronActivation and the transfer function parameter alpha into the transfer function\n",
    "    print('compute the activation by calling computeTransferFnctn()')\n",
    "    activation = computeTransferFnctn(summedNeuronInput, alpha)\n",
    "\n",
    "    if not debugComputeSingleNeuronActivationOff:        \n",
    "        print()\n",
    "        print(\"  In computeSingleNeuronActivation with input0, input 1 given as: \", input0, \", \", input1)\n",
    "        print(\"    The summed neuron input is %.4f\" % summedNeuronInput)   \n",
    "        print(\"    The activation (applied transfer function) for that neuron is %.4f\" % activation) \n",
    "        \n",
    "    print('ending computeSingleNeuronActivation() function and returning activation')\n",
    "    print()\n",
    "    return activation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray, \n",
    "biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff):\n",
    "\n",
    "# Some prints to verify that the input data and the weight arrays have been transferred:\n",
    "    print()\n",
    "    print('Starting ComputeSingleFeedforwardPass() function')\n",
    "    input0 = inputDataList[0]\n",
    "    input1 = inputDataList[1]      \n",
    "    print( 'The inputs transferred in are: ')\n",
    "    print('Input0 = ', input0)\n",
    "    print('Input1 = ', input1)     \n",
    "    print()\n",
    "    print('The initial weights for this neural network are:')\n",
    "    print('     Input-to-Hidden             Hidden-to-Output')\n",
    "    # need to correct this code \n",
    "    # switching the w(0,1) value in below statement from wWeightArray[0,1], to [1,0]\n",
    "    # switching the v(0,1) value in below statement from vWeightArray[0,1], to [1,0]\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (wWeightArray[0,0], \n",
    "    wWeightArray[0,1], vWeightArray[0,0], vWeightArray [0,1]))\n",
    "    # need to correct this code\n",
    "    # switching the w(1,0) value in below statement from wWeightArray[1,0], to [0,1]\n",
    "    # switching the v(1,0) value in below statement from vWeightArray[1,0], to [0,1]    \n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n",
    "    wWeightArray[1,1], vWeightArray[1,0], vWeightArray [1,1]))   \n",
    "#    actualOutputList = (0,0) \n",
    "\n",
    "# Recall from InitializeWeightArray: \n",
    "# Notice that the position of the weights in the weightArray is not as would be expected:\n",
    "#  Row 0, Col 1: wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray [0,0]\n",
    "#  Row 0, Col 1: wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray [0,1]\n",
    "#  Row 0, Col 1: wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray [1,0]\n",
    "#  Row 0, Col 1: wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray [1,1]\n",
    "# Notice that wt01 & wt10 are reversed from what we'd expect \n",
    "    \n",
    "            \n",
    "# Assign the input-to-hidden weights to specific variables\n",
    "    wWt00 = wWeightArray[0,0]\n",
    "    wWt10 = wWeightArray[0,1]\n",
    "    wWt01 = wWeightArray[1,0]       \n",
    "    wWt11 = wWeightArray[1,1]\n",
    "    \n",
    "# Assign the hidden-to-output weights to specific variables\n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt10 = vWeightArray[0,1]\n",
    "    vWt01 = vWeightArray[1,0]       \n",
    "    vWt11 = vWeightArray[1,1]    \n",
    "    \n",
    "    biasHidden0 = biasHiddenWeightArray[0]\n",
    "    biasHidden1 = biasHiddenWeightArray[1]\n",
    "    biasOutput0 = biasOutputWeightArray[0]\n",
    "    biasOutput1 = biasOutputWeightArray[1]\n",
    "    \n",
    "# Obtain the activations of the hidden nodes  \n",
    "    if not debugComputeSingleFeedforwardPassOff:\n",
    "        debugComputeSingleNeuronActivationOff = False\n",
    "    else: \n",
    "        debugComputeSingleNeuronActivationOff = True\n",
    "        \n",
    "    if not debugComputeSingleNeuronActivationOff:\n",
    "        print()\n",
    "        print(\"  For hiddenActivation0 from input0, input1 = \", input0, \", \", input1)\n",
    "    \n",
    "    print()\n",
    "    print('computing hiddenActivation0 by calling computeSingleNeuronActivation')\n",
    "    hiddenActivation0 = computeSingleNeuronActivation(alpha, wWt00, wWt10, input0, input1, biasHidden0,\n",
    "    debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    if not debugComputeSingleNeuronActivationOff:\n",
    "        print()\n",
    "        print(\"  For hiddenActivation1 from input0, input1 = \", input0, \", \", input1 )\n",
    "        \n",
    "    print('computing hiddenActivation1 by calling computeSingleNeuronActivation')\n",
    "    hiddenActivation1 = computeSingleNeuronActivation(alpha, wWt01, wWt11, input0, input1, biasHidden1,\n",
    "    debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    if not debugComputeSingleFeedforwardPassOff: \n",
    "        print()\n",
    "        print(\"  In computeSingleFeedforwardPass: \")\n",
    "        print(\"  Input node values: \", input0, \", \", input1)\n",
    "        print(\"  The activations for the hidden nodes are:\")\n",
    "        print(\"    Hidden0 = %.4f\" % hiddenActivation0, \"Hidden1 = %.4f\" % hiddenActivation1)\n",
    "        print()\n",
    "\n",
    "\n",
    "# Obtain the activations of the output nodes \n",
    "    print('computing the activation of output node0 by calling computeSingleNeuronActivation')\n",
    "    outputActivation0 = computeSingleNeuronActivation(alpha, vWt00, vWt10, hiddenActivation0, \n",
    "        hiddenActivation1, biasOutput0, debugComputeSingleNeuronActivationOff)\n",
    "    \n",
    "    print('computing the activation of output node1 by calling computeSingleNeuronActivation')\n",
    "    outputActivation1 = computeSingleNeuronActivation(alpha, vWt01, vWt11, hiddenActivation0, \n",
    "        hiddenActivation1, biasOutput1, debugComputeSingleNeuronActivationOff)\n",
    "    if not debugComputeSingleFeedforwardPassOff: \n",
    "        print()\n",
    "        print(\"  Computing the output neuron activations\") \n",
    "        print()        \n",
    "        print(\"  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\")\n",
    "        print(\"  The activations for the output nodes are:\")\n",
    "        print(\"    Output0 = %.4f\" % outputActivation0, \"Output1 = %.4f\" % outputActivation1)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('creating the actualAllNodesOutputList')            \n",
    "    actualAllNodesOutputList = (hiddenActivation0, hiddenActivation1, outputActivation0, outputActivation1)\n",
    "    print('the hiddenActivation0 = ', hiddenActivation0)\n",
    "    print('the hiddenActivation1 = ', hiddenActivation1)\n",
    "    print('the outputActivation0 = ', outputActivation0)\n",
    "    print('the outputActivation1 = ', outputActivation1)\n",
    "    print()\n",
    "    print('ending ComputeSingleFeedforwardPass() function and returning to main')\n",
    "    \n",
    "    return (actualAllNodesOutputList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSSE_Values (alpha, SSE_InitialArray, wWeightArray, vWeightArray, biasHiddenWeightArray, \n",
    "biasOutputWeightArray, debugSSE_InitialComputationOff): \n",
    "    \n",
    "    print('starting the computeSSE_Values() function')\n",
    "\n",
    "    if not debugSSE_InitialComputationOff:\n",
    "        debugComputeSingleFeedforwardPassOff = False\n",
    "    else: \n",
    "        debugComputeSingleFeedforwardPassOff = True\n",
    "              \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set')\n",
    "    inputDataList = (0, 0)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "        biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 0.0 - actualOutput0\n",
    "    error1 = 1.0 - actualOutput1\n",
    "    SSE_InitialArray[0] = error0**2 + error1**2\n",
    "\n",
    "# debug print for function:\n",
    "    if not debugSSE_InitialComputationOff:\n",
    "        print()\n",
    "        print(\"  In computeSSE_Values\")\n",
    "\n",
    "# debug print for (0,0):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (0,0) training set:\")\n",
    "        print(\"      input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"      actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"      error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"   Initial SSE for (0,0) = %.4f\" % SSE_InitialArray[0])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for zeroth data set')\n",
    "        \n",
    "\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for first data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for first data set')\n",
    "    inputDataList = (0, 1)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 1.0 - actualOutput0\n",
    "    error1 = 0.0 - actualOutput1\n",
    "    SSE_InitialArray[1] = error0**2 + error1**2\n",
    "\n",
    "# debug print for (0,1):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (0,1) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (0,1) = %.4f\" % SSE_InitialArray[1])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for first data set')   \n",
    "                                                        \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for second data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for second data set')\n",
    "    inputDataList = (1, 0)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray, \n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 1.0 - actualOutput0\n",
    "    error1 = 0.0 - actualOutput1\n",
    "    SSE_InitialArray[2] = error0**2 + error1**2\n",
    "    \n",
    "# debug print for (1,0):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (1,0) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (1,0) = %.4f\" % SSE_InitialArray[2] )   \n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for second data set')\n",
    "    \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs for third data set\n",
    "    print()\n",
    "    print('Compute a single feed-forward pass and obtain the Actual Outputs for third data set')\n",
    "    inputDataList = (1, 1)           \n",
    "    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "    biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)        \n",
    "    actualOutput0 = actualAllNodesOutputList [2]\n",
    "    actualOutput1 = actualAllNodesOutputList [3] \n",
    "    error0 = 0.0 - actualOutput0\n",
    "    error1 = 1.0 - actualOutput1\n",
    "    SSE_InitialArray[3] = error0**2 + error1**2\n",
    "\n",
    "# debug print for (1,1):\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        input0 = inputDataList [0]\n",
    "        input1 = inputDataList [1]\n",
    "        print()\n",
    "        print(\"  Actual Node Outputs for (1,1) training set:\")\n",
    "        print(\"     input0 = \", input0, \"   input1 = \", input1)\n",
    "        print(\"     actualOutput0 = %.4f   actualOutput1 = %.4f\" %(actualOutput0, actualOutput1))\n",
    "        print(\"     error0 =        %.4f   error1 =        %.4f\" %(error0, error1))\n",
    "        print(\"  Initial SSE for (1,1) = %.4f\" % SSE_InitialArray[3])\n",
    "    print()\n",
    "    print('Ending a single feed-forward pass to obtain the Actual Outputs for third data set')\n",
    "    \n",
    "# Initialize an array of SSE values\n",
    "    print()\n",
    "    print('Initializing array of SSE values')\n",
    "    SSE_InitialTotal = SSE_InitialArray[0] + SSE_InitialArray[1] +SSE_InitialArray[2] + SSE_InitialArray[3]\n",
    "    print('the SSE_InitialArray[0] = ', SSE_InitialArray[0], 'the SSE_InitialArray[1] = ', SSE_InitialArray[1])\n",
    "    print('the SSE_InitialArray[2] = ', SSE_InitialArray[2], 'the SSE_InitialArray[3] = ', SSE_InitialArray[3])\n",
    "\n",
    "# debug print for SSE_InitialTotal:\n",
    "    if not debugSSE_InitialComputationOff: \n",
    "        print()\n",
    "        print(\"  The initial total of the SSEs is %.4f\" %SSE_InitialTotal)\n",
    "\n",
    "    SSE_InitialArray[4] = SSE_InitialTotal\n",
    "    print()\n",
    "    print('ending the computeSSE_Values() function and returning to main')\n",
    "    print()\n",
    "    \n",
    "    return SSE_InitialArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList, \n",
    "actualAllNodesOutputList, transFuncDerivList, deltaVWtArray, vWeightArray, newVWeightArray):\n",
    "    print()\n",
    "    print('Starting PrintAndTraceBackpropagateOutputToHidden() function')\n",
    "\n",
    "    hiddenNode0 = actualAllNodesOutputList[0]\n",
    "    hiddenNode1 = actualAllNodesOutputList[1]\n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]\n",
    "    transFuncDeriv0 = transFuncDerivList[0]\n",
    "    transFuncDeriv1 = transFuncDerivList[1]\n",
    "#  Pre Debug: These variables did not get the mirror image fix\n",
    "#    deltaVWt00 = deltaVWtArray[0,0]\n",
    "#    deltaVWt01 = deltaVWtArray[0,1]\n",
    "#    deltaVWt10 = deltaVWtArray[1,0]\n",
    "#    deltaVWt11 = deltaVWtArray[1,1]\n",
    "\n",
    "#  New code: \n",
    "    deltaVWt00 = deltaVWtArray[0,0]\n",
    "    deltaVWt01 = deltaVWtArray[1,0]\n",
    "    deltaVWt10 = deltaVWtArray[0,1]\n",
    "    deltaVWt11 = deltaVWtArray[1,1]    \n",
    "    \n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]                 \n",
    "        \n",
    "    print(' ')\n",
    "    print('In Print and Trace for Backpropagation: Hidden to Output Weights')\n",
    "    print('  Assuming alpha = 1')\n",
    "    print(' ')\n",
    "    print('  The hidden node activations are:')\n",
    "    print('    Hidden node 0: ', '  %.4f' % hiddenNode0, '  Hidden node 1: ', '  %.4f' % hiddenNode1)   \n",
    "    print(' ')\n",
    "    print('  The output node activations are:')\n",
    "    print('    Output node 0: ', '  %.3f' % outputNode0, '   Output node 1: ', '  %.3f' % outputNode1)       \n",
    "    print(' ') \n",
    "    print('  The transfer function derivatives are: ')\n",
    "    print('    Deriv-F(0): ', '     %.3f' % transFuncDeriv0, '   Deriv-F(1): ', '     %.3f' % transFuncDeriv1)\n",
    "\n",
    "    print(' ') \n",
    "    print('The computed values for the deltas are: ')\n",
    "    print('                eta  *  error  *   trFncDeriv *   hidden')\n",
    "    print('  deltaVWt00 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDeriv0, '  * %.4f' % hiddenNode0)\n",
    "    print('  deltaVWt01 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDeriv1, '  * %.4f' % hiddenNode0)                       \n",
    "    print('  deltaVWt10 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDeriv0, '  * %.4f' % hiddenNode1)\n",
    "    print('  deltaVWt11 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDeriv1, '  * %.4f' % hiddenNode1)\n",
    "    print(' ')\n",
    "    print('Values for the hidden-to-output connection weights:')\n",
    "    print('           Old:     New:      eta*Delta:')\n",
    "    print('[0,0]:   %.4f' % vWeightArray[0,0], '  %.4f' % newVWeightArray[0,0], '  %.4f' % deltaVWtArray[0,0])\n",
    "    print('[0,1]:   %.4f' % vWeightArray[1,0], '  %.4f' % newVWeightArray[1,0], '  %.4f' % deltaVWtArray[1,0])\n",
    "    print('[1,0]:   %.4f' % vWeightArray[0,1], '  %.4f' % newVWeightArray[0,1], '  %.4f' % deltaVWtArray[0,1])\n",
    "    print('[1,1]:   %.4f' % vWeightArray[1,1], '  %.4f' % newVWeightArray[1,1], '  %.4f' % deltaVWtArray[1,1])\n",
    "    \n",
    "    print()\n",
    "    print('Ending PrintAndTraceBackpropagateOutputToHidden() function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintAndTraceBackpropagateHiddenToInput (alpha, eta, inputDataList, errorList, \n",
    "actualAllNodesOutputList, transFuncDerivHiddenList, transFuncDerivOutputList, deltaWWtArray, vWeightArray, wWeightArray, \n",
    "newWWeightArray, biasWeightArray):\n",
    "    print()\n",
    "    print('Starting PrintAndTraceBackpropagateHiddenToInput() function')\n",
    "    print('Traces the backpropagation of the input-to-hidden weights')\n",
    "\n",
    "    inputNode0 = inputDataList[0]\n",
    "    inputNode1 = inputDataList[1]    \n",
    "    hiddenNode0 = actualAllNodesOutputList[0]\n",
    "    hiddenNode1 = actualAllNodesOutputList[1]\n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]\n",
    "    transFuncDerivHidden0 = transFuncDerivHiddenList[0]\n",
    "    transFuncDerivHidden1 = transFuncDerivHiddenList[1]\n",
    "    transFuncDerivOutput0 = transFuncDerivOutputList[0]\n",
    "    transFuncDerivOutput1 = transFuncDerivOutputList[1]    \n",
    "            \n",
    "#  Pre Debug: These variables did not get the mirror image fix\n",
    "#    deltaVWt00 = deltaVWtArray[0,0]\n",
    "#    deltaVWt01 = deltaVWtArray[0,1]\n",
    "#    deltaVWt10 = deltaVWtArray[1,0]\n",
    "#    deltaVWt11 = deltaVWtArray[1,1]\n",
    "\n",
    "#  New code: \n",
    "    wWt00 = wWeightArray[0,0]\n",
    "    wWt01 = wWeightArray[1,0]\n",
    "    wWt10 = wWeightArray[0,1]       \n",
    "    wWt11 = wWeightArray[1,1]\n",
    "    \n",
    "    deltaWWt00 = deltaWWtArray[0,0]\n",
    "    deltaWWt01 = deltaWWtArray[1,0]\n",
    "    deltaWWt10 = deltaWWtArray[0,1]\n",
    "    deltaWWt11 = deltaWWtArray[1,1] \n",
    "    \n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt01 = vWeightArray[1,0]\n",
    "    vWt10 = vWeightArray[0,1]\n",
    "    vWt11 = vWeightArray[1,1]                \n",
    "    \n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]\n",
    "\n",
    "    errorTimesTransD0 = error0*transFuncDerivOutput0\n",
    "    errorTimesTransD1 = error1*transFuncDerivOutput1    \n",
    "            \n",
    "    biasHidden0 = biasWeightArray[0,0]\n",
    "    biasHidden1 = biasWeightArray[0,1]                                      \n",
    "\n",
    "    partialSSE_w_Wwt00 = -transFuncDerivHidden0*inputNode0*(vWt00*error0 + vWt01*error1)                                                             \n",
    "    partialSSE_w_Wwt01 = -transFuncDerivHidden1*inputNode0*(vWt10*error0 + vWt11*error1)\n",
    "    partialSSE_w_Wwt10 = -transFuncDerivHidden0*inputNode1*(vWt00*error0 + vWt01*error1)\n",
    "    partialSSE_w_Wwt11 = -transFuncDerivHidden1*inputNode1*(vWt10*error0 + vWt11*error1)\n",
    "    \n",
    "    sumTermH0 = vWt00*error0+vWt01*error1\n",
    "    sumTermH1 = vWt10*error0+vWt11*error1\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "                   \n",
    "                                \n",
    "    print(' ')\n",
    "    print('In Print and Trace for Backpropagation: Input to Hidden Weights')\n",
    "    print('  Assuming alpha = 1')\n",
    "    print(' ')\n",
    "    print('  The hidden node activations are:')\n",
    "    print('    Hidden node 0: ', '  %.4f' % hiddenNode0, '  Hidden node 1: ', '  %.4f' % hiddenNode1)   \n",
    "    print(' ')\n",
    "    print('  The output node activations are:')\n",
    "    print('    Output node 0: ', '  %.3f' % outputNode0, '   Output node 1: ', '  %.3f' % outputNode1)       \n",
    "    print(' ') \n",
    "    print('  The transfer function derivatives at the hidden nodes are: ')\n",
    "    # I edited code to reflect transFuncDerivHidden1 instead of what was in there\n",
    "    print('    Deriv-F(0): ', '     %.3f' % transFuncDerivHidden0, '   Deriv-F(1): ', '     %.3f' % transFuncDerivHidden1)\n",
    "\n",
    "    print(' ') \n",
    "    print('The computed values for the deltas are: ')\n",
    "    print('                eta  *  error  *   trFncDeriv *   input    * SumTerm for given H')\n",
    "    print('  deltaWWt00 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDerivHidden0, '  * %.4f' % inputNode0, '  * %.4f' % sumTermH0)\n",
    "    print('  deltaWWt01 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDerivHidden1, '  * %.4f' % inputNode0, '  * %.4f' % sumTermH1)                       \n",
    "    print('  deltaWWt10 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDerivHidden0, '  * %.4f' % inputNode1, '  * %.4f' % sumTermH0)\n",
    "    print('  deltaWWt11 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDerivHidden1, '  * %.4f' % inputNode1, '  * %.4f' % sumTermH1)\n",
    "    print(' ')\n",
    "    print('Values for the input-to-hidden connection weights:')\n",
    "    print('           Old:     New:      eta*Delta:')\n",
    "    print('[0,0]:   %.4f' % wWeightArray[0,0], '  %.4f' % newWWeightArray[0,0], '  %.4f' % deltaWWtArray[0,0])\n",
    "    print('[0,1]:   %.4f' % wWeightArray[1,0], '  %.4f' % newWWeightArray[1,0], '  %.4f' % deltaWWtArray[1,0])\n",
    "    print('[1,0]:   %.4f' % wWeightArray[0,1], '  %.4f' % newWWeightArray[0,1], '  %.4f' % deltaWWtArray[0,1])\n",
    "    print('[1,1]:   %.4f' % wWeightArray[1,1], '  %.4f' % newWWeightArray[1,1], '  %.4f' % deltaWWtArray[1,1])\n",
    "    \n",
    "    print()\n",
    "    print('Ending the PrintAndTraceBackpropagateHiddenToInput() function')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList, vWeightArray):\n",
    "    print()\n",
    "    print('Starting the BackpropagateOutputToHidden() function')\n",
    "    print('Performs the backpropagation of weight changes onto hidden-to-output weigts')\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight v. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]\n",
    "    \n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt01 = vWeightArray[1,0]\n",
    "    vWt10 = vWeightArray[0,1]       \n",
    "    vWt11 = vWeightArray[1,1]  \n",
    "    \n",
    "    hiddenNode0 = actualAllNodesOutputList[0]\n",
    "    hiddenNode1 = actualAllNodesOutputList[1]\n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]  \n",
    "        \n",
    "    transFuncDeriv0 = computeTransferFnctnDeriv(outputNode0, alpha) \n",
    "    transFuncDeriv1 = computeTransferFnctnDeriv(outputNode1, alpha)\n",
    "    transFuncDerivList = (transFuncDeriv0, transFuncDeriv1) \n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "# The equation for the actual dependence of the Summed Squared Error on a given hidden-to-output weight v(h,o) is:\n",
    "#   partial(SSE)/partial(v(h,o)) = -alpha*E(o)*F(o)*[1-F(o)]*H(h)\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput)\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(v(h,o)) as\n",
    "#   partial(SSE)/partial(v(h,o)) = E(o)*transFuncDeriv*H(h)\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    partialSSE_w_Vwt00 = -error0*transFuncDeriv0*hiddenNode0                                                             \n",
    "    partialSSE_w_Vwt01 = -error1*transFuncDeriv1*hiddenNode0\n",
    "    partialSSE_w_Vwt10 = -error0*transFuncDeriv0*hiddenNode1\n",
    "    partialSSE_w_Vwt11 = -error1*transFuncDeriv1*hiddenNode1                                                                                                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                                                                \n",
    "    deltaVWt00 = -eta*partialSSE_w_Vwt00\n",
    "    deltaVWt01 = -eta*partialSSE_w_Vwt01        \n",
    "    deltaVWt10 = -eta*partialSSE_w_Vwt10\n",
    "    deltaVWt11 = -eta*partialSSE_w_Vwt11 \n",
    "    deltaVWtArray = np.array([[deltaVWt00, deltaVWt10],[deltaVWt01, deltaVWt11]])\n",
    "\n",
    "    vWt00 = vWt00+deltaVWt00\n",
    "    vWt01 = vWt01+deltaVWt01\n",
    "    vWt10 = vWt10+deltaVWt10\n",
    "    vWt11 = vWt11+deltaVWt11 \n",
    "    \n",
    "    newVWeightArray = np.array([[vWt00, vWt10], [vWt01, vWt11]])\n",
    "\n",
    "    print('Calling the PrintAndTraceBackpropagateOutputToHidden() function')\n",
    "\n",
    "    PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "    transFuncDerivList, deltaVWtArray, vWeightArray, newVWeightArray)\n",
    "\n",
    "    print()\n",
    "    print('Back in the BackpropagateOutputToHidden() function')\n",
    "\n",
    "    print()\n",
    "    print('Ending the BackpropagateOutputToHidden() function')    \n",
    "                                                                                                                                            \n",
    "    return (newVWeightArray); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateBiasOutputWeights (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "biasOutputWeightArray):\n",
    "    print()\n",
    "    print('Starting the BackpropagateBiasOutputWeights() function')\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Unpack the errorList \n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]\n",
    "\n",
    "# Unpack the biasOutputWeightArray, we will only be modifying the biasOutput terms   \n",
    "    biasOutputWt0 = biasOutputWeightArray[0]\n",
    "    biasOutputWt1 = biasOutputWeightArray[1]\n",
    "    \n",
    "# Unpack the outputNodes  \n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]  \n",
    "    \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence\n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode0')              \n",
    "    transFuncDeriv0 = computeTransferFnctnDeriv(outputNode0, alpha) \n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode1')\n",
    "    transFuncDeriv1 = computeTransferFnctnDeriv(outputNode1, alpha)\n",
    "# This is actually an unnecessary step; we're not passing the list back. \n",
    "    transFuncDerivList = (transFuncDeriv0, transFuncDeriv1) \n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    \n",
    "    partialSSE_w_BiasOutput0 = -error0*transFuncDeriv0\n",
    "    partialSSE_w_BiasOutput1 = -error1*transFuncDeriv1    \n",
    "                                                                                                                                                                                                                                                                \n",
    "    deltaBiasOutput0 = -eta*partialSSE_w_BiasOutput0\n",
    "    deltaBiasOutput1 = -eta*partialSSE_w_BiasOutput1\n",
    "\n",
    "    biasOutputWt0 = biasOutputWt0+deltaBiasOutput0\n",
    "    biasOutputWt1 = biasOutputWt1+deltaBiasOutput1 \n",
    "\n",
    "# Note that only the bias weights for the output nodes have been changed.         \n",
    "    newBiasOutputWeightArray = np.array([biasOutputWt0, biasOutputWt1])\n",
    "\n",
    "#    A new procedure to debug trace this function has not been written yet. \n",
    "#    PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newVWeightArray) \n",
    "\n",
    "    print()\n",
    "    print('The newBiasOutputWeightArray = ', newBiasOutputWeightArray)\n",
    "\n",
    "    print()\n",
    "    print('Ending the BackpropagateBiasOutputWeights() function')   \n",
    "                                                                                                                                            \n",
    "    return (newBiasOutputWeightArray); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateHiddenToInput (alpha, eta, errorList, actualAllNodesOutputList, inputDataList, \n",
    "vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "    print()\n",
    "    print('Starting the BackpropagateHiddenToInput() function')\n",
    "    print('This Backpropagates the weight changes onto the input-to-hidden weights')\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the input-to-hidden wts w. \n",
    "# Core equation for the second part of backpropagation: \n",
    "# d(SSE)/dw(i,h) = -eta*alpha*F(h)(1-F(h))*Input(i)*sum(v(h,o)*Error(o))\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- w(i,h) is the connection weight w between the input node i and the hidden node h\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1 \n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# ---- NOTE: in this second step, the transfer function is applied to the output of the hidden node,\n",
    "# ------ so that F = F(h)\n",
    "# -- Hidden(h) = the output of hidden node h (used in computing the derivative of the transfer function). \n",
    "# -- Input(i) = the input at node i.\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "\n",
    "# We will DECREMENT the connection weight v by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight w. \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# Unpack the errorList and the vWeightArray\n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]\n",
    "    \n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt01 = vWeightArray[1,0]\n",
    "    vWt10 = vWeightArray[0,1]       \n",
    "    vWt11 = vWeightArray[1,1]  \n",
    "    \n",
    "    wWt00 = wWeightArray[0,0]\n",
    "    wWt01 = wWeightArray[1,0]\n",
    "    wWt10 = wWeightArray[0,1]       \n",
    "    wWt11 = wWeightArray[1,1] \n",
    "    \n",
    "    inputNode0 = inputDataList[0] \n",
    "    inputNode1 = inputDataList[1]         \n",
    "    hiddenNode0 = actualAllNodesOutputList[0]\n",
    "    hiddenNode1 = actualAllNodesOutputList[1]\n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]\n",
    "\n",
    "# This is RT Code - to try and correct error that I recieved in calling the\n",
    "# PrintAndTraceBackpropagateHiddenToInput function, and got the error that\n",
    "# biasWeightArray is not defined;  I think this should be the combined\n",
    "# biasHiddenWeightArray and biasOuputWeightArray inputted as arguments to\n",
    "# this BackpropagateHiddenToInput function\n",
    "    biasWeightArray = np.array([biasHiddenWeightArray, biasOutputWeightArray])\n",
    "    \n",
    "# For the second step in backpropagation (computing deltas on the input-to-hidden weights)\n",
    "#   we need the transfer function derivative is applied to the output at the hidden node   \n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for hiddenNode0')     \n",
    "    transFuncDerivHidden0 = computeTransferFnctnDeriv(hiddenNode0, alpha)\n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for hiddenNode1') \n",
    "    transFuncDerivHidden1 = computeTransferFnctnDeriv(hiddenNode1, alpha)\n",
    "    transFuncDerivHiddenList = (transFuncDerivHidden0, transFuncDerivHidden1) \n",
    "    \n",
    "# We also need the transfer function derivative applied to the output at the output node\n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode0')\n",
    "    transFuncDerivOutput0 = computeTransferFnctnDeriv(outputNode0, alpha) \n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode1')\n",
    "    transFuncDerivOutput1 = computeTransferFnctnDeriv(outputNode1, alpha)\n",
    "    transFuncDerivOutputList = (transFuncDerivOutput0, transFuncDerivOutput1) \n",
    "    \n",
    "    errorTimesTransDOutput0 = error0*transFuncDerivOutput0\n",
    "    errorTimesTransDOutput1 = error1*transFuncDerivOutput1\n",
    "            \n",
    "               \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations\n",
    "    partialSSE_w_Wwt00 = -transFuncDerivHidden0*inputNode0*(vWt00*errorTimesTransDOutput0 + vWt01*errorTimesTransDOutput1)                                                             \n",
    "    partialSSE_w_Wwt01 = -transFuncDerivHidden1*inputNode0*(vWt10*errorTimesTransDOutput0 + vWt11*errorTimesTransDOutput1)\n",
    "    partialSSE_w_Wwt10 = -transFuncDerivHidden0*inputNode1*(vWt00*errorTimesTransDOutput0 + vWt01*errorTimesTransDOutput1)\n",
    "    partialSSE_w_Wwt11 = -transFuncDerivHidden1*inputNode1*(vWt10*errorTimesTransDOutput0 + vWt11*errorTimesTransDOutput1)                                                                                                    \n",
    "                                                                                                                                                                                                                                                \n",
    "    deltaWWt00 = -eta*partialSSE_w_Wwt00\n",
    "    deltaWWt01 = -eta*partialSSE_w_Wwt01        \n",
    "    deltaWWt10 = -eta*partialSSE_w_Wwt10\n",
    "    deltaWWt11 = -eta*partialSSE_w_Wwt11 \n",
    "    deltaWWtArray = np.array([[deltaWWt00, deltaWWt10],[deltaWWt01, deltaWWt11]])\n",
    "\n",
    "\n",
    "\n",
    "    wWt00 = wWt00+deltaWWt00\n",
    "    wWt01 = wWt01+deltaWWt01\n",
    "    wWt10 = wWt10+deltaWWt10\n",
    "    wWt11 = wWt11+deltaWWt11 \n",
    "    \n",
    "    newWWeightArray = np.array([[wWt00, wWt10], [wWt01, wWt11]])\n",
    "\n",
    "    print()\n",
    "    print('Calling the PrintAndTraceBackpropagateHiddenToInput() function')\n",
    "\n",
    "    PrintAndTraceBackpropagateHiddenToInput (alpha, eta, inputDataList, errorList, actualAllNodesOutputList, \n",
    "        transFuncDerivHiddenList, transFuncDerivOutputList, deltaWWtArray, vWeightArray, wWeightArray, newWWeightArray, biasWeightArray)     \n",
    "\n",
    "    print()\n",
    "    print('Ending the BackpropagateHiddenToInput() function')                                                                    \n",
    "    return (newWWeightArray); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateBiasHiddenWeights (alpha, eta, errorList, actualAllNodesOutputList, vWeightArray, \n",
    "biasHiddenWeightArray, biasOutputWeightArray):\n",
    "    print()\n",
    "    print('Starting the BackpropagateBiasHiddenWeights() function')\n",
    "    print('This backpropagates weight changes onto the bias-to-hidden connection weights')\n",
    "\n",
    "# The first step here applies a backpropagation-based weight change to the hidden-to-output wts v. \n",
    "# Core equation for the first part of backpropagation: \n",
    "# d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n",
    "# where:\n",
    "# -- SSE = sum of squared errors, and only the error associated with a given output node counts\n",
    "# -- v(h,o) is the connection weight v between the hidden node h and the output node o\n",
    "# -- alpha is the scaling term within the transfer function, often set to 1\n",
    "# ---- (this is included in transfFuncDeriv) \n",
    "# -- Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n",
    "# -- F = transfer function, here using the sigmoid transfer function\n",
    "# -- Hidden(h) = the output of hidden node h. \n",
    "\n",
    "# Note that the training rate parameter is assigned in main; Greek letter \"eta,\" looks like n, \n",
    "#   scales amount of change to connection weight\n",
    "\n",
    "# We will DECREMENT the connection weight biasOutput by a small amount proportional to the derivative eqn\n",
    "#   of the SSE w/r/t the weight biasOutput(o). \n",
    "# This means, since there is a minus sign in that derivative, that we will add a small amount. \n",
    "# (Decrementing is -, applied to a (-), which yields a positive.)\n",
    "\n",
    "# For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand), \n",
    "#   please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X. \n",
    "#   (Meaning: exact chapter is still TBD.) \n",
    "# For the latest updates, etc., please visit: www.aliannajmaren.com\n",
    "\n",
    "# Unpack the errorList and vWeightArray\n",
    "    error0 = errorList[0]\n",
    "    error1 = errorList[1]\n",
    "    \n",
    "    vWt00 = vWeightArray[0,0]\n",
    "    vWt01 = vWeightArray[1,0]\n",
    "    vWt10 = vWeightArray[0,1]       \n",
    "    vWt11 = vWeightArray[1,1]      \n",
    "\n",
    "# Unpack the biasWeightArray, we will only be modifying the biasOutput terms, but need to have\n",
    "#   all the bias weights for when we redefine the biasWeightArray\n",
    "    biasHiddenWt0 = biasHiddenWeightArray[0]\n",
    "    biasHiddenWt1 = biasHiddenWeightArray[1]    \n",
    "    biasOutputWt0 = biasOutputWeightArray[0]\n",
    "    biasOutputWt1 = biasOutputWeightArray[1]\n",
    "    \n",
    "# Unpack the outputNodes  \n",
    "    hiddenNode0= actualAllNodesOutputList[0]\n",
    "    hiddenNode1= actualAllNodesOutputList[1]\n",
    "    outputNode0 = actualAllNodesOutputList[2]    \n",
    "    outputNode1 = actualAllNodesOutputList[3]  \n",
    "    \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence   \n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode0')           \n",
    "    transFuncDerivOutput0 = computeTransferFnctnDeriv(outputNode0, alpha)\n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for outputNode1')  \n",
    "    transFuncDerivOutput1 = computeTransferFnctnDeriv(outputNode1, alpha)\n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for hiddenNode0') \n",
    "    transFuncDerivHidden0 = computeTransferFnctnDeriv(hiddenNode0, alpha) \n",
    "    print()\n",
    "    print('Calling the computeTransferFnctnDeriv() function for hiddenNode1')\n",
    "    transFuncDerivHidden1 = computeTransferFnctnDeriv(hiddenNode1, alpha)    \n",
    "\n",
    "# This list will be used only if we call a print-and-trace debug function. \n",
    "    transFuncDerivOutputList = (transFuncDerivOutput0, transFuncDerivOutput1) \n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "\n",
    "# ===>>> AJM needs to double-check these equations in the comments area\n",
    "# ===>>> The code should be fine. \n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    errorTimesTransDOutput0 = error0*transFuncDerivOutput0\n",
    "    errorTimesTransDOutput1 = error1*transFuncDerivOutput1\n",
    "    \n",
    "    partialSSE_w_BiasHidden0 = -transFuncDerivHidden0*(errorTimesTransDOutput0*vWt00 + \n",
    "    errorTimesTransDOutput1*vWt01)\n",
    "    partialSSE_w_BiasHidden1 = -transFuncDerivHidden1*(errorTimesTransDOutput0*vWt10 + \n",
    "    errorTimesTransDOutput1*vWt11)  \n",
    "                                                                                                                                                                                                                                                                \n",
    "    deltaBiasHidden0 = -eta*partialSSE_w_BiasHidden0\n",
    "    deltaBiasHidden1 = -eta*partialSSE_w_BiasHidden1\n",
    "\n",
    "    biasHiddenWt0 = biasHiddenWt0+deltaBiasHidden0\n",
    "    biasHiddenWt1 = biasHiddenWt1+deltaBiasHidden1 \n",
    "\n",
    "# Note that only the bias weights for the hidden nodes have been changed.         \n",
    "    newBiasHiddenWeightArray = np.array([biasHiddenWt0, biasHiddenWt1])\n",
    "\n",
    "#    A new procedure to debug trace this function has not been written yet. \n",
    "#    PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newVWeightArray)\n",
    "\n",
    "    print()\n",
    "    print('The newBiasHiddenWeightArray = ', newBiasHiddenWeightArray) \n",
    "\n",
    "    print()\n",
    "    print('Ending the BackpropagateBiasHiddenWeights() function')   \n",
    "                                                                                                                                            \n",
    "    return (newBiasHiddenWeightArray);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling the main() procedure from the program\n",
      "Starting main() function\n",
      "\n",
      "Calling welcome() function\n",
      "\n",
      "starting the welcome() function\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "  trained using the backpropagation method.\n",
      "Version 1.0, 03/25/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "ending the welcome() function and returning to main\n",
      "\n",
      "setting alpha, summedInput, maxNumIterations, and eta\n",
      "\n",
      "alpha =  1.0 summedInput =  1\n",
      "maxNumIterations =  2 eta =  0.5\n",
      "Setting the debug_xxx_off statements to be false, so will print extra\n",
      "\n",
      "calling the obtainNeuralNetworkSizeSpecs() function\n",
      "Starting the obtainNeuralNetworkSizeSpecs() function\n",
      "\n",
      "This network is set up to run the X-OR problem.\n",
      "The numbers of nodes in the input, hidden, and output layers have been set to 2 each.\n",
      "The number of input nodes is  2\n",
      "The number of hidden nodes is  2\n",
      "The number of output nodes is  2\n",
      "Ending the obtainNeuralNetworkSizeSpecs() function\n",
      "\n",
      "Flow-of-control trace: Back in main\n",
      "I = number of nodes in input layer is 2\n",
      "H = number of nodes in hidden layer is 2\n",
      "O = number of nodes in output layer is 2\n",
      "\n",
      "Initialize the trainingDataList to (0,0,0,0,0\n",
      "\n",
      "Calling initializeWeightArray for input-to-hidden weights\n",
      "starting the initializeWeightArray() function\n",
      "initialize wt00 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.21743456638411818\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt01 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.4959239453596678\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt02 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.8992641008342508\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt03 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.06190016545930832\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "  Inside initializeWeightArray\n",
      "\n",
      "    The weights just initialized are: \n",
      "      weight00 = -0.2174,\n",
      "      weight01 = -0.4959,\n",
      "      weight10 = -0.8993,\n",
      "      weight11 = 0.0619,\n",
      "\n",
      "    The weightArray just established is:  [[-0.21743457 -0.8992641 ]\n",
      " [-0.49592395  0.06190017]]\n",
      "\n",
      "    Within this array: \n",
      "       weight00 = -0.2174    weight10 = -0.8993\n",
      "       weight01 = -0.4959    weight11 = 0.0619\n",
      "  Returning to calling procedure\n",
      "\n",
      "ending the initializeWeightArray() function and returning to main\n",
      "\n",
      "\n",
      "Calling initializeWeightArray for Hidden-to-Output weights\n",
      "starting the initializeWeightArray() function\n",
      "initialize wt00 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.7525350878360544\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt01 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.4612654724957872\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt02 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.7796483469864266\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize wt03 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.6225840698205876\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "  Inside initializeWeightArray\n",
      "\n",
      "    The weights just initialized are: \n",
      "      weight00 = -0.7525,\n",
      "      weight01 = -0.4613,\n",
      "      weight10 = -0.7796,\n",
      "      weight11 = 0.6226,\n",
      "\n",
      "    The weightArray just established is:  [[-0.75253509 -0.77964835]\n",
      " [-0.46126547  0.62258407]]\n",
      "\n",
      "    Within this array: \n",
      "       weight00 = -0.7525    weight10 = -0.7796\n",
      "       weight01 = -0.4613    weight11 = 0.6226\n",
      "  Returning to calling procedure\n",
      "\n",
      "ending the initializeWeightArray() function and returning to main\n",
      "\n",
      "\n",
      "calling initializeBiasWeightArray() function for Hidden nodes\n",
      "\n",
      "starting initializeBiasWeightArray() function\n",
      "initialize biasWeight0 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.5810902352190823\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize biasWeight1 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  0.4896599445678782\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "biasWeight0 =  -0.5810902352190823\n",
      "biasWeight1 =  0.4896599445678782\n",
      "The biasWeightArray =  [-0.58109024  0.48965994]\n",
      "ending initializeBiasWeightArray() function\n",
      "returning to main\n",
      "\n",
      "\n",
      "calling initializeBiasWeightArray() function for Output nodes\n",
      "\n",
      "starting initializeBiasWeightArray() function\n",
      "initialize biasWeight0 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.6420504459218677\n",
      "ending the InitializWeight() function\n",
      "\n",
      "initialize biasWeight1 by calling initializeWeight() function\n",
      "starting the InitializeWeight() function\n",
      "weight =  -0.01928120724884952\n",
      "ending the InitializWeight() function\n",
      "\n",
      "\n",
      "biasWeight0 =  -0.6420504459218677\n",
      "biasWeight1 =  -0.01928120724884952\n",
      "The biasWeightArray =  [-0.64205045 -0.01928121]\n",
      "ending initializeBiasWeightArray() function\n",
      "returning to main\n",
      "\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "       Input-to-Hidden                            Hidden-to-Output\n",
      "  w(0,0) = -0.2174   w(1,0) = -0.8993         v(0,0) = -0.7525   v(1,0) = -0.7796\n",
      "  w(0,1) = -0.4959   w(1,1) = 0.0619         v(0,1) = -0.4613   v(1,1) = 0.6226\n",
      "\n",
      "       Bias at Hidden Layer                          Bias at Output Layer\n",
      "       b(hidden,0) = -0.5811                           b(output,0) = -0.6421\n",
      "       b(hidden,1) = 0.4897                           b(output,1) = -0.0193\n",
      "\n",
      "Establishing parameters - epsilon, iteration counter, SSE_InitialTotal\n",
      "\n",
      "Initialize SSE_InitialArray to zeros\n",
      "calling the computeSSE_Values() function to initialize the SSE_InitialArray\n",
      "starting the computeSSE_Values() function\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for zeroth data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.3586817691564017\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is -0.5811\n",
      "    The activation (applied transfer function) for that neuron is 0.3587\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.6200263207022142\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is 0.4897\n",
      "    The activation (applied transfer function) for that neuron is 0.6200\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.3587 Hidden1 = 0.6200\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.19855128430718372\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is -1.3954\n",
      "    The activation (applied transfer function) for that neuron is 0.1986\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5501532204485046\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is 0.2013\n",
      "    The activation (applied transfer function) for that neuron is 0.5502\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.1986 Output1 = 0.5502\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.3586817691564017\n",
      "the hiddenActivation1 =  0.6200263207022142\n",
      "the outputActivation0 =  0.19855128430718372\n",
      "the outputActivation1 =  0.5501532204485046\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  In computeSSE_Values\n",
      "\n",
      "  Actual Node Outputs for (0,0) training set:\n",
      "      input0 =  0    input1 =  0\n",
      "      actualOutput0 = 0.1986   actualOutput1 = 0.5502\n",
      "      error0 =        -0.1986   error1 =        0.4498\n",
      "   Initial SSE for (0,0) = 0.2418\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for zeroth data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for first data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.1853739048912434\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is -1.4804\n",
      "    The activation (applied transfer function) for that neuron is 0.1854\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.6344974726583524\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  1\n",
      "    The summed neuron input is 0.5516\n",
      "    The activation (applied transfer function) for that neuron is 0.6345\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.1854 Hidden1 = 0.6345\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.21819166864714318\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1853739048912434 ,  0.6344974726583524\n",
      "    The summed neuron input is -1.2762\n",
      "    The activation (applied transfer function) for that neuron is 0.2182\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5720549441322538\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1853739048912434 ,  0.6344974726583524\n",
      "    The summed neuron input is 0.2902\n",
      "    The activation (applied transfer function) for that neuron is 0.5721\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.2182 Output1 = 0.5721\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.1853739048912434\n",
      "the hiddenActivation1 =  0.6344974726583524\n",
      "the outputActivation0 =  0.21819166864714318\n",
      "the outputActivation1 =  0.5720549441322538\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (0,1) training set:\n",
      "     input0 =  0    input1 =  1\n",
      "     actualOutput0 = 0.2182   actualOutput1 = 0.5721\n",
      "     error0 =        0.7818   error1 =        -0.5721\n",
      "  Initial SSE for (0,1) = 0.9385\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for first data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for second data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.3103411665167793\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is -0.7985\n",
      "    The activation (applied transfer function) for that neuron is 0.3103\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.4984340049225538\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  0\n",
      "    The summed neuron input is -0.0063\n",
      "    The activation (applied transfer function) for that neuron is 0.4984\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.3103 Hidden1 = 0.4984\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.22025236653844157\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3103411665167793 ,  0.4984340049225538\n",
      "    The summed neuron input is -1.2642\n",
      "    The activation (applied transfer function) for that neuron is 0.2203\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5369043151967392\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3103411665167793 ,  0.4984340049225538\n",
      "    The summed neuron input is 0.1479\n",
      "    The activation (applied transfer function) for that neuron is 0.5369\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.2203 Output1 = 0.5369\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.3103411665167793\n",
      "the hiddenActivation1 =  0.4984340049225538\n",
      "the outputActivation0 =  0.22025236653844157\n",
      "the outputActivation1 =  0.5369043151967392\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (1,0) training set:\n",
      "     input0 =  1    input1 =  0\n",
      "     actualOutput0 = 0.2203   actualOutput1 = 0.5369\n",
      "     error0 =        0.7797   error1 =        -0.5369\n",
      "  Initial SSE for (1,0) = 0.8963\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for second data set\n",
      "\n",
      "Compute a single feed-forward pass and obtain the Actual Outputs for third data set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.1547542678158326\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is -1.6978\n",
      "    The activation (applied transfer function) for that neuron is 0.1548\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5139054544598441\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is 0.0556\n",
      "    The activation (applied transfer function) for that neuron is 0.5139\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.1548 Hidden1 = 0.5139\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.23881740596401638\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1547542678158326 ,  0.5139054544598441\n",
      "    The summed neuron input is -1.1592\n",
      "    The activation (applied transfer function) for that neuron is 0.2388\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5570715249606158\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1547542678158326 ,  0.5139054544598441\n",
      "    The summed neuron input is 0.2293\n",
      "    The activation (applied transfer function) for that neuron is 0.5571\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.2388 Output1 = 0.5571\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.1547542678158326\n",
      "the hiddenActivation1 =  0.5139054544598441\n",
      "the outputActivation0 =  0.23881740596401638\n",
      "the outputActivation1 =  0.5570715249606158\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "  Actual Node Outputs for (1,1) training set:\n",
      "     input0 =  1    input1 =  1\n",
      "     actualOutput0 = 0.2388   actualOutput1 = 0.5571\n",
      "     error0 =        -0.2388   error1 =        0.4429\n",
      "  Initial SSE for (1,1) = 0.2532\n",
      "\n",
      "Ending a single feed-forward pass to obtain the Actual Outputs for third data set\n",
      "\n",
      "Initializing array of SSE values\n",
      "the SSE_InitialArray[0] =  0.24178473757288382 the SSE_InitialArray[1] =  0.9384711260788943\n",
      "the SSE_InitialArray[2] =  0.8962726155657803 the SSE_InitialArray[3] =  0.25321938739209615\n",
      "\n",
      "  The initial total of the SSEs is 2.3297\n",
      "\n",
      "ending the computeSSE_Values() function and returning to main\n",
      "\n",
      "The SSE_Array =  [0.24178473757288382, 0.9384711260788943, 0.8962726155657803, 0.25321938739209615, 2.329747866609655]\n",
      "The SSE_InitialTotal =  2.329747866609655\n",
      "\n",
      "In main, SSE computations completed, Total of all SSEs = 2.3297\n",
      "  For input nodes (0,0), SSE_Array[0] = 0.2418\n",
      "  For input nodes (0,1), SSE_Array[1] = 0.9385\n",
      "  For input nodes (1,0), SSE_Array[2] = 0.8963\n",
      "  For input nodes (1,1), SSE_Array[3] = 0.2532\n",
      "\n",
      "\n",
      "About to enter the while loop for  2  iterations\n",
      "\n",
      "\n",
      "starting at top of while loop for first time\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (1, 1, 0, 1, 3)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  0\n",
      "\n",
      "Randomly selected training data set number  3\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  1             Input1 =  1\n",
      " Desired Output0 =  0    Desired Output1 =  1\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.1547542678158326\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is -1.6978\n",
      "    The activation (applied transfer function) for that neuron is 0.1548\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5139054544598441\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is 0.0556\n",
      "    The activation (applied transfer function) for that neuron is 0.5139\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.1548 Hidden1 = 0.5139\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.23881740596401638\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1547542678158326 ,  0.5139054544598441\n",
      "    The summed neuron input is -1.1592\n",
      "    The activation (applied transfer function) for that neuron is 0.2388\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5570715249606158\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.1547542678158326 ,  0.5139054544598441\n",
      "    The summed neuron input is 0.2293\n",
      "    The activation (applied transfer function) for that neuron is 0.5571\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.2388 Output1 = 0.5571\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.1547542678158326\n",
      "the hiddenActivation1 =  0.5139054544598441\n",
      "the outputActivation0 =  0.23881740596401638\n",
      "the outputActivation1 =  0.5570715249606158\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 1 1\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.1548\n",
      "    actualHiddenOutput1 = 0.5139\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.2388\n",
      "    actualOutput1 = 0.5571'\n",
      "  Initial SSE (before backpropagation) = 0.253219\n",
      "  Corresponding SSE (from initial SSE determination) = 0.253219\n",
      "\n",
      "Calling BackpropagateOutputToHidden() function\n",
      "\n",
      "Starting the BackpropagateOutputToHidden() function\n",
      "Performs the backpropagation of weight changes onto hidden-to-output weigts\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "Calling the PrintAndTraceBackpropagateOutputToHidden() function\n",
      "\n",
      "Starting PrintAndTraceBackpropagateOutputToHidden() function\n",
      " \n",
      "In Print and Trace for Backpropagation: Hidden to Output Weights\n",
      "  Assuming alpha = 1\n",
      " \n",
      "  The hidden node activations are:\n",
      "    Hidden node 0:    0.1548   Hidden node 1:    0.5139\n",
      " \n",
      "  The output node activations are:\n",
      "    Output node 0:    0.239    Output node 1:    0.557\n",
      " \n",
      "  The transfer function derivatives are: \n",
      "    Deriv-F(0):       0.182    Deriv-F(1):       0.247\n",
      " \n",
      "The computed values for the deltas are: \n",
      "                eta  *  error  *   trFncDeriv *   hidden\n",
      "  deltaVWt00 =   0.50 * -0.2388  * 0.1818   * 0.1548\n",
      "  deltaVWt01 =   0.50 * 0.4429  * 0.2467   * 0.1548\n",
      "  deltaVWt10 =   0.50 * -0.2388  * 0.1818   * 0.5139\n",
      "  deltaVWt11 =   0.50 * 0.4429  * 0.2467   * 0.5139\n",
      " \n",
      "Values for the hidden-to-output connection weights:\n",
      "           Old:     New:      eta*Delta:\n",
      "[0,0]:   -0.7525   -0.7559   -0.0034\n",
      "[0,1]:   -0.4613   -0.4528   0.0085\n",
      "[1,0]:   -0.7796   -0.7908   -0.0112\n",
      "[1,1]:   0.6226   0.6507   0.0281\n",
      "\n",
      "Ending PrintAndTraceBackpropagateOutputToHidden() function\n",
      "\n",
      "Back in the BackpropagateOutputToHidden() function\n",
      "\n",
      "Ending the BackpropagateOutputToHidden() function\n",
      "\n",
      "Calling BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Starting the BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "The newBiasOutputWeightArray =  [-0.663757    0.03536351]\n",
      "\n",
      "Ending the BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Calling BackpropagateHiddenToInput() function\n",
      "\n",
      "Starting the BackpropagateHiddenToInput() function\n",
      "This Backpropagates the weight changes onto the input-to-hidden weights\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the PrintAndTraceBackpropagateHiddenToInput() function\n",
      "\n",
      "Starting PrintAndTraceBackpropagateHiddenToInput() function\n",
      "Traces the backpropagation of the input-to-hidden weights\n",
      " \n",
      "In Print and Trace for Backpropagation: Input to Hidden Weights\n",
      "  Assuming alpha = 1\n",
      " \n",
      "  The hidden node activations are:\n",
      "    Hidden node 0:    0.1548   Hidden node 1:    0.5139\n",
      " \n",
      "  The output node activations are:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Output node 0:    0.239    Output node 1:    0.557\n",
      " \n",
      "  The transfer function derivatives at the hidden nodes are: \n",
      "    Deriv-F(0):       0.131    Deriv-F(1):       0.250\n",
      " \n",
      "The computed values for the deltas are: \n",
      "                eta  *  error  *   trFncDeriv *   input    * SumTerm for given H\n",
      "  deltaWWt00 =   0.50 * -0.2388  * 0.1308   * 1.0000   * -0.0246\n",
      "  deltaWWt01 =   0.50 * 0.4429  * 0.2498   * 1.0000   * 0.4620\n",
      "  deltaWWt10 =   0.50 * -0.2388  * 0.1308   * 1.0000   * -0.0246\n",
      "  deltaWWt11 =   0.50 * 0.4429  * 0.2498   * 1.0000   * 0.4620\n",
      " \n",
      "Values for the input-to-hidden connection weights:\n",
      "           Old:     New:      eta*Delta:\n",
      "[0,0]:   -0.2174   -0.2186   -0.0012\n",
      "[0,1]:   -0.4959   -0.4832   0.0127\n",
      "[1,0]:   -0.8993   -0.9004   -0.0012\n",
      "[1,1]:   0.0619   0.0746   0.0127\n",
      "\n",
      "Ending the PrintAndTraceBackpropagateHiddenToInput() function\n",
      "\n",
      "Ending the BackpropagateHiddenToInput() function\n",
      "\n",
      "Calling BackpropagateBiasHiddenWeights() function\n",
      "\n",
      "Starting the BackpropagateBiasHiddenWeights() function\n",
      "This backpropagates weight changes onto the bias-to-hidden connection weights\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "The newBiasHiddenWeightArray =  [-0.58225058  0.5023862 ]\n",
      "\n",
      "Ending the BackpropagateBiasHiddenWeights() function\n",
      "\n",
      "The newBiasWeightArray =  [[-0.6637569960988979, 0.03536350790522207], [-0.5822505809691644, 0.5023861951527234]]\n",
      " \n",
      "    The weights before backpropagation are:\n",
      "         Input-to-Hidden                           Hidden-to-Output\n",
      "    w(0,0) = -0.217   w(1,0) = -0.899         v(0,0) = -0.753   v(1,0) = -0.780\n",
      "    w(0,1) = -0.496   w(1,1) = 0.062         v(0,1) = -0.461   v(1,1) = 0.623\n",
      " \n",
      "    The weights after backpropagation are:\n",
      "         Input-to-Hidden                           Hidden-to-Output\n",
      "    w(0,0) = -0.219   w(1,0) = -0.900         v(0,0) = -0.756   v(1,0) = -0.791\n",
      "    w(0,1) = -0.483   w(1,1) = 0.075         v(0,1) = -0.453   v(1,1) = 0.651\n",
      "\n",
      "Calling the ComputeSingleFeedforwardPass() function\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  1\n",
      "Input1 =  1\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.219   w(0,1) = -0.900         v(0,0) = -0.756   v(0,1) = -0.791\n",
      "w(1,0) = -0.483   w(1,1) = 0.075         v(1,0) = -0.453   v(1,1) = 0.651\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  1 ,  1\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.15445095202747794\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is -1.7001\n",
      "    The activation (applied transfer function) for that neuron is 0.1545\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  1 ,  1\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5202610656725762\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  1 ,  1\n",
      "    The summed neuron input is 0.0811\n",
      "    The activation (applied transfer function) for that neuron is 0.5203\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  1 ,  1\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.1545 Hidden1 = 0.5203\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.2368146264613002\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.15445095202747794 ,  0.5202610656725762\n",
      "    The summed neuron input is -1.1702\n",
      "    The activation (applied transfer function) for that neuron is 0.2368\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.562003793915188\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.15445095202747794 ,  0.5202610656725762\n",
      "    The summed neuron input is 0.2493\n",
      "    The activation (applied transfer function) for that neuron is 0.5620\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.2368 Output1 = 0.5620\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.15445095202747794\n",
      "the hiddenActivation1 =  0.5202610656725762\n",
      "the outputActivation0 =  0.2368146264613002\n",
      "the outputActivation1 =  0.562003793915188\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      " \n",
      "In main; have just completed a single step of backpropagation with inputs 1 1\n",
      "    The new SSE (after backpropagation) = 0.247922\n",
      "    Error(0) = -0.2368,   Error(1) = 0.4380\n",
      "    SSE(0) =   0.0561,   SSE(1) =   0.1918\n",
      "  The difference in initial and the resulting SSEs is: 0.0053\n",
      " \n",
      "   The training has resulted in improving the total SSEs\n",
      " \n",
      "  The previous SSE Total was 2.3297\n",
      "  The new SSE Total was 2.3245\n",
      "    For node 0: Desired Output =  0  New Output = 0.2368\n",
      "    For node 1: Desired Output =  1  New Output = 0.5620\n",
      "    Error(0) = -0.2368,   Error(1) = 0.4380\n",
      "    SSE0(0) =   0.0561,   SSE(1) =   0.1918\n",
      "  Delta in the SSEs is 0.0053\n",
      "SSE improvement\n",
      " \n",
      "Iteration number  0\n",
      "\n",
      "starting at the top of the while loop again\n",
      "\n",
      "create random trainingDataList by calling obtainRandomXORTrainingValues() function\n",
      "starting obtainRandomXORTrainingValues() function\n",
      "The trainingDataList =  (0, 0, 0, 1, 0)\n",
      "ending obtainRandomXORTrainingValues() function and returning to main\n",
      "\n",
      "Iteration number  1\n",
      "\n",
      "Randomly selected training data set number  0\n",
      "The inputs and desired outputs for the X-OR problem from this data set are:\n",
      "          Input0 =  0             Input1 =  0\n",
      " Desired Output0 =  0    Desired Output1 =  1\n",
      "\n",
      "calling ComputeSingleFeedforwardPass to obtain actual outputs for training set\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.219   w(0,1) = -0.900         v(0,0) = -0.756   v(0,1) = -0.791\n",
      "w(1,0) = -0.483   w(1,1) = 0.075         v(1,0) = -0.453   v(1,1) = 0.651\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.3586817691564017\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is -0.5811\n",
      "    The activation (applied transfer function) for that neuron is 0.3587\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.6200263207022142\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is 0.4897\n",
      "    The activation (applied transfer function) for that neuron is 0.6200\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.3587 Hidden1 = 0.6200\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.19726210919135376\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is -1.4035\n",
      "    The activation (applied transfer function) for that neuron is 0.1973\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5552076624718325\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is 0.2217\n",
      "    The activation (applied transfer function) for that neuron is 0.5552\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.1973 Output1 = 0.5552\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.3586817691564017\n",
      "the hiddenActivation1 =  0.6200263207022142\n",
      "the outputActivation0 =  0.19726210919135376\n",
      "the outputActivation1 =  0.5552076624718325\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      "\n",
      "In main; have just completed a feedfoward pass with training set inputs 0 0\n",
      "  The activations (actual outputs) for the two hidden neurons are:\n",
      "    actualHiddenOutput0 = 0.3587\n",
      "    actualHiddenOutput1 = 0.6200\n",
      "  The activations (actual outputs) for the two output neurons are:\n",
      "    actualOutput0 = 0.1973\n",
      "    actualOutput1 = 0.5552'\n",
      "  Initial SSE (before backpropagation) = 0.236753\n",
      "  Corresponding SSE (from initial SSE determination) = 0.241785\n",
      "\n",
      "Calling BackpropagateOutputToHidden() function\n",
      "\n",
      "Starting the BackpropagateOutputToHidden() function\n",
      "Performs the backpropagation of weight changes onto hidden-to-output weigts\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "Calling the PrintAndTraceBackpropagateOutputToHidden() function\n",
      "\n",
      "Starting PrintAndTraceBackpropagateOutputToHidden() function\n",
      " \n",
      "In Print and Trace for Backpropagation: Hidden to Output Weights\n",
      "  Assuming alpha = 1\n",
      " \n",
      "  The hidden node activations are:\n",
      "    Hidden node 0:    0.3587   Hidden node 1:    0.6200\n",
      " \n",
      "  The output node activations are:\n",
      "    Output node 0:    0.197    Output node 1:    0.555\n",
      " \n",
      "  The transfer function derivatives are: \n",
      "    Deriv-F(0):       0.158    Deriv-F(1):       0.247\n",
      " \n",
      "The computed values for the deltas are: \n",
      "                eta  *  error  *   trFncDeriv *   hidden\n",
      "  deltaVWt00 =   0.50 * -0.1973  * 0.1583   * 0.3587\n",
      "  deltaVWt01 =   0.50 * 0.4448  * 0.2470   * 0.3587\n",
      "  deltaVWt10 =   0.50 * -0.1973  * 0.1583   * 0.6200\n",
      "  deltaVWt11 =   0.50 * 0.4448  * 0.2470   * 0.6200\n",
      " \n",
      "Values for the hidden-to-output connection weights:\n",
      "           Old:     New:      eta*Delta:\n",
      "[0,0]:   -0.7559   -0.7615   -0.0056\n",
      "[0,1]:   -0.4528   -0.4331   0.0197\n",
      "[1,0]:   -0.7908   -0.8005   -0.0097\n",
      "[1,1]:   0.6507   0.6847   0.0341\n",
      "\n",
      "Ending PrintAndTraceBackpropagateOutputToHidden() function\n",
      "\n",
      "Back in the BackpropagateOutputToHidden() function\n",
      "\n",
      "Ending the BackpropagateOutputToHidden() function\n",
      "\n",
      "Calling BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Starting the BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "The newBiasOutputWeightArray =  [-0.65766865  0.03564   ]\n",
      "\n",
      "Ending the BackpropagateBiasOutputWeights() function\n",
      "\n",
      "Calling BackpropagateHiddenToInput() function\n",
      "\n",
      "Starting the BackpropagateHiddenToInput() function\n",
      "This Backpropagates the weight changes onto the input-to-hidden weights\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the PrintAndTraceBackpropagateHiddenToInput() function\n",
      "\n",
      "Starting PrintAndTraceBackpropagateHiddenToInput() function\n",
      "Traces the backpropagation of the input-to-hidden weights\n",
      " \n",
      "In Print and Trace for Backpropagation: Input to Hidden Weights\n",
      "  Assuming alpha = 1\n",
      " \n",
      "  The hidden node activations are:\n",
      "    Hidden node 0:    0.3587   Hidden node 1:    0.6200\n",
      " \n",
      "  The output node activations are:\n",
      "    Output node 0:    0.197    Output node 1:    0.555\n",
      " \n",
      "  The transfer function derivatives at the hidden nodes are: \n",
      "    Deriv-F(0):       0.230    Deriv-F(1):       0.236\n",
      " \n",
      "The computed values for the deltas are: \n",
      "                eta  *  error  *   trFncDeriv *   input    * SumTerm for given H\n",
      "  deltaWWt00 =   0.50 * -0.1973  * 0.2300   * 0.0000   * -0.0523\n",
      "  deltaWWt01 =   0.50 * 0.4448  * 0.2356   * 0.0000   * 0.4454\n",
      "  deltaWWt10 =   0.50 * -0.1973  * 0.2300   * 0.0000   * -0.0523\n",
      "  deltaWWt11 =   0.50 * 0.4448  * 0.2356   * 0.0000   * 0.4454\n",
      " \n",
      "Values for the input-to-hidden connection weights:\n",
      "           Old:     New:      eta*Delta:\n",
      "[0,0]:   -0.2186   -0.2186   -0.0000\n",
      "[0,1]:   -0.4832   -0.4832   0.0000\n",
      "[1,0]:   -0.9004   -0.9004   -0.0000\n",
      "[1,1]:   0.0746   0.0746   0.0000\n",
      "\n",
      "Ending the PrintAndTraceBackpropagateHiddenToInput() function\n",
      "\n",
      "Ending the BackpropagateHiddenToInput() function\n",
      "\n",
      "Calling BackpropagateBiasHiddenWeights() function\n",
      "\n",
      "Starting the BackpropagateBiasHiddenWeights() function\n",
      "This backpropagates weight changes onto the bias-to-hidden connection weights\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for outputNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode0\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "Calling the computeTransferFnctnDeriv() function for hiddenNode1\n",
      "\n",
      "Starting the computeTransferFnctnDeriv() function\n",
      "This computes the derivate of the transfer function\n",
      "Ending the computeTransferFnctnDeriv() function\n",
      "\n",
      "The newBiasHiddenWeightArray =  [-0.58409513  0.50098877]\n",
      "\n",
      "Ending the BackpropagateBiasHiddenWeights() function\n",
      "\n",
      "The newBiasWeightArray =  [[-0.657668650679551, 0.03563999677391943], [-0.5840951296448392, 0.5009887745353752]]\n",
      " \n",
      "    The weights before backpropagation are:\n",
      "         Input-to-Hidden                           Hidden-to-Output\n",
      "    w(0,0) = -0.219   w(1,0) = -0.900         v(0,0) = -0.756   v(1,0) = -0.791\n",
      "    w(0,1) = -0.483   w(1,1) = 0.075         v(0,1) = -0.453   v(1,1) = 0.651\n",
      " \n",
      "    The weights after backpropagation are:\n",
      "         Input-to-Hidden                           Hidden-to-Output\n",
      "    w(0,0) = -0.219   w(1,0) = -0.900         v(0,0) = -0.761   v(1,0) = -0.800\n",
      "    w(0,1) = -0.483   w(1,1) = 0.075         v(0,1) = -0.433   v(1,1) = 0.685\n",
      "\n",
      "Calling the ComputeSingleFeedforwardPass() function\n",
      "\n",
      "Starting ComputeSingleFeedforwardPass() function\n",
      "The inputs transferred in are: \n",
      "Input0 =  0\n",
      "Input1 =  0\n",
      "\n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden             Hidden-to-Output\n",
      "w(0,0) = -0.219   w(0,1) = -0.900         v(0,0) = -0.761   v(0,1) = -0.800\n",
      "w(1,0) = -0.483   w(1,1) = 0.075         v(1,0) = -0.433   v(1,1) = 0.685\n",
      "\n",
      "  For hiddenActivation0 from input0, input1 =  0 ,  0\n",
      "\n",
      "computing hiddenActivation0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.3586817691564017\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is -0.5811\n",
      "    The activation (applied transfer function) for that neuron is 0.3587\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  For hiddenActivation1 from input0, input1 =  0 ,  0\n",
      "computing hiddenActivation1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.6200263207022142\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0 ,  0\n",
      "    The summed neuron input is 0.4897\n",
      "    The activation (applied transfer function) for that neuron is 0.6200\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  In computeSingleFeedforwardPass: \n",
      "  Input node values:  0 ,  0\n",
      "  The activations for the hidden nodes are:\n",
      "    Hidden0 = 0.3587 Hidden1 = 0.6200\n",
      "\n",
      "computing the activation of output node0 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.1959962556971612\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is -1.4115\n",
      "    The activation (applied transfer function) for that neuron is 0.1960\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "computing the activation of output node1 by calling computeSingleNeuronActivation\n",
      "\n",
      "starting computeSingleNeuronActivation() function\n",
      "compute the activation by calling computeTransferFnctn()\n",
      "\n",
      "Starting the computeTransferFnction() function\n",
      "The activation is  0.5621553219344545\n",
      "Ending the computeTransferFnction() function and returning the activation\n",
      "\n",
      "\n",
      "  In computeSingleNeuronActivation with input0, input 1 given as:  0.3586817691564017 ,  0.6200263207022142\n",
      "    The summed neuron input is 0.2499\n",
      "    The activation (applied transfer function) for that neuron is 0.5622\n",
      "ending computeSingleNeuronActivation() function and returning activation\n",
      "\n",
      "\n",
      "  Computing the output neuron activations\n",
      "\n",
      "  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)\n",
      "  The activations for the output nodes are:\n",
      "    Output0 = 0.1960 Output1 = 0.5622\n",
      "\n",
      "creating the actualAllNodesOutputList\n",
      "the hiddenActivation0 =  0.3586817691564017\n",
      "the hiddenActivation1 =  0.6200263207022142\n",
      "the outputActivation0 =  0.1959962556971612\n",
      "the outputActivation1 =  0.5621553219344545\n",
      "\n",
      "ending ComputeSingleFeedforwardPass() function and returning to main\n",
      " \n",
      "In main; have just completed a single step of backpropagation with inputs 0 0\n",
      "    The new SSE (after backpropagation) = 0.230122\n",
      "    Error(0) = -0.1960,   Error(1) = 0.4378\n",
      "    SSE(0) =   0.0384,   SSE(1) =   0.1917\n",
      "  The difference in initial and the resulting SSEs is: 0.0066\n",
      " \n",
      "   The training has resulted in improving the total SSEs\n",
      " \n",
      "  The previous SSE Total was 2.3245\n",
      "  The new SSE Total was 2.3128\n",
      "    For node 0: Desired Output =  0  New Output = 0.1960\n",
      "    For node 1: Desired Output =  1  New Output = 0.5622\n",
      "    Error(0) = -0.1960,   Error(1) = 0.4378\n",
      "    SSE0(0) =   0.0384,   SSE(1) =   0.1917\n",
      "  Delta in the SSEs is 0.0117\n",
      "SSE improvement\n",
      " \n",
      "Iteration number  1\n",
      "\n",
      "Out of while loop\n",
      "  Initial Total SSE = 2.3297\n",
      "  Final Total SSE = 2.3128\n",
      "  Delta in the SSEs is 0.0170\n",
      "SSE total improvement\n",
      " \n",
      "The initial weights for this neural network are:\n",
      "     Input-to-Hidden                       Hidden-to-Output\n",
      "w(0,0) = -0.217   w(0,1) = -0.899         v(0,0) = -0.753   v(0,1) = -0.780\n",
      "w(1,0) = -0.496   w(1,1) = 0.062         v(1,0) = -0.461   v(1,1) = 0.623\n",
      " \n",
      "The final weights for this neural network are:\n",
      "     Input-to-Hidden                       Hidden-to-Output\n",
      "w(0,0) = -0.219   w(0,1) = -0.900         v(0,0) = -0.761   v(0,1) = -0.800\n",
      "w(1,0) = -0.483   w(1,1) = 0.075         v(1,0) = -0.433   v(1,1) = 0.685\n",
      " \n",
      "The SSE values at the beginning of training were: \n",
      "  SSE_Initial[0] = 0.2418\n",
      "  SSE_Initial[1] = 0.9385\n",
      "  SSE_Initial[2] = 0.8963\n",
      "  SSE_Initial[3] = 0.2532\n",
      " \n",
      "The total of the SSE values at the beginning of training is 2.3297\n",
      " \n",
      "The SSE values at the end of training were: \n",
      "  SSE[0] = 0.2301\n",
      "  SSE[1] = 0.9385\n",
      "  SSE[2] = 0.8963\n",
      "  SSE[3] = 0.2479\n",
      " \n",
      "The total of the SSE values at the end of training is 2.3128\n",
      " \n",
      "Values for the new outputs compared with previous, given only a partial backpropagation training:\n",
      "     Old:     New:     nu*Delta:\n",
      "Output 0:  Desired =  0 Old actual =  0.1973 Newactual  0.1960\n",
      "Output 1:  Desired =  1 Old actual =  0.5552 Newactual  0.5622\n",
      "\n",
      "ending main() function\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    print('Starting main() function')\n",
    "    print()\n",
    "    print('Calling welcome() function')\n",
    "\n",
    "    welcome()\n",
    "    \n",
    "\n",
    "# Parameter definitions, to be replaced with user inputs\n",
    "    print('setting alpha, summedInput, maxNumIterations, and eta')\n",
    "    alpha = 1.0             # parameter governing steepness of sigmoid transfer function\n",
    "    summedInput = 1\n",
    "    maxNumIterations = 2    # You can adjust this parameter; 10,000 typically gives good results when training. \n",
    "#   To debug, or to see detailed step-by-step results, change maxNumIterations to a very small number\n",
    "#     e.g., maxNumIterations = 10000\n",
    "#   And then set the debug parameters below to be \"False\"      \n",
    "    eta = 0.5               # training rate  \n",
    "    print()\n",
    "    print('alpha = ', alpha, 'summedInput = ', summedInput)\n",
    "    print('maxNumIterations = ', maxNumIterations, 'eta = ', eta)   \n",
    "    \n",
    "# Set default values for debug parameters\n",
    "#    We are setting the debug parameters to be \"Off\" (debugxxxOff = True)\n",
    "#    This means that we will NOT see most of the print statements\n",
    "#    If we want to see a lot of interim print statements, change either or both of the \n",
    "#      debugxxxOff parameters to be False, e.g., \"debugInitializeOff = False\"\n",
    "    print('Setting the debug_xxx_off statements to be false, so will print extra')\n",
    "    debugCallInitializeOff = False\n",
    "    debugInitializeOff = False    \n",
    "\n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# This defines the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Notice that I'm using the same variable name, 'arraySizeList' both here in main and in the \n",
    "#    called procedure, 'obtainNeuralNetworkSizeSpecs.' \n",
    "# I don't have to use the same name; the procedure returns a list and I'm assigning it HERE \n",
    "#    to the list named arraySizeList in THIS 'main' procedure. \n",
    "# I could use different names. \n",
    "# I'm keeping the same name so that it is easier for us to connect what happens in the called procedure\n",
    "#    'obtainNeuralNetworkSizeSpecs' with this procedure, 'main.' \n",
    "       \n",
    "# Obtain the array sizes (arraySizeList) by calling the appropriate function\n",
    "    print()\n",
    "    print('calling the obtainNeuralNetworkSizeSpecs() function')\n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs()\n",
    "\n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers    \n",
    "    inputArrayLength = arraySizeList[0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# In addition to connection weights, we also use bias weights:\n",
    "#   - one bias term for each of the hidden nodes\n",
    "#   - one bias term for each of the output nodes\n",
    "#   This means that a 1-D array of bias weights for the hidden nodes will have the same dimension as \n",
    "#      the hidden array length, and\n",
    "#   also a 1-D array of bias weights for the output nodes will have the same dimension as\n",
    "#      the output array length. \n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength     \n",
    "\n",
    "# I have all sorts of debug statements left in this, so you can trace the code moving into and out of\n",
    "#   various procedures and functions.    \n",
    "            \n",
    "    print('Flow-of-control trace: Back in main')\n",
    "    print('I = number of nodes in input layer is', inputArrayLength)\n",
    "    print('H = number of nodes in hidden layer is', hiddenArrayLength)         \n",
    "    print('O = number of nodes in output layer is', outputArrayLength)                             \n",
    "\n",
    "\n",
    "# Initialize the training list\n",
    "# Note: The training list has, in order, the two input nodes, the two output nodes (this is a two-output\n",
    "#   version of the X-OR problem), and the data set number (0..3), meaning that each data set is numbered. \n",
    "#   This helps in going through the entire data set once the initial weights are established to get a \n",
    "#   total sum (across all data sets) of the Summed Squared Error, or SSE.\n",
    "\n",
    "# Thus, I am initializing the training data list with FIVE values; \n",
    "#  -- Zeroth and first values: two inputs into the input layer\n",
    "#  -- Second and third values: desired values for the two different nodes in the output layer\n",
    "#  -- Fourth value: the NUMBER of the training data set (which runs from 0 .. 3, for four different data sets)\n",
    "# All of these are being given an initial value of \"0\"; we will get actual values\n",
    "#     when we call a function to return an actual (randomly-selected) training data set. \n",
    "    print()\n",
    "    print('Initialize the trainingDataList to (0,0,0,0,0')\n",
    "    trainingDataList = (0,0,0,0,0)\n",
    "           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "\n",
    "# We have a single function to initialize weights in a connection weight matrix (2-D array).\n",
    "#   This function needs to know the sizes (lengths) of the lower and the upper sets of nodes.\n",
    "#   These form the [row, column] size specifications for the returned weight matrices (2-D arrays). \n",
    "#   We will store these sizes in each of two different lists. \n",
    "\n",
    "# Specify the sizes for the input-to-hidden connection weight matrix (2-D array)\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "# Specify the sizes for the hidden-to-output connection weight matrix (2-D array)    \n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength  \n",
    "    \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array    \n",
    "\n",
    "# Debug parameter for examining results within initializeWeightArray is currently set to False\n",
    "\n",
    "    if not debugCallInitializeOff:\n",
    "        print()\n",
    "        print(\"Calling initializeWeightArray for input-to-hidden weights\")\n",
    "\n",
    "# Obtain the actual (randomly-initialized) values for the input-to-hidden connection weight matrix.\n",
    "\n",
    "    wWeightArray = initializeWeightArray(wWeightArraySizeList, debugInitializeOff)\n",
    "\n",
    "    if not debugCallInitializeOff:\n",
    "        print()\n",
    "        print(\"Calling initializeWeightArray for Hidden-to-Output weights\")    \n",
    "\n",
    "# Obtain the actual (randomly-initialized) values for the hidden-to-output connection weight matrix.    \n",
    "    vWeightArray = initializeWeightArray(vWeightArraySizeList, debugInitializeOff)\n",
    "\n",
    "# Now, we similarly need to obtain randomly-initialized values for the two sets of bias weights. \n",
    "#    Each set of bias weights is stored in its respective 1-D array \n",
    "#    Recall that we have previously initialized the SIZE for each of these 1-D arrays.  \n",
    "\n",
    "    print()\n",
    "    print('calling initializeBiasWeightArray() function for Hidden nodes')\n",
    "    print()\n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    print()\n",
    "    print('calling initializeBiasWeightArray() function for Output nodes')\n",
    "    print()\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "\n",
    "# In a typical program, we would start changing the values of all the connection and bias weights.\n",
    "#    Here, we want to keep track of our initial values, so we can see how much they change. \n",
    "#    To do this, we create separate weightArray matrices, and separate biasArrays, and set them equal to\n",
    "#    the initial values that we've just obtained. \n",
    "\n",
    "    initialWWeightArray = wWeightArray[:]\n",
    "    initialVWeightArray = vWeightArray[:]\n",
    "    initialBiasHiddenWeightArray = biasHiddenWeightArray[:]   \n",
    "    initialBiasOutputWeightArray = biasOutputWeightArray[:] \n",
    "    \n",
    "    print()\n",
    "    print(\"The initial weights for this neural network are:\")\n",
    "    print(\"       Input-to-Hidden                            Hidden-to-Output\")\n",
    "    print(\"  w(0,0) = %.4f   w(1,0) = %.4f         v(0,0) = %.4f   v(1,0) = %.4f\" % (initialWWeightArray[0,0], \n",
    "        initialWWeightArray[0,1], initialVWeightArray[0,0], initialVWeightArray[0,1]))\n",
    "    print(\"  w(0,1) = %.4f   w(1,1) = %.4f         v(0,1) = %.4f   v(1,1) = %.4f\" % (initialWWeightArray[1,0], \n",
    "        initialWWeightArray[1,1], initialVWeightArray[1,0], initialVWeightArray[1,1])) \n",
    "    print()\n",
    "    print(\"       Bias at Hidden Layer                          Bias at Output Layer\")\n",
    "    print(\"       b(hidden,0) = %.4f                           b(output,0) = %.4f\" % (biasHiddenWeightArray[0],\n",
    "        biasOutputWeightArray[0] ) )                 \n",
    "    print(\"       b(hidden,1) = %.4f                           b(output,1) = %.4f\" % (biasHiddenWeightArray[1],\n",
    "        biasOutputWeightArray[1] )  )\n",
    "  \n",
    "# Establish some parameters just before we start training\n",
    "    print()\n",
    "    print('Establishing parameters - epsilon, iteration counter, SSE_InitialTotal')\n",
    "    print()\n",
    "    epsilon = 0.2 # epsilon determines when we are done training; \n",
    "                  # for each presentation of a training data set, we get a new value for the summed squared error (SSE)\n",
    "                  # and we will terminate the run when any ONE of these SSEs is < epsilon;\n",
    "                  # Note that this is a very crude stopping criterion, we can refine it in later versions. \n",
    "                  # must b\n",
    "    iteration = 0 # This counts the number of iterations that we've made through the training cycle.\n",
    "    SSE_InitialTotal = 0.0 # We initially set the SSE to be zero before any training pass; we accumulate inputs\n",
    "                           # into the SSE once we have a set of weights, and push the input data through the network,\n",
    "                           # generating a set of outputs. \n",
    "                           # We compare the generated outputs (actuals) with the desired, obtain errors, square them, \n",
    "                           # and sum across all the outputs. This gives our SSE for that particular data set, for that \n",
    "                           # particular training pass. \n",
    "                           # If the SSE is low enough (< epsilon), we stop training. \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Next step - Get an initial value for the Total Summed Squared Error (Total_SSE)\n",
    "#   The function will return an array of SSE values, SSE_Array[0] ... SSE_Array[3] are the initial SSEs\n",
    "#   for training sets 0..3; SSE_Array[4] is the sum of the SSEs. \n",
    "####################################################################################################                \n",
    "        \n",
    "                        \n",
    "# Initialize an array of SSE values\n",
    "# The first four SSE values are the SSE's for specific input/output pairs; \n",
    "#   the fifth is the sum of all the SSE's.\n",
    "    print('Initialize SSE_InitialArray to zeros')\n",
    "    SSE_InitialArray = [0,0,0,0,0]\n",
    "    \n",
    "# Before starting the training run, compute the initial SSE Total \n",
    "#   (sum across SSEs for each training data set) \n",
    "    debugSSE_InitialComputationOff = False\n",
    "\n",
    "    print('calling the computeSSE_Values() function to initialize the SSE_InitialArray')\n",
    "    SSE_InitialArray = computeSSE_Values(alpha, SSE_InitialArray, wWeightArray, vWeightArray, \n",
    "                                          biasHiddenWeightArray, biasOutputWeightArray, debugSSE_InitialComputationOff)    \n",
    "\n",
    "# Start the SSE_Array at the same values as the Initial SSE Array\n",
    "    SSE_Array = SSE_InitialArray[:] \n",
    "    SSE_InitialTotal = SSE_Array[4] \n",
    "    print('The SSE_Array = ', SSE_Array)\n",
    "    print('The SSE_InitialTotal = ', SSE_InitialTotal)\n",
    "    \n",
    "# Optionally, print a summary of the initial SSE Total (sum across SSEs for each training data set) \n",
    "#   and the specific SSE values \n",
    "# Set a local debug print parameter \n",
    "    debugSSE_InitialComputationReportOff = False    \n",
    "\n",
    "    if not debugSSE_InitialComputationReportOff:\n",
    "        print()\n",
    "        print(\"In main, SSE computations completed, Total of all SSEs = %.4f\" % SSE_Array[4])\n",
    "        print(\"  For input nodes (0,0), SSE_Array[0] = %.4f\" % SSE_Array[0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        print(\"  For input nodes (0,1), SSE_Array[1] = %.4f\" % SSE_Array[1]) \n",
    "        print(\"  For input nodes (1,0), SSE_Array[2] = %.4f\" % SSE_Array[2]) \n",
    "        print(\"  For input nodes (1,1), SSE_Array[3] = %.4f\" % SSE_Array[3]) \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"About to enter the while loop for \", maxNumIterations, \" iterations\")\n",
    "    print() \n",
    "    \n",
    "# start the while loop\n",
    "\n",
    "    while iteration < maxNumIterations:\n",
    "        if iteration == 0:\n",
    "            print()\n",
    "            print('starting at top of while loop for first time')\n",
    "        else:\n",
    "            print()\n",
    "            print('starting at the top of the while loop again')\n",
    "           \n",
    "\n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of input values for the X-OR problem; two integers - can be 0 or 1\n",
    "####################################################################################################                \n",
    "\n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        print()\n",
    "        print('create random trainingDataList by calling obtainRandomXORTrainingValues() function')\n",
    "        trainingDataList = obtainRandomXORTrainingValues () \n",
    "        input0 = trainingDataList[0]\n",
    "        input1 = trainingDataList[1] \n",
    "        desiredOutput0 = trainingDataList[2]\n",
    "        desiredOutput1 = trainingDataList[3]\n",
    "        setNumber = trainingDataList[4] # obtain the number (0 ... 3) of the training data set.       \n",
    "        print()\n",
    "        print(\"Iteration number \", iteration)\n",
    "        print()\n",
    "        print(\"Randomly selected training data set number \", trainingDataList[4]) \n",
    "        print(\"The inputs and desired outputs for the X-OR problem from this data set are:\")\n",
    "        print(\"          Input0 = \", input0,         \"            Input1 = \", input1 )  \n",
    "        print(\" Desired Output0 = \", desiredOutput0, \"   Desired Output1 = \", desiredOutput1 )   \n",
    "        print()\n",
    "         \n",
    "\n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass\n",
    "####################################################################################################                \n",
    " \n",
    "# Initialize the error list\n",
    "        errorList = (0,0)\n",
    "    \n",
    "# Initialize the actualOutput list\n",
    "#    Remember, we've hard-coded the number of hidden nodes and output nodes in this version;\n",
    "#      numHiddenNodes = 2; numOutputNodes = 2. \n",
    "#    We want to see the ACTUAL VALUES (\"activations\") of both the hidden AND the output nodes;\n",
    "#      this is just to satisfy our own interest. \n",
    "#    In just a few lines down, we will use the function \"ComputeSingleFeedforwardPass\" to get us all \n",
    "#      of those activations. \n",
    "        actualAllNodesOutputList = (0,0,0,0)     \n",
    "\n",
    "# Create the inputData list      \n",
    "        inputDataList = (input0, input1)         \n",
    "    \n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "        debugComputeSingleFeedforwardPassOff = False\n",
    "        print('calling ComputeSingleFeedforwardPass to obtain actual outputs for training set')\n",
    "        actualAllNodesOutputList = ComputeSingleFeedforwardPass(alpha, inputDataList, \n",
    "            wWeightArray, vWeightArray, biasHiddenWeightArray, biasOutputWeightArray,debugComputeSingleFeedforwardPassOff)\n",
    "\n",
    "# Assign the hidden and output values to specific different variables\n",
    "        actualHiddenOutput0 = actualAllNodesOutputList [0] \n",
    "        actualHiddenOutput1 = actualAllNodesOutputList [1] \n",
    "        actualOutput0 = actualAllNodesOutputList [2]\n",
    "        actualOutput1 = actualAllNodesOutputList [3] \n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        error0 = desiredOutput0 - actualOutput0\n",
    "        error1 = desiredOutput1 - actualOutput1\n",
    "        errorList = (error0, error1)\n",
    "    \n",
    "# Compute the Summed Squared Error, or SSE\n",
    "        SSE0 = error0**2\n",
    "        SSE1 =  error1**2\n",
    "        SSEInitial = SSE0 + SSE1\n",
    "        \n",
    "                           \n",
    "        debugMainComputeForwardPassOutputsOff = False\n",
    "        \n",
    "# Debug print: the actual outputs from the two output neurons\n",
    "        if not debugMainComputeForwardPassOutputsOff:\n",
    "            print()\n",
    "            print(\"In main; have just completed a feedfoward pass with training set inputs\", input0, input1)\n",
    "            print(\"  The activations (actual outputs) for the two hidden neurons are:\")\n",
    "            print(\"    actualHiddenOutput0 = %.4f\" % actualHiddenOutput0)\n",
    "            print(\"    actualHiddenOutput1 = %.4f\" % actualHiddenOutput1)   \n",
    "            print(\"  The activations (actual outputs) for the two output neurons are:\")\n",
    "            print(\"    actualOutput0 = %.4f\" % actualOutput0)\n",
    "            print(\"    actualOutput1 = %.4f'\" % actualOutput1 )\n",
    "            print(\"  Initial SSE (before backpropagation) = %.6f\" % SSEInitial)\n",
    "            print(\"  Corresponding SSE (from initial SSE determination) = %.6f\" % SSE_Array[setNumber])    \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes\n",
    "        print()\n",
    "        print('Calling BackpropagateOutputToHidden() function')    \n",
    "        newVWeightArray = BackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "        vWeightArray)\n",
    "\n",
    "        print()\n",
    "        print('Calling BackpropagateBiasOutputWeights() function')\n",
    "        newBiasOutputWeightArray = BackpropagateBiasOutputWeights (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "        biasOutputWeightArray)\n",
    "        newBiasOutputWeight0 = newBiasOutputWeightArray[0]\n",
    "        newBiasOutputWeight1 = newBiasOutputWeightArray[1]\n",
    "        \n",
    "        print()\n",
    "        print('Calling BackpropagateHiddenToInput() function')\n",
    "        newWWeightArray = BackpropagateHiddenToInput (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "        print()\n",
    "        print('Calling BackpropagateBiasHiddenWeights() function')\n",
    "        newBiasHiddenWeightArray = BackpropagateBiasHiddenWeights (alpha, eta, errorList, actualAllNodesOutputList, \n",
    "        vWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "        newBiasHiddenWeight0 = newBiasHiddenWeightArray[0]\n",
    "        newBiasHiddenWeight1 = newBiasHiddenWeightArray[1]        \n",
    "        \n",
    "        newBiasWeightArray = [[newBiasOutputWeight0, newBiasOutputWeight1], [newBiasHiddenWeight0, newBiasHiddenWeight1]] \n",
    "        # I moved these here from just above this line, because got a 'referenced before assignment' erro\n",
    "        print()\n",
    "        print('The newBiasWeightArray = ', newBiasWeightArray)\n",
    "\n",
    "# Debug prints on the weight arrays\n",
    "        debugWeightArrayOff = False\n",
    "        if not debugWeightArrayOff:\n",
    "            print(' ')\n",
    "            print('    The weights before backpropagation are:')\n",
    "            print('         Input-to-Hidden                           Hidden-to-Output')\n",
    "            print('    w(0,0) = %.3f   w(1,0) = %.3f         v(0,0) = %.3f   v(1,0) = %.3f' % (wWeightArray[0,0], \n",
    "            wWeightArray[0,1], vWeightArray[0,0], vWeightArray[0,1]))\n",
    "            print('    w(0,1) = %.3f   w(1,1) = %.3f         v(0,1) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n",
    "            wWeightArray[1,1], vWeightArray[1,0], vWeightArray[1,1]))             \n",
    "            print(' ')\n",
    "            print('    The weights after backpropagation are:')\n",
    "            print('         Input-to-Hidden                           Hidden-to-Output')\n",
    "            print('    w(0,0) = %.3f   w(1,0) = %.3f         v(0,0) = %.3f   v(1,0) = %.3f' % (newWWeightArray[0,0], \n",
    "            newWWeightArray[0,1], newVWeightArray[0,0], newVWeightArray[0,1]))\n",
    "            print('    w(0,1) = %.3f   w(1,1) = %.3f         v(0,1) = %.3f   v(1,1) = %.3f' % (newWWeightArray[1,0], \n",
    "            newWWeightArray[1,1], newVWeightArray[1,0], newVWeightArray[1,1]))\n",
    "            \n",
    "# Assign the old hidden-to-output weight array to be the same as what was returned from the BP weight update\n",
    "        vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "# Assign the old input-to-hidden weight array to be the same as what was returned from the BP weight update\n",
    "        wWeightArray = newWWeightArray[:]\n",
    "    \n",
    "# Run the computeSingleFeedforwardPass again, to compare the results after just adjusting the hidden-to-output weights\n",
    "        print()\n",
    "        print('Calling the ComputeSingleFeedforwardPass() function')\n",
    "        newAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n",
    "        biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)         \n",
    "        newOutput0 = newAllNodesOutputList [2]\n",
    "        newOutput1 = newAllNodesOutputList [3] \n",
    "\n",
    "# Determine the new error between actual and desired outputs\n",
    "        newError0 = desiredOutput0 - newOutput0\n",
    "        newError1 = desiredOutput1 - newOutput1\n",
    "        newErrorList = (newError0, newError1)\n",
    "\n",
    "# Compute the new Summed Squared Error, or SSE\n",
    "        SSE0 = newError0**2\n",
    "        SSE1 = newError1**2\n",
    "        newSSE = SSE0 + SSE1\n",
    "      \n",
    "# Print the Summed Squared Error  \n",
    "\n",
    "# Debug print the actual outputs from the two output neurons\n",
    "        if not debugMainComputeForwardPassOutputsOff:\n",
    "            print(' ')\n",
    "            print('In main; have just completed a single step of backpropagation with inputs', input0, input1)\n",
    "            print('    The new SSE (after backpropagation) = %.6f' % newSSE)\n",
    "            # RT - changed NewError to newError, because an error was created\n",
    "            print('    Error(0) = %.4f,   Error(1) = %.4f' %(newError0, newError1))\n",
    "            print('    SSE(0) =   %.4f,   SSE(1) =   %.4f' % (SSE0, SSE1))\n",
    "            deltaSSE = SSEInitial - newSSE\n",
    "            print('  The difference in initial and the resulting SSEs is: %.4f' % deltaSSE) \n",
    "            if deltaSSE >0:\n",
    "                print(' ')\n",
    "                print('   The training has resulted in improving the total SSEs')  \n",
    "           \n",
    "\n",
    "# Assign the SSE to the SSE for the appropriate training set\n",
    "        SSE_Array[setNumber] = newSSE\n",
    "\n",
    "# Obtain the previous SSE Total from the SSE array\n",
    "        previousSSE_Total = SSE_Array[4]\n",
    "        print(' ') \n",
    "        print('  The previous SSE Total was %.4f' % previousSSE_Total)\n",
    "\n",
    "# Compute the new sum of SSEs (across all the different training sets)\n",
    "#   ... this will be different because we've changed one of the SSE's\n",
    "        newSSE_Total = SSE_Array[0] + SSE_Array[1] +SSE_Array[2] + SSE_Array[3]\n",
    "        print('  The new SSE Total was %.4f' % newSSE_Total)\n",
    "        print('    For node 0: Desired Output = ',desiredOutput0,  ' New Output = %.4f' % newOutput0) \n",
    "        print('    For node 1: Desired Output = ',desiredOutput1,  ' New Output = %.4f' % newOutput1)  \n",
    "        print('    Error(0) = %.4f,   Error(1) = %.4f' %(newError0, newError1))\n",
    "        print('    SSE0(0) =   %.4f,   SSE(1) =   %.4f' % (SSE0, SSE1))                \n",
    "# Assign the new SSE to the final place in the SSE array\n",
    "        SSE_Array[4] = newSSE_Total\n",
    "        deltaSSE = previousSSE_Total - newSSE_Total\n",
    "        print('  Delta in the SSEs is %.4f' % deltaSSE) \n",
    "        if deltaSSE > 0:\n",
    "            print('SSE improvement')\n",
    "        else: print('NO improvement')     \n",
    "                \n",
    "                         \n",
    "# Assign the new errors to the error list             \n",
    "        errorList = newErrorList[:]\n",
    "        \n",
    "        \n",
    "        print(' ')\n",
    "        print('Iteration number ', iteration)\n",
    "        iteration = iteration + 1\n",
    "\n",
    "        if newSSE_Total < epsilon:\n",
    "\n",
    "            \n",
    "            break\n",
    "    print()\n",
    "    print('Out of while loop')  \n",
    "\n",
    "\n",
    "    debugEndingSSEComparisonOff = False\n",
    "    if not debugEndingSSEComparisonOff:\n",
    "        SSE_Array[4] = newSSE_Total\n",
    "        deltaSSE = previousSSE_Total - newSSE_Total\n",
    "        print('  Initial Total SSE = %.4f'  % SSE_InitialTotal)\n",
    "        print('  Final Total SSE = %.4f'  % newSSE_Total)\n",
    "        finalDeltaSSE = SSE_InitialTotal - newSSE_Total\n",
    "        print('  Delta in the SSEs is %.4f' % finalDeltaSSE) \n",
    "        if finalDeltaSSE > 0:\n",
    "            print('SSE total improvement')\n",
    "        else: print('NO improvement in total SSE') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(' ')\n",
    "    print('The initial weights for this neural network are:')\n",
    "    print('     Input-to-Hidden                       Hidden-to-Output')\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (initialWWeightArray[0,0], \n",
    "        initialWWeightArray[0,1], initialVWeightArray[0,0], initialVWeightArray[0,1]))\n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (initialWWeightArray[1,0], \n",
    "        initialWWeightArray[1,1], initialVWeightArray[1,0], initialVWeightArray[1,1]))        \n",
    "\n",
    "                                                                                    \n",
    "    print(' ')\n",
    "    print('The final weights for this neural network are:')\n",
    "    print('     Input-to-Hidden                       Hidden-to-Output')\n",
    "    print('w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (wWeightArray[0,0], \n",
    "        wWeightArray[0,1], vWeightArray[0,0], vWeightArray[0,1]))\n",
    "    print('w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n",
    "        wWeightArray[1,1], vWeightArray[1,0], vWeightArray[1,1]))        \n",
    "                                                                                    \n",
    "   \n",
    "# Print the SSE's at the beginning of training\n",
    "    print(' ')\n",
    "    print('The SSE values at the beginning of training were: ')\n",
    "    print('  SSE_Initial[0] = %.4f' % SSE_InitialArray[0])\n",
    "    print('  SSE_Initial[1] = %.4f' % SSE_InitialArray[1])\n",
    "    print('  SSE_Initial[2] = %.4f' % SSE_InitialArray[2])\n",
    "    print('  SSE_Initial[3] = %.4f' % SSE_InitialArray[3])   \n",
    "    print(' ')\n",
    "    print('The total of the SSE values at the beginning of training is %.4f' % SSE_InitialTotal) \n",
    "\n",
    "\n",
    "# Print the SSE's at the end of training\n",
    "    print(' ')\n",
    "    print('The SSE values at the end of training were: ')\n",
    "    print('  SSE[0] = %.4f' % SSE_Array[0])\n",
    "    print('  SSE[1] = %.4f' % SSE_Array[1])\n",
    "    print('  SSE[2] = %.4f' % SSE_Array[2])\n",
    "    print('  SSE[3] = %.4f' % SSE_Array[3])    \n",
    "    print(' ')\n",
    "    print('The total of the SSE values at the end of training is %.4f' % SSE_Array[4])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "# Print comparison of previous and new outputs if using backprop            \n",
    "    print(' ') \n",
    "    print('Values for the new outputs compared with previous, given only a partial backpropagation training:')\n",
    "    print('     Old:', '   ', 'New:', '   ', 'nu*Delta:')\n",
    "    print('Output 0:  Desired = ', desiredOutput0, 'Old actual =  %.4f' % actualOutput0, 'Newactual  %.4f' % newOutput0)\n",
    "    print('Output 1:  Desired = ', desiredOutput1, 'Old actual =  %.4f' % actualOutput1, 'Newactual  %.4f' % newOutput1)                                                                        \n",
    "    \n",
    "    print()\n",
    "    print('ending main() function')   \n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "#################################################################################################### \n",
    "# This command is what executes the main() function\n",
    "\n",
    "print('Calling the main() procedure from the program')\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
