{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are written by both Yining Feng and Yunze Chen based on Dr. Jason Brownlee's book **Deep Learning for Time Series Forecasting**. The Monte Carlo Simulation function *monte_carlo_sim(avg, std_dev, num_simulations, df_input)* is created based on the tutorial by Chris Moffitt (https://pbpython.com/monte-carlo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-04</td>\n",
       "      <td>6386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>7653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>7811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>8530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Order\n",
       "0  2017-02-04   6386\n",
       "1  2017-02-05    328\n",
       "2  2017-02-06   7653\n",
       "3  2017-02-07   7811\n",
       "4  2017-02-08   8530"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values:\n",
      "Date     1092\n",
      "Order    1058\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_unique = data.nunique()\n",
    "print(\"Number of unique values:\\n{}\".format(n_unique))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "We will use the first two years of data for training predictive models and the final year for evaluating models.\n",
    "\n",
    "The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday.\n",
    "\n",
    "We will split the data into standard weeks, working backwards from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cyxlq\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# split into standard weeks\n",
    "from numpy import split\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "# split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    train.reset_index(inplace=True)\n",
    "    test.reset_index(inplace=True)\n",
    "    train.drop(columns={'index'}, inplace=True)\n",
    "    test.drop(columns={'index'}, inplace=True)\n",
    "    return train, test\n",
    "# validate train data\n",
    "train, test = split_dataset(data)\n",
    "\n",
    "# get random series for Monte Carlo\n",
    "def monte_carlo_sim(avg, std_dev, num_simulations, df_input):\n",
    "\n",
    "    # get the parameter of normal distribution\n",
    "    num_reps = len(df_input['Date'])\n",
    "    df_final = df_input\n",
    "\n",
    "    for i in range(1, num_simulations):\n",
    "        # get random number based on normal distribution\n",
    "        pct_to_target = np.random.normal(avg, std_dev, num_reps).round(2)\n",
    "\n",
    "        # assemble the dataframe of a new series\n",
    "        df_temp = pd.DataFrame(index=range(num_reps),\n",
    "                               data={\n",
    "                                   'Date': df_input['Date'],\n",
    "                                   'Target': df_input['Order'],\n",
    "                                   'Pct_To_Target': pct_to_target})\n",
    "        df_temp['Order'] = df_temp['Target'] * df_temp['Pct_To_Target']\n",
    "        df_temp = df_temp.round({'Order': 0})\n",
    "        # df_temp['Series'] = i\n",
    "        df_temp.drop(columns=['Target', 'Pct_To_Target'], inplace=True)\n",
    "\n",
    "        # append the new series\n",
    "        df_final = df_final.append(df_temp, ignore_index=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# validate simulation data\n",
    "test_1 = monte_carlo_sim(1, 0.1, 10, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# restructure into windows of weekly data\n",
    "def split_weekly(data):\n",
    "    data_return = array(split(data, len(data)/7))\n",
    "\n",
    "    return data_return\n",
    "\n",
    "# validate split weekly window\n",
    "weekly_train = split_weekly(train)\n",
    "weekly_test = split_weekly(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily data starts in early 2017.\n",
    "\n",
    "The first Sunday in the dataset is February 5th, which is the second row of data.\n",
    "\n",
    "Organizing the data into standard weeks gives 109 full standard weeks for training a predictive model.\n",
    "\n",
    "The final year of the data is in 2019 and the first Sunday for 2019 was March 10th. The data ends in mid January 2020 and the closest final Saturday in the data is January 25th. This gives 46 weeks of test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-Forward Validation\n",
    "Models will be evaluated using a scheme called walk-forward validation.\n",
    "\n",
    "This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data.\n",
    "\n",
    "The walk-forward validation approach to evaluating predictive models on this dataset is provided below named evaluate_model().\n",
    "\n",
    "The train and test datasets in standard-week format are provided to the function as arguments. An additional argument n_input is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction.\n",
    "\n",
    "Two new functions are called: one to build a model from the training data called build_model() and another that uses the model to make forecasts for each new standard week called forecast(). \n",
    "\n",
    "We are working with neural networks, and as such, they are generally slow to train but fast to evaluate. This means that the preferred usage of the models is to build them once on historical data and to use them to forecast each step of the walk-forward validation. The models are static (i.e. not updated) during their evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "    # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "    # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "    # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the evaluation for a model, we can summarize the performance.\n",
    "\n",
    "The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs for Multi-Step Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore a suite of LSTM architectures for multi-step time series forecasting. Specifically, we will look at how to develop the following models:\n",
    "\n",
    "* LSTM model with vector output for multi-step forecasting with univariate input data.\n",
    "* Encoder-Decoder LSTM model for multi-step forecasting with univariate input data.\n",
    "* CNN-LSTM Encoder-Decoder model for multi-step forecasting with univariate input data.\n",
    "* ConvLSTM Encoder-Decoder model for multi-step forecasting with univariate input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model With Univariate Input and Vector Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by developing a vanilla LSTM model that reads in a sequence of days of total number of daily orders and predicts a vector output of the next standard week of daily orders.\n",
    "\n",
    "The number of prior days used as input defines the one-dimensional (1D) subsequence of data that the LSTM will read and learn to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sample will be comprised of seven time steps with one feature for the seven days of total daily orders.\n",
    "\n",
    "The training dataset has 159 weeks of data, so the shape of the training dataset would be:\n",
    "* [109, 7, 1]\n",
    "\n",
    "The data would use the prior standard week to predict the next standard week. A problem is that 109 instances is not a lot to train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "        # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.5f' % s for s in scores])\n",
    "    print('%s: [%.6f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to create a lot more training data is to change the problem during training to predict the next seven days given the prior seven days, regardless of the standard week.\n",
    "\n",
    "This only impacts the training data, and the test problem remains the same: predict the daily power consumption for the next standard week given the prior standard week.\n",
    "\n",
    "This will require a little preparation of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "# flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to iterate over the time steps and divide the data into overlapping windows; each iteration moves along one time step and predicts the subsequent seven days.\n",
    "\n",
    "We can do this by keeping track of start and end indexes for the inputs and outputs as we iterate across the length of the flattened data in terms of time steps.\n",
    "\n",
    "We can also do this in a way where the number of inputs and outputs are parameterized (e.g. n_input, n_out) so that you can experiment with different values or adapt it for your own problem.\n",
    "\n",
    "Above is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as inputs and outputs and returns the data in the overlapping moving window format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 2, 200, 40960\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='sigmoid',\n",
    "                   recurrent_dropout=0,\n",
    "                   unroll=False,\n",
    "                   use_bias=True,\n",
    "                   input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and fit the LSTM model on the training data.\n",
    "\n",
    "This multi-step time series forecasting problem is an autoregression. That means it is likely best modeled where that the next seven days is some function of observations at prior time steps. This and the relatively small amount of data means that a small model is required.\n",
    "\n",
    "We will develop a model with a single hidden LSTM layer with 200 units. The number of units in the hidden layer is unrelated to the number of time steps in the input sequences. The LSTM layer is followed by a fully connected layer with 200 nodes that will interpret the features learned by the LSTM layer. Finally, an output layer will directly predict a vector with seven elements, one for each day in the output sequence.\n",
    "\n",
    "We will use the mean squared error loss function as it is a good match for our chosen error metric of RMSE. We will use the efficient Adam implementation of stochastic gradient descent and fit the model for 70 epochs with a batch size of 16.\n",
    "\n",
    "The small batch size and the stochastic nature of the algorithm means that the same model will learn a slightly different mapping of inputs to outputs each time it is trained. This means results may vary when the model is evaluated. You can try running the model multiple times and calculate an average of model performance.\n",
    "\n",
    "The build_model() above prepares the training data, defines the model, and fits the model on the training data, returning the fit model ready for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using walk-forward validation to evaluate the model as described in the previous section.\n",
    "\n",
    "This means that we have the observations available for the prior week in order to predict the coming week. These are collected into an array of standard weeks called history.\n",
    "\n",
    "In order to predict the next standard week, we need to retrieve the last days of observations. As with the training data, we must first flatten the history data to remove the weekly structure so that we end up with eight parallel time series.\n",
    "\n",
    "Next, we need to retrieve the last seven days of daily total power consumed (feature index 0).\n",
    "\n",
    "We will parameterize this as we did for the training data so that the number of prior days used as input by the model can be modified in the future.\n",
    "\n",
    "We then make a prediction using the fit model and the input data and retrieve the vector of seven days of output.\n",
    "\n",
    "The forecast() function above implements this and takes as arguments the model fit on the training dataset, the history of data observed so far, and the number of input time steps expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cyxlq\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# set GPU memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# load the new file\n",
    "dataset = pd.read_csv('data.csv',\n",
    "                      infer_datetime_format=True,\n",
    "                      parse_dates=['Date'])\n",
    "\n",
    "# split into train and test\n",
    "train_ori, test_ori = split_dataset(dataset)\n",
    "\n",
    "train = train_ori\n",
    "test = test_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters for Monte Carlo\n",
    "avg = 1\n",
    "std_dev = 0.1\n",
    "num_sim = 100\n",
    "\n",
    "# add monte carlo simulated data\n",
    "train = monte_carlo_sim(avg, std_dev, num_sim, train)\n",
    "test = monte_carlo_sim(avg, std_dev, num_sim, test)\n",
    "\n",
    "# strip date and series\n",
    "train.drop(columns={'Date'}, inplace=True)\n",
    "test.drop(columns={'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# break into 7 days window\n",
    "train = split_weekly(train)\n",
    "test = split_weekly(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 - 0s - loss: 0.0282\n",
      "Epoch 2/200\n",
      "2/2 - 0s - loss: 0.0220\n",
      "Epoch 3/200\n",
      "2/2 - 0s - loss: 0.0163\n",
      "Epoch 4/200\n",
      "2/2 - 0s - loss: 0.0118\n",
      "Epoch 5/200\n",
      "2/2 - 0s - loss: 0.0108\n",
      "Epoch 6/200\n",
      "2/2 - 0s - loss: 0.0118\n",
      "Epoch 7/200\n",
      "2/2 - 0s - loss: 0.0108\n",
      "Epoch 8/200\n",
      "2/2 - 0s - loss: 0.0100\n",
      "Epoch 9/200\n",
      "2/2 - 0s - loss: 0.0101\n",
      "Epoch 10/200\n",
      "2/2 - 0s - loss: 0.0103\n",
      "Epoch 11/200\n",
      "2/2 - 0s - loss: 0.0102\n",
      "Epoch 12/200\n",
      "2/2 - 0s - loss: 0.0100\n",
      "Epoch 13/200\n",
      "2/2 - 0s - loss: 0.0099\n",
      "Epoch 14/200\n",
      "2/2 - 0s - loss: 0.0098\n",
      "Epoch 15/200\n",
      "2/2 - 0s - loss: 0.0097\n",
      "Epoch 16/200\n",
      "2/2 - 0s - loss: 0.0097\n",
      "Epoch 17/200\n",
      "2/2 - 0s - loss: 0.0097\n",
      "Epoch 18/200\n",
      "2/2 - 0s - loss: 0.0097\n",
      "Epoch 19/200\n",
      "2/2 - 0s - loss: 0.0097\n",
      "Epoch 20/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 21/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 22/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 23/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 24/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 25/200\n",
      "2/2 - 0s - loss: 0.0096\n",
      "Epoch 26/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 27/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 28/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 29/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 30/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 31/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 32/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 33/200\n",
      "2/2 - 0s - loss: 0.0095\n",
      "Epoch 34/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 35/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 36/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 37/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 38/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 39/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 40/200\n",
      "2/2 - 0s - loss: 0.0094\n",
      "Epoch 41/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 42/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 43/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 44/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 45/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 46/200\n",
      "2/2 - 0s - loss: 0.0093\n",
      "Epoch 47/200\n",
      "2/2 - 0s - loss: 0.0092\n",
      "Epoch 48/200\n",
      "2/2 - 0s - loss: 0.0092\n",
      "Epoch 49/200\n",
      "2/2 - 0s - loss: 0.0092\n",
      "Epoch 50/200\n",
      "2/2 - 0s - loss: 0.0092\n",
      "Epoch 51/200\n",
      "2/2 - 0s - loss: 0.0092\n",
      "Epoch 52/200\n",
      "2/2 - 0s - loss: 0.0091\n",
      "Epoch 53/200\n",
      "2/2 - 0s - loss: 0.0091\n",
      "Epoch 54/200\n",
      "2/2 - 0s - loss: 0.0091\n",
      "Epoch 55/200\n",
      "2/2 - 0s - loss: 0.0091\n",
      "Epoch 56/200\n",
      "2/2 - 0s - loss: 0.0091\n",
      "Epoch 57/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 58/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 59/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 60/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 61/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 62/200\n",
      "2/2 - 0s - loss: 0.0090\n",
      "Epoch 63/200\n",
      "2/2 - 0s - loss: 0.0089\n",
      "Epoch 64/200\n",
      "2/2 - 0s - loss: 0.0089\n",
      "Epoch 65/200\n",
      "2/2 - 0s - loss: 0.0089\n",
      "Epoch 66/200\n",
      "2/2 - 0s - loss: 0.0089\n",
      "Epoch 67/200\n",
      "2/2 - 0s - loss: 0.0089\n",
      "Epoch 68/200\n",
      "2/2 - 0s - loss: 0.0088\n",
      "Epoch 69/200\n",
      "2/2 - 0s - loss: 0.0088\n",
      "Epoch 70/200\n",
      "2/2 - 0s - loss: 0.0088\n",
      "Epoch 71/200\n",
      "2/2 - 0s - loss: 0.0088\n",
      "Epoch 72/200\n",
      "2/2 - 0s - loss: 0.0087\n",
      "Epoch 73/200\n",
      "2/2 - 0s - loss: 0.0087\n",
      "Epoch 74/200\n",
      "2/2 - 0s - loss: 0.0087\n",
      "Epoch 75/200\n",
      "2/2 - 0s - loss: 0.0087\n",
      "Epoch 76/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 77/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 78/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 79/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 80/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 81/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 82/200\n",
      "2/2 - 0s - loss: 0.0086\n",
      "Epoch 83/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 84/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 85/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 86/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 87/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 88/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 89/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 90/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 91/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 92/200\n",
      "2/2 - 0s - loss: 0.0085\n",
      "Epoch 93/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 94/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 95/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 96/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 97/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 98/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 99/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 100/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 101/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 102/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 103/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 104/200\n",
      "2/2 - 0s - loss: 0.0084\n",
      "Epoch 105/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 106/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 107/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 108/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 109/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 110/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 111/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 112/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 113/200\n",
      "2/2 - 0s - loss: 0.0083\n",
      "Epoch 114/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 115/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 116/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 117/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 118/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 119/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 120/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 121/200\n",
      "2/2 - 0s - loss: 0.0082\n",
      "Epoch 122/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 123/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 124/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 125/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 126/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 127/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 128/200\n",
      "2/2 - 0s - loss: 0.0081\n",
      "Epoch 129/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 130/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 131/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 132/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 133/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 134/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 135/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 136/200\n",
      "2/2 - 0s - loss: 0.0080\n",
      "Epoch 137/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 138/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 139/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 140/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 141/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 142/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 143/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 144/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 145/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 146/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 147/200\n",
      "2/2 - 0s - loss: 0.0079\n",
      "Epoch 148/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 149/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 150/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 151/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 152/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 153/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 154/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 155/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 156/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 157/200\n",
      "2/2 - 0s - loss: 0.0078\n",
      "Epoch 158/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 159/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 160/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 161/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 162/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 163/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 164/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 165/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 166/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 167/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 168/200\n",
      "2/2 - 0s - loss: 0.0077\n",
      "Epoch 169/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 170/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 171/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 172/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 173/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 174/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 175/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 176/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 177/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 178/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 179/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 180/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 181/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 182/200\n",
      "2/2 - 0s - loss: 0.0076\n",
      "Epoch 183/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 184/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 185/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 186/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 187/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 188/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 189/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 190/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 191/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 192/200\n",
      "2/2 - 0s - loss: 0.0075\n",
      "Epoch 193/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 194/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 195/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 196/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 197/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 198/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 199/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "Epoch 200/200\n",
      "2/2 - 0s - loss: 0.0074\n",
      "LSTM: [0.160600] 0.06478, 0.16589, 0.18083, 0.13127, 0.09965, 0.08484, 0.28596\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('LSTM', score, scores)\n",
    "# plot scores\n",
    "days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thr', 'Fri', 'Sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the daily RMSE is created and shown as above.\n",
    "\n",
    "The plot shows that perhaps Fridays are easier days to forecast than the other days and that perhaps Saturday at the end of a standard week is the hardest day to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder LSTM Model With Univariate Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be comprised of two sub models, the encoder to read and encode the input sequence, and the decoder that will read the encoded input sequence and make a one-step prediction for each element in the output sequence.\n",
    "\n",
    "As before, we define an LSTM hidden layer with 200 units. This is the decoder model that will read the input sequence and will output a 200 element vector (one output per unit) that captures features from the input sequence. We will use 14 days of total power consumption as input.\n",
    "\n",
    "First, the internal representation of the input sequence is repeated multiple times, once for each time step in the output sequence. This sequence of vectors will be presented to the LSTM decoder.\n",
    "\n",
    "We then define the decoder as an LSTM hidden layer with 200 units. Importantly, the decoder will output the entire sequence, not just the output at the end of the sequence as we did with the encoder. This means that each of the 200 units will output a value for each of the seven days, representing the basis for what to predict for each day in the output sequence.\n",
    "\n",
    "We will then use a fully connected layer to interpret each time step in the output sequence before the final output layer. Importantly, the output layer predicts a single step in the output sequence, not all seven days at a time. \n",
    "\n",
    "It means that the same fully connected layer and output layer will be used to process each time step provided by the decoder. To achieve this, we will wrap the interpretation layer and the output layer in a TimeDistributed wrapper that allows the wrapped layers to be used for each time step from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 2, 200, 128\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='sigmoid',\n",
    "                   recurrent_dropout=0,\n",
    "                   unroll=False,\n",
    "                   use_bias=True,\n",
    "                   input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='sigmoid',\n",
    "                   recurrent_dropout=0,\n",
    "                   unroll=False,\n",
    "                   use_bias=True,\n",
    "                   return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "596/596 - 3s - loss: 0.0090\n",
      "Epoch 2/200\n",
      "596/596 - 3s - loss: 0.0079\n",
      "Epoch 3/200\n",
      "596/596 - 2s - loss: 0.0073\n",
      "Epoch 4/200\n",
      "596/596 - 2s - loss: 0.0067\n",
      "Epoch 5/200\n",
      "596/596 - 3s - loss: 0.0059\n",
      "Epoch 6/200\n",
      "596/596 - 2s - loss: 0.0049\n",
      "Epoch 7/200\n",
      "596/596 - 3s - loss: 0.0040\n",
      "Epoch 8/200\n",
      "596/596 - 2s - loss: 0.0035\n",
      "Epoch 9/200\n",
      "596/596 - 2s - loss: 0.0030\n",
      "Epoch 10/200\n",
      "596/596 - 3s - loss: 0.0027\n",
      "Epoch 11/200\n",
      "596/596 - 2s - loss: 0.0024\n",
      "Epoch 12/200\n",
      "596/596 - 3s - loss: 0.0022\n",
      "Epoch 13/200\n",
      "596/596 - 2s - loss: 0.0020\n",
      "Epoch 14/200\n",
      "596/596 - 3s - loss: 0.0019\n",
      "Epoch 15/200\n",
      "596/596 - 3s - loss: 0.0018\n",
      "Epoch 16/200\n",
      "596/596 - 2s - loss: 0.0017\n",
      "Epoch 17/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 18/200\n",
      "596/596 - 2s - loss: 0.0015\n",
      "Epoch 19/200\n",
      "596/596 - 2s - loss: 0.0015\n",
      "Epoch 20/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 21/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 22/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 23/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 24/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 25/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 26/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 27/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 28/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 29/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 30/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 31/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 32/200\n",
      "596/596 - 3s - loss: 0.0010\n",
      "Epoch 33/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 34/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 35/200\n",
      "596/596 - 2s - loss: 9.7851e-04\n",
      "Epoch 36/200\n",
      "596/596 - 2s - loss: 9.6398e-04\n",
      "Epoch 37/200\n",
      "596/596 - 2s - loss: 9.5756e-04\n",
      "Epoch 38/200\n",
      "596/596 - 2s - loss: 9.4048e-04\n",
      "Epoch 39/200\n",
      "596/596 - 2s - loss: 9.2299e-04\n",
      "Epoch 40/200\n",
      "596/596 - 3s - loss: 9.0723e-04\n",
      "Epoch 41/200\n",
      "596/596 - 2s - loss: 8.9806e-04\n",
      "Epoch 42/200\n",
      "596/596 - 2s - loss: 8.9450e-04\n",
      "Epoch 43/200\n",
      "596/596 - 2s - loss: 8.6764e-04\n",
      "Epoch 44/200\n",
      "596/596 - 3s - loss: 8.6558e-04\n",
      "Epoch 45/200\n",
      "596/596 - 2s - loss: 8.5604e-04\n",
      "Epoch 46/200\n",
      "596/596 - 2s - loss: 8.4432e-04\n",
      "Epoch 47/200\n",
      "596/596 - 2s - loss: 8.4236e-04\n",
      "Epoch 48/200\n",
      "596/596 - 3s - loss: 8.1704e-04\n",
      "Epoch 49/200\n",
      "596/596 - 2s - loss: 8.0930e-04\n",
      "Epoch 50/200\n",
      "596/596 - 2s - loss: 8.0680e-04\n",
      "Epoch 51/200\n",
      "596/596 - 2s - loss: 8.0025e-04\n",
      "Epoch 52/200\n",
      "596/596 - 2s - loss: 7.7907e-04\n",
      "Epoch 53/200\n",
      "596/596 - 3s - loss: 7.7468e-04\n",
      "Epoch 54/200\n",
      "596/596 - 2s - loss: 7.7190e-04\n",
      "Epoch 55/200\n",
      "596/596 - 2s - loss: 7.6216e-04\n",
      "Epoch 56/200\n",
      "596/596 - 3s - loss: 7.5704e-04\n",
      "Epoch 57/200\n",
      "596/596 - 3s - loss: 7.5817e-04\n",
      "Epoch 58/200\n",
      "596/596 - 3s - loss: 7.4298e-04\n",
      "Epoch 59/200\n",
      "596/596 - 3s - loss: 7.3832e-04\n",
      "Epoch 60/200\n",
      "596/596 - 3s - loss: 7.2305e-04\n",
      "Epoch 61/200\n",
      "596/596 - 2s - loss: 7.2987e-04\n",
      "Epoch 62/200\n",
      "596/596 - 2s - loss: 7.1777e-04\n",
      "Epoch 63/200\n",
      "596/596 - 2s - loss: 7.0562e-04\n",
      "Epoch 64/200\n",
      "596/596 - 2s - loss: 7.1497e-04\n",
      "Epoch 65/200\n",
      "596/596 - 2s - loss: 7.0035e-04\n",
      "Epoch 66/200\n",
      "596/596 - 2s - loss: 6.9596e-04\n",
      "Epoch 67/200\n",
      "596/596 - 3s - loss: 6.8967e-04\n",
      "Epoch 68/200\n",
      "596/596 - 2s - loss: 6.8135e-04\n",
      "Epoch 69/200\n",
      "596/596 - 2s - loss: 6.7510e-04\n",
      "Epoch 70/200\n",
      "596/596 - 2s - loss: 6.8302e-04\n",
      "Epoch 71/200\n",
      "596/596 - 2s - loss: 6.7198e-04\n",
      "Epoch 72/200\n",
      "596/596 - 2s - loss: 6.7155e-04\n",
      "Epoch 73/200\n",
      "596/596 - 2s - loss: 6.6058e-04\n",
      "Epoch 74/200\n",
      "596/596 - 2s - loss: 6.6678e-04\n",
      "Epoch 75/200\n",
      "596/596 - 2s - loss: 6.6206e-04\n",
      "Epoch 76/200\n",
      "596/596 - 2s - loss: 6.4153e-04\n",
      "Epoch 77/200\n",
      "596/596 - 2s - loss: 6.5116e-04\n",
      "Epoch 78/200\n",
      "596/596 - 2s - loss: 6.3260e-04\n",
      "Epoch 79/200\n",
      "596/596 - 2s - loss: 6.3274e-04\n",
      "Epoch 80/200\n",
      "596/596 - 2s - loss: 6.2927e-04\n",
      "Epoch 81/200\n",
      "596/596 - 2s - loss: 6.3049e-04\n",
      "Epoch 82/200\n",
      "596/596 - 2s - loss: 6.2015e-04\n",
      "Epoch 83/200\n",
      "596/596 - 2s - loss: 6.1645e-04\n",
      "Epoch 84/200\n",
      "596/596 - 2s - loss: 6.1287e-04\n",
      "Epoch 85/200\n",
      "596/596 - 2s - loss: 6.2800e-04\n",
      "Epoch 86/200\n",
      "596/596 - 2s - loss: 6.0486e-04\n",
      "Epoch 87/200\n",
      "596/596 - 3s - loss: 6.0970e-04\n",
      "Epoch 88/200\n",
      "596/596 - 3s - loss: 6.1767e-04\n",
      "Epoch 89/200\n",
      "596/596 - 3s - loss: 5.9823e-04\n",
      "Epoch 90/200\n",
      "596/596 - 3s - loss: 5.9461e-04\n",
      "Epoch 91/200\n",
      "596/596 - 3s - loss: 5.8916e-04\n",
      "Epoch 92/200\n",
      "596/596 - 2s - loss: 5.9151e-04\n",
      "Epoch 93/200\n",
      "596/596 - 3s - loss: 5.9256e-04\n",
      "Epoch 94/200\n",
      "596/596 - 2s - loss: 5.8277e-04\n",
      "Epoch 95/200\n",
      "596/596 - 3s - loss: 5.8087e-04\n",
      "Epoch 96/200\n",
      "596/596 - 3s - loss: 5.8527e-04\n",
      "Epoch 97/200\n",
      "596/596 - 3s - loss: 5.7503e-04\n",
      "Epoch 98/200\n",
      "596/596 - 2s - loss: 5.6045e-04\n",
      "Epoch 99/200\n",
      "596/596 - 3s - loss: 5.7271e-04\n",
      "Epoch 100/200\n",
      "596/596 - 3s - loss: 5.8212e-04\n",
      "Epoch 101/200\n",
      "596/596 - 3s - loss: 5.7956e-04\n",
      "Epoch 102/200\n",
      "596/596 - 3s - loss: 5.5500e-04\n",
      "Epoch 103/200\n",
      "596/596 - 2s - loss: 5.5925e-04\n",
      "Epoch 104/200\n",
      "596/596 - 2s - loss: 5.6099e-04\n",
      "Epoch 105/200\n",
      "596/596 - 2s - loss: 5.5501e-04\n",
      "Epoch 106/200\n",
      "596/596 - 3s - loss: 5.4894e-04\n",
      "Epoch 107/200\n",
      "596/596 - 2s - loss: 5.4459e-04\n",
      "Epoch 108/200\n",
      "596/596 - 3s - loss: 5.4134e-04\n",
      "Epoch 109/200\n",
      "596/596 - 3s - loss: 5.4739e-04\n",
      "Epoch 110/200\n",
      "596/596 - 3s - loss: 5.3734e-04\n",
      "Epoch 111/200\n",
      "596/596 - 3s - loss: 5.4692e-04\n",
      "Epoch 112/200\n",
      "596/596 - 3s - loss: 5.4094e-04\n",
      "Epoch 113/200\n",
      "596/596 - 2s - loss: 5.3243e-04\n",
      "Epoch 114/200\n",
      "596/596 - 2s - loss: 5.2543e-04\n",
      "Epoch 115/200\n",
      "596/596 - 2s - loss: 5.3499e-04\n",
      "Epoch 116/200\n",
      "596/596 - 3s - loss: 5.2814e-04\n",
      "Epoch 117/200\n",
      "596/596 - 3s - loss: 5.1956e-04\n",
      "Epoch 118/200\n",
      "596/596 - 2s - loss: 5.3531e-04\n",
      "Epoch 119/200\n",
      "596/596 - 2s - loss: 5.1999e-04\n",
      "Epoch 120/200\n",
      "596/596 - 3s - loss: 5.0785e-04\n",
      "Epoch 121/200\n",
      "596/596 - 2s - loss: 5.2400e-04\n",
      "Epoch 122/200\n",
      "596/596 - 3s - loss: 5.1442e-04\n",
      "Epoch 123/200\n",
      "596/596 - 3s - loss: 5.1004e-04\n",
      "Epoch 124/200\n",
      "596/596 - 3s - loss: 5.0176e-04\n",
      "Epoch 125/200\n",
      "596/596 - 3s - loss: 5.2226e-04\n",
      "Epoch 126/200\n",
      "596/596 - 3s - loss: 5.0040e-04\n",
      "Epoch 127/200\n",
      "596/596 - 2s - loss: 5.0857e-04\n",
      "Epoch 128/200\n",
      "596/596 - 2s - loss: 5.0657e-04\n",
      "Epoch 129/200\n",
      "596/596 - 2s - loss: 4.8737e-04\n",
      "Epoch 130/200\n",
      "596/596 - 3s - loss: 4.9780e-04\n",
      "Epoch 131/200\n",
      "596/596 - 2s - loss: 4.9150e-04\n",
      "Epoch 132/200\n",
      "596/596 - 2s - loss: 5.0452e-04\n",
      "Epoch 133/200\n",
      "596/596 - 2s - loss: 5.0446e-04\n",
      "Epoch 134/200\n",
      "596/596 - 2s - loss: 4.8818e-04\n",
      "Epoch 135/200\n",
      "596/596 - 2s - loss: 4.7709e-04\n",
      "Epoch 136/200\n",
      "596/596 - 2s - loss: 4.8270e-04\n",
      "Epoch 137/200\n",
      "596/596 - 2s - loss: 4.7484e-04\n",
      "Epoch 138/200\n",
      "596/596 - 2s - loss: 4.7335e-04\n",
      "Epoch 139/200\n",
      "596/596 - 2s - loss: 4.9084e-04\n",
      "Epoch 140/200\n",
      "596/596 - 3s - loss: 4.8547e-04\n",
      "Epoch 141/200\n",
      "596/596 - 3s - loss: 4.8116e-04\n",
      "Epoch 142/200\n",
      "596/596 - 3s - loss: 4.7829e-04\n",
      "Epoch 143/200\n",
      "596/596 - 2s - loss: 4.7999e-04\n",
      "Epoch 144/200\n",
      "596/596 - 2s - loss: 4.6519e-04\n",
      "Epoch 145/200\n",
      "596/596 - 2s - loss: 4.7197e-04\n",
      "Epoch 146/200\n",
      "596/596 - 2s - loss: 4.6975e-04\n",
      "Epoch 147/200\n",
      "596/596 - 2s - loss: 4.6378e-04\n",
      "Epoch 148/200\n",
      "596/596 - 2s - loss: 4.5787e-04\n",
      "Epoch 149/200\n",
      "596/596 - 2s - loss: 4.5452e-04\n",
      "Epoch 150/200\n",
      "596/596 - 2s - loss: 4.6496e-04\n",
      "Epoch 151/200\n",
      "596/596 - 2s - loss: 4.5895e-04\n",
      "Epoch 152/200\n",
      "596/596 - 2s - loss: 4.5388e-04\n",
      "Epoch 153/200\n",
      "596/596 - 3s - loss: 4.5457e-04\n",
      "Epoch 154/200\n",
      "596/596 - 3s - loss: 4.5251e-04\n",
      "Epoch 155/200\n",
      "596/596 - 3s - loss: 4.5965e-04\n",
      "Epoch 156/200\n",
      "596/596 - 2s - loss: 4.3848e-04\n",
      "Epoch 157/200\n",
      "596/596 - 2s - loss: 4.4378e-04\n",
      "Epoch 158/200\n",
      "596/596 - 3s - loss: 4.5921e-04\n",
      "Epoch 159/200\n",
      "596/596 - 2s - loss: 4.4010e-04\n",
      "Epoch 160/200\n",
      "596/596 - 3s - loss: 4.3522e-04\n",
      "Epoch 161/200\n",
      "596/596 - 3s - loss: 4.4278e-04\n",
      "Epoch 162/200\n",
      "596/596 - 2s - loss: 4.4387e-04\n",
      "Epoch 163/200\n",
      "596/596 - 2s - loss: 4.4688e-04\n",
      "Epoch 164/200\n",
      "596/596 - 3s - loss: 4.2383e-04\n",
      "Epoch 165/200\n",
      "596/596 - 2s - loss: 4.2747e-04\n",
      "Epoch 166/200\n",
      "596/596 - 2s - loss: 4.7199e-04\n",
      "Epoch 167/200\n",
      "596/596 - 2s - loss: 4.2137e-04\n",
      "Epoch 168/200\n",
      "596/596 - 2s - loss: 4.3361e-04\n",
      "Epoch 169/200\n",
      "596/596 - 2s - loss: 4.3380e-04\n",
      "Epoch 170/200\n",
      "596/596 - 2s - loss: 4.5354e-04\n",
      "Epoch 171/200\n",
      "596/596 - 2s - loss: 4.2247e-04\n",
      "Epoch 172/200\n",
      "596/596 - 2s - loss: 4.0967e-04\n",
      "Epoch 173/200\n",
      "596/596 - 2s - loss: 4.1087e-04\n",
      "Epoch 174/200\n",
      "596/596 - 2s - loss: 4.1542e-04\n",
      "Epoch 175/200\n",
      "596/596 - 2s - loss: 4.4013e-04\n",
      "Epoch 176/200\n",
      "596/596 - 3s - loss: 4.1566e-04\n",
      "Epoch 177/200\n",
      "596/596 - 2s - loss: 4.2469e-04\n",
      "Epoch 178/200\n",
      "596/596 - 2s - loss: 4.2301e-04\n",
      "Epoch 179/200\n",
      "596/596 - 2s - loss: 4.1310e-04\n",
      "Epoch 180/200\n",
      "596/596 - 3s - loss: 4.0040e-04\n",
      "Epoch 181/200\n",
      "596/596 - 3s - loss: 4.0296e-04\n",
      "Epoch 182/200\n",
      "596/596 - 3s - loss: 4.0950e-04\n",
      "Epoch 183/200\n",
      "596/596 - 3s - loss: 4.2157e-04\n",
      "Epoch 184/200\n",
      "596/596 - 2s - loss: 4.2335e-04\n",
      "Epoch 185/200\n",
      "596/596 - 2s - loss: 4.0406e-04\n",
      "Epoch 186/200\n",
      "596/596 - 2s - loss: 3.9323e-04\n",
      "Epoch 187/200\n",
      "596/596 - 2s - loss: 3.9810e-04\n",
      "Epoch 188/200\n",
      "596/596 - 2s - loss: 4.0715e-04\n",
      "Epoch 189/200\n",
      "596/596 - 2s - loss: 3.9162e-04\n",
      "Epoch 190/200\n",
      "596/596 - 2s - loss: 4.1876e-04\n",
      "Epoch 191/200\n",
      "596/596 - 2s - loss: 4.0960e-04\n",
      "Epoch 192/200\n",
      "596/596 - 2s - loss: 4.2165e-04\n",
      "Epoch 193/200\n",
      "596/596 - 2s - loss: 4.1324e-04\n",
      "Epoch 194/200\n",
      "596/596 - 2s - loss: 4.0182e-04\n",
      "Epoch 195/200\n",
      "596/596 - 2s - loss: 3.8861e-04\n",
      "Epoch 196/200\n",
      "596/596 - 2s - loss: 3.7416e-04\n",
      "Epoch 197/200\n",
      "596/596 - 2s - loss: 3.9665e-04\n",
      "Epoch 198/200\n",
      "596/596 - 2s - loss: 3.9858e-04\n",
      "Epoch 199/200\n",
      "596/596 - 3s - loss: 3.7597e-04\n",
      "Epoch 200/200\n",
      "596/596 - 3s - loss: 4.0022e-04\n",
      "LSTM-LSTM: [0.170459] 0.05559, 0.15607, 0.18545, 0.13987, 0.09028, 0.09999, 0.32224\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('LSTM-LSTM', score, scores)\n",
    "# plot scores\n",
    "days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thr', 'Fri', 'Sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the daily RMSE is created and shown as above.\n",
    "\n",
    "The plot shows that perhaps Thursdays are easier days to forecast than the other days and that perhaps Saturday at the end of a standard week is the hardest day to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM Encoder-Decoder Model With Univariate Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a simple but effective CNN architecture for the encoder that is comprised of two convolutional layers followed by a max pooling layer, the results of which are then flattened.\n",
    "\n",
    "The first convolutional layer reads across the input sequence and projects the results onto feature maps. The second performs the same operation on the feature maps created by the first layer, attempting to amplify any salient features. We will use 64 feature maps per convolutional layer and read the input sequences with a kernel size of three time steps.\n",
    "\n",
    "The max pooling layer simplifies the feature maps by keeping 1/4 of the values with the largest (max) signal. The distilled feature maps after the pooling layer are then flattened into one long vector that can then be used as input to the decoding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder cnn-lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 2, 200, 128\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     activation='relu',\n",
    "                     input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='sigmoid',\n",
    "                   recurrent_dropout=0,\n",
    "                   unroll=False,\n",
    "                   use_bias=True,\n",
    "                   return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "596/596 - 2s - loss: 0.0084\n",
      "Epoch 2/200\n",
      "596/596 - 2s - loss: 0.0057\n",
      "Epoch 3/200\n",
      "596/596 - 2s - loss: 0.0047\n",
      "Epoch 4/200\n",
      "596/596 - 2s - loss: 0.0041\n",
      "Epoch 5/200\n",
      "596/596 - 2s - loss: 0.0038\n",
      "Epoch 6/200\n",
      "596/596 - 2s - loss: 0.0034\n",
      "Epoch 7/200\n",
      "596/596 - 2s - loss: 0.0032\n",
      "Epoch 8/200\n",
      "596/596 - 2s - loss: 0.0030\n",
      "Epoch 9/200\n",
      "596/596 - 2s - loss: 0.0029\n",
      "Epoch 10/200\n",
      "596/596 - 2s - loss: 0.0027\n",
      "Epoch 11/200\n",
      "596/596 - 2s - loss: 0.0026\n",
      "Epoch 12/200\n",
      "596/596 - 2s - loss: 0.0025\n",
      "Epoch 13/200\n",
      "596/596 - 2s - loss: 0.0024\n",
      "Epoch 14/200\n",
      "596/596 - 2s - loss: 0.0023\n",
      "Epoch 15/200\n",
      "596/596 - 2s - loss: 0.0022\n",
      "Epoch 16/200\n",
      "596/596 - 2s - loss: 0.0022\n",
      "Epoch 17/200\n",
      "596/596 - 2s - loss: 0.0021\n",
      "Epoch 18/200\n",
      "596/596 - 2s - loss: 0.0021\n",
      "Epoch 19/200\n",
      "596/596 - 2s - loss: 0.0020\n",
      "Epoch 20/200\n",
      "596/596 - 2s - loss: 0.0019\n",
      "Epoch 21/200\n",
      "596/596 - 2s - loss: 0.0019\n",
      "Epoch 22/200\n",
      "596/596 - 2s - loss: 0.0018\n",
      "Epoch 23/200\n",
      "596/596 - 2s - loss: 0.0018\n",
      "Epoch 24/200\n",
      "596/596 - 2s - loss: 0.0018\n",
      "Epoch 25/200\n",
      "596/596 - 2s - loss: 0.0017\n",
      "Epoch 26/200\n",
      "596/596 - 2s - loss: 0.0017\n",
      "Epoch 27/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 28/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 29/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 30/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 31/200\n",
      "596/596 - 2s - loss: 0.0016\n",
      "Epoch 32/200\n",
      "596/596 - 2s - loss: 0.0015\n",
      "Epoch 33/200\n",
      "596/596 - 2s - loss: 0.0015\n",
      "Epoch 34/200\n",
      "596/596 - 2s - loss: 0.0015\n",
      "Epoch 35/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 36/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 37/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 38/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 39/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 40/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 41/200\n",
      "596/596 - 2s - loss: 0.0014\n",
      "Epoch 42/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 43/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 44/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 45/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 46/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 47/200\n",
      "596/596 - 2s - loss: 0.0013\n",
      "Epoch 48/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 49/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 50/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 51/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 52/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 53/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 54/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 55/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 56/200\n",
      "596/596 - 2s - loss: 0.0012\n",
      "Epoch 57/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 58/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 59/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 60/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 61/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 62/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 63/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 64/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 65/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 66/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 67/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 68/200\n",
      "596/596 - 2s - loss: 0.0011\n",
      "Epoch 69/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 70/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 71/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 72/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 73/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 74/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 75/200\n",
      "596/596 - 2s - loss: 0.0010\n",
      "Epoch 76/200\n",
      "596/596 - 2s - loss: 9.9242e-04\n",
      "Epoch 77/200\n",
      "596/596 - 2s - loss: 9.9841e-04\n",
      "Epoch 78/200\n",
      "596/596 - 2s - loss: 9.8306e-04\n",
      "Epoch 79/200\n",
      "596/596 - 2s - loss: 9.8170e-04\n",
      "Epoch 80/200\n",
      "596/596 - 2s - loss: 9.6816e-04\n",
      "Epoch 81/200\n",
      "596/596 - 2s - loss: 9.7037e-04\n",
      "Epoch 82/200\n",
      "596/596 - 2s - loss: 9.7286e-04\n",
      "Epoch 83/200\n",
      "596/596 - 2s - loss: 9.5671e-04\n",
      "Epoch 84/200\n",
      "596/596 - 2s - loss: 9.5295e-04\n",
      "Epoch 85/200\n",
      "596/596 - 2s - loss: 9.4386e-04\n",
      "Epoch 86/200\n",
      "596/596 - 2s - loss: 9.4307e-04\n",
      "Epoch 87/200\n",
      "596/596 - 2s - loss: 9.4195e-04\n",
      "Epoch 88/200\n",
      "596/596 - 2s - loss: 9.3922e-04\n",
      "Epoch 89/200\n",
      "596/596 - 2s - loss: 9.4709e-04\n",
      "Epoch 90/200\n",
      "596/596 - 2s - loss: 9.2374e-04\n",
      "Epoch 91/200\n",
      "596/596 - 2s - loss: 9.1734e-04\n",
      "Epoch 92/200\n",
      "596/596 - 2s - loss: 9.1834e-04\n",
      "Epoch 93/200\n",
      "596/596 - 2s - loss: 9.0690e-04\n",
      "Epoch 94/200\n",
      "596/596 - 2s - loss: 9.0430e-04\n",
      "Epoch 95/200\n",
      "596/596 - 2s - loss: 9.0742e-04\n",
      "Epoch 96/200\n",
      "596/596 - 2s - loss: 8.9432e-04\n",
      "Epoch 97/200\n",
      "596/596 - 2s - loss: 8.9434e-04\n",
      "Epoch 98/200\n",
      "596/596 - 2s - loss: 8.8248e-04\n",
      "Epoch 99/200\n",
      "596/596 - 2s - loss: 8.8811e-04\n",
      "Epoch 100/200\n",
      "596/596 - 2s - loss: 8.7875e-04\n",
      "Epoch 101/200\n",
      "596/596 - 2s - loss: 8.7120e-04\n",
      "Epoch 102/200\n",
      "596/596 - 2s - loss: 8.6861e-04\n",
      "Epoch 103/200\n",
      "596/596 - 2s - loss: 8.7860e-04\n",
      "Epoch 104/200\n",
      "596/596 - 2s - loss: 8.7027e-04\n",
      "Epoch 105/200\n",
      "596/596 - 2s - loss: 8.5989e-04\n",
      "Epoch 106/200\n",
      "596/596 - 2s - loss: 8.5845e-04\n",
      "Epoch 107/200\n",
      "596/596 - 2s - loss: 8.5506e-04\n",
      "Epoch 108/200\n",
      "596/596 - 2s - loss: 8.5127e-04\n",
      "Epoch 109/200\n",
      "596/596 - 2s - loss: 8.5465e-04\n",
      "Epoch 110/200\n",
      "596/596 - 2s - loss: 8.4442e-04\n",
      "Epoch 111/200\n",
      "596/596 - 2s - loss: 8.4699e-04\n",
      "Epoch 112/200\n",
      "596/596 - 2s - loss: 8.3324e-04\n",
      "Epoch 113/200\n",
      "596/596 - 2s - loss: 8.4762e-04\n",
      "Epoch 114/200\n",
      "596/596 - 2s - loss: 8.2155e-04\n",
      "Epoch 115/200\n",
      "596/596 - 2s - loss: 8.3684e-04\n",
      "Epoch 116/200\n",
      "596/596 - 2s - loss: 8.2983e-04\n",
      "Epoch 117/200\n",
      "596/596 - 2s - loss: 8.2050e-04\n",
      "Epoch 118/200\n",
      "596/596 - 2s - loss: 8.2249e-04\n",
      "Epoch 119/200\n",
      "596/596 - 2s - loss: 8.2617e-04\n",
      "Epoch 120/200\n",
      "596/596 - 2s - loss: 8.1836e-04\n",
      "Epoch 121/200\n",
      "596/596 - 2s - loss: 8.1518e-04\n",
      "Epoch 122/200\n",
      "596/596 - 2s - loss: 8.1301e-04\n",
      "Epoch 123/200\n",
      "596/596 - 2s - loss: 8.0606e-04\n",
      "Epoch 124/200\n",
      "596/596 - 2s - loss: 7.9895e-04\n",
      "Epoch 125/200\n",
      "596/596 - 2s - loss: 8.0070e-04\n",
      "Epoch 126/200\n",
      "596/596 - 2s - loss: 8.0044e-04\n",
      "Epoch 127/200\n",
      "596/596 - 2s - loss: 7.8938e-04\n",
      "Epoch 128/200\n",
      "596/596 - 2s - loss: 7.9937e-04\n",
      "Epoch 129/200\n",
      "596/596 - 2s - loss: 7.8556e-04\n",
      "Epoch 130/200\n",
      "596/596 - 2s - loss: 7.7823e-04\n",
      "Epoch 131/200\n",
      "596/596 - 2s - loss: 7.8504e-04\n",
      "Epoch 132/200\n",
      "596/596 - 2s - loss: 7.7822e-04\n",
      "Epoch 133/200\n",
      "596/596 - 2s - loss: 7.8617e-04\n",
      "Epoch 134/200\n",
      "596/596 - 2s - loss: 7.7263e-04\n",
      "Epoch 135/200\n",
      "596/596 - 2s - loss: 7.6723e-04\n",
      "Epoch 136/200\n",
      "596/596 - 2s - loss: 7.6979e-04\n",
      "Epoch 137/200\n",
      "596/596 - 2s - loss: 7.6158e-04\n",
      "Epoch 138/200\n",
      "596/596 - 2s - loss: 7.6788e-04\n",
      "Epoch 139/200\n",
      "596/596 - 2s - loss: 7.6988e-04\n",
      "Epoch 140/200\n",
      "596/596 - 2s - loss: 7.5731e-04\n",
      "Epoch 141/200\n",
      "596/596 - 2s - loss: 7.6052e-04\n",
      "Epoch 142/200\n",
      "596/596 - 2s - loss: 7.5786e-04\n",
      "Epoch 143/200\n",
      "596/596 - 2s - loss: 7.4134e-04\n",
      "Epoch 144/200\n",
      "596/596 - 2s - loss: 7.5673e-04\n",
      "Epoch 145/200\n",
      "596/596 - 2s - loss: 7.4389e-04\n",
      "Epoch 146/200\n",
      "596/596 - 2s - loss: 7.4538e-04\n",
      "Epoch 147/200\n",
      "596/596 - 2s - loss: 7.3910e-04\n",
      "Epoch 148/200\n",
      "596/596 - 2s - loss: 7.3505e-04\n",
      "Epoch 149/200\n",
      "596/596 - 2s - loss: 7.4213e-04\n",
      "Epoch 150/200\n",
      "596/596 - 2s - loss: 7.2505e-04\n",
      "Epoch 151/200\n",
      "596/596 - 2s - loss: 7.3199e-04\n",
      "Epoch 152/200\n",
      "596/596 - 2s - loss: 7.2201e-04\n",
      "Epoch 153/200\n",
      "596/596 - 2s - loss: 7.3332e-04\n",
      "Epoch 154/200\n",
      "596/596 - 2s - loss: 7.2463e-04\n",
      "Epoch 155/200\n",
      "596/596 - 2s - loss: 7.1658e-04\n",
      "Epoch 156/200\n",
      "596/596 - 2s - loss: 7.2019e-04\n",
      "Epoch 157/200\n",
      "596/596 - 2s - loss: 7.2848e-04\n",
      "Epoch 158/200\n",
      "596/596 - 2s - loss: 7.1464e-04\n",
      "Epoch 159/200\n",
      "596/596 - 2s - loss: 7.2308e-04\n",
      "Epoch 160/200\n",
      "596/596 - 2s - loss: 7.1324e-04\n",
      "Epoch 161/200\n",
      "596/596 - 2s - loss: 7.0776e-04\n",
      "Epoch 162/200\n",
      "596/596 - 2s - loss: 7.1138e-04\n",
      "Epoch 163/200\n",
      "596/596 - 2s - loss: 7.1098e-04\n",
      "Epoch 164/200\n",
      "596/596 - 2s - loss: 7.0366e-04\n",
      "Epoch 165/200\n",
      "596/596 - 2s - loss: 6.9393e-04\n",
      "Epoch 166/200\n",
      "596/596 - 2s - loss: 6.9725e-04\n",
      "Epoch 167/200\n",
      "596/596 - 2s - loss: 6.9577e-04\n",
      "Epoch 168/200\n",
      "596/596 - 2s - loss: 6.8488e-04\n",
      "Epoch 169/200\n",
      "596/596 - 2s - loss: 7.0596e-04\n",
      "Epoch 170/200\n",
      "596/596 - 2s - loss: 6.9294e-04\n",
      "Epoch 171/200\n",
      "596/596 - 2s - loss: 6.9556e-04\n",
      "Epoch 172/200\n",
      "596/596 - 2s - loss: 6.8437e-04\n",
      "Epoch 173/200\n",
      "596/596 - 2s - loss: 6.8655e-04\n",
      "Epoch 174/200\n",
      "596/596 - 2s - loss: 6.8604e-04\n",
      "Epoch 175/200\n",
      "596/596 - 2s - loss: 6.7133e-04\n",
      "Epoch 176/200\n",
      "596/596 - 2s - loss: 6.8684e-04\n",
      "Epoch 177/200\n",
      "596/596 - 2s - loss: 6.7345e-04\n",
      "Epoch 178/200\n",
      "596/596 - 2s - loss: 6.6928e-04\n",
      "Epoch 179/200\n",
      "596/596 - 2s - loss: 6.7771e-04\n",
      "Epoch 180/200\n",
      "596/596 - 2s - loss: 6.7547e-04\n",
      "Epoch 181/200\n",
      "596/596 - 2s - loss: 6.7132e-04\n",
      "Epoch 182/200\n",
      "596/596 - 2s - loss: 6.6957e-04\n",
      "Epoch 183/200\n",
      "596/596 - 2s - loss: 6.6008e-04\n",
      "Epoch 184/200\n",
      "596/596 - 2s - loss: 6.7987e-04\n",
      "Epoch 185/200\n",
      "596/596 - 2s - loss: 6.5280e-04\n",
      "Epoch 186/200\n",
      "596/596 - 2s - loss: 6.6074e-04\n",
      "Epoch 187/200\n",
      "596/596 - 2s - loss: 6.5665e-04\n",
      "Epoch 188/200\n",
      "596/596 - 2s - loss: 6.5906e-04\n",
      "Epoch 189/200\n",
      "596/596 - 2s - loss: 6.5502e-04\n",
      "Epoch 190/200\n",
      "596/596 - 2s - loss: 6.6046e-04\n",
      "Epoch 191/200\n",
      "596/596 - 2s - loss: 6.5714e-04\n",
      "Epoch 192/200\n",
      "596/596 - 2s - loss: 6.4569e-04\n",
      "Epoch 193/200\n",
      "596/596 - 2s - loss: 6.4824e-04\n",
      "Epoch 194/200\n",
      "596/596 - 2s - loss: 6.4292e-04\n",
      "Epoch 195/200\n",
      "596/596 - 2s - loss: 6.3944e-04\n",
      "Epoch 196/200\n",
      "596/596 - 2s - loss: 6.5232e-04\n",
      "Epoch 197/200\n",
      "596/596 - 2s - loss: 6.3058e-04\n",
      "Epoch 198/200\n",
      "596/596 - 2s - loss: 6.3622e-04\n",
      "Epoch 199/200\n",
      "596/596 - 2s - loss: 6.4199e-04\n",
      "Epoch 200/200\n",
      "596/596 - 2s - loss: 6.3557e-04\n",
      "Con1D-LSTM: [0.198755] 0.07370, 0.16865, 0.19358, 0.16874, 0.10640, 0.11942, 0.38875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('Con1D-LSTM', score, scores)\n",
    "# plot scores\n",
    "days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thr', 'Fri', 'Sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the daily RMSE is created and shown as above.\n",
    "\n",
    "The plot shows that perhaps Thursdays are easier days to forecast than the other days and that perhaps Saturday at the end of a standard week is the hardest day to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Encoder-Decoder Model With Univariate Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN-LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units for each time step themselves.\n",
    "\n",
    "Where each time step of data is defined as an image of (rows * columns) data points.\n",
    "\n",
    "We are working with a one-dimensional sequence of total daily orders, which we can split into a sequence with a length of seven days for each time step. The ConvLSTM can then perform the CNN process on the seven days of data within each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder convlstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_steps, n_length, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 2, 200, 128\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape into subsequences [samples, time steps, rows, cols, channels]\n",
    "    train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64,\n",
    "                         kernel_size=(1,3),\n",
    "                         activation='relu',\n",
    "                         input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200,\n",
    "                   activation='tanh',\n",
    "                   recurrent_activation='sigmoid',\n",
    "                   recurrent_dropout=0,\n",
    "                   unroll=False,\n",
    "                   use_bias=True,\n",
    "                   return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_steps, n_length, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [samples, time steps, rows, cols, channels]\n",
    "    input_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_steps, n_length, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_steps, n_length, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "596/596 - 8s - loss: 0.0083\n",
      "Epoch 2/200\n",
      "596/596 - 8s - loss: 0.0050\n",
      "Epoch 3/200\n",
      "596/596 - 8s - loss: 0.0038\n",
      "Epoch 4/200\n",
      "596/596 - 8s - loss: 0.0030\n",
      "Epoch 5/200\n",
      "596/596 - 8s - loss: 0.0025\n",
      "Epoch 6/200\n",
      "596/596 - 8s - loss: 0.0022\n",
      "Epoch 7/200\n",
      "596/596 - 8s - loss: 0.0019\n",
      "Epoch 8/200\n",
      "596/596 - 8s - loss: 0.0017\n",
      "Epoch 9/200\n",
      "596/596 - 8s - loss: 0.0016\n",
      "Epoch 10/200\n",
      "596/596 - 8s - loss: 0.0014\n",
      "Epoch 11/200\n",
      "596/596 - 8s - loss: 0.0013\n",
      "Epoch 12/200\n",
      "596/596 - 8s - loss: 0.0012\n",
      "Epoch 13/200\n",
      "596/596 - 8s - loss: 0.0011\n",
      "Epoch 14/200\n",
      "596/596 - 8s - loss: 0.0011\n",
      "Epoch 15/200\n",
      "596/596 - 8s - loss: 9.9522e-04\n",
      "Epoch 16/200\n",
      "596/596 - 8s - loss: 9.4637e-04\n",
      "Epoch 17/200\n",
      "596/596 - 8s - loss: 9.0214e-04\n",
      "Epoch 18/200\n",
      "596/596 - 8s - loss: 8.6176e-04\n",
      "Epoch 19/200\n",
      "596/596 - 8s - loss: 8.2802e-04\n",
      "Epoch 20/200\n",
      "596/596 - 8s - loss: 7.9467e-04\n",
      "Epoch 21/200\n",
      "596/596 - 8s - loss: 7.6272e-04\n",
      "Epoch 22/200\n",
      "596/596 - 8s - loss: 7.3762e-04\n",
      "Epoch 23/200\n",
      "596/596 - 9s - loss: 7.1831e-04\n",
      "Epoch 24/200\n",
      "596/596 - 9s - loss: 6.9891e-04\n",
      "Epoch 25/200\n",
      "596/596 - 9s - loss: 6.8350e-04\n",
      "Epoch 26/200\n",
      "596/596 - 9s - loss: 6.6101e-04\n",
      "Epoch 27/200\n",
      "596/596 - 8s - loss: 6.4691e-04\n",
      "Epoch 28/200\n",
      "596/596 - 9s - loss: 6.3271e-04\n",
      "Epoch 29/200\n",
      "596/596 - 10s - loss: 6.1794e-04\n",
      "Epoch 30/200\n",
      "596/596 - 9s - loss: 6.1296e-04\n",
      "Epoch 31/200\n",
      "596/596 - 9s - loss: 5.9787e-04\n",
      "Epoch 32/200\n",
      "596/596 - 9s - loss: 5.8392e-04\n",
      "Epoch 33/200\n",
      "596/596 - 9s - loss: 5.7993e-04\n",
      "Epoch 34/200\n",
      "596/596 - 9s - loss: 5.7090e-04\n",
      "Epoch 35/200\n",
      "596/596 - 9s - loss: 5.5846e-04\n",
      "Epoch 36/200\n",
      "596/596 - 9s - loss: 5.5450e-04\n",
      "Epoch 37/200\n",
      "596/596 - 10s - loss: 5.4449e-04\n",
      "Epoch 38/200\n",
      "596/596 - 9s - loss: 5.3870e-04\n",
      "Epoch 39/200\n",
      "596/596 - 9s - loss: 5.3201e-04\n",
      "Epoch 40/200\n",
      "596/596 - 9s - loss: 5.2790e-04\n",
      "Epoch 41/200\n",
      "596/596 - 9s - loss: 5.2015e-04\n",
      "Epoch 42/200\n",
      "596/596 - 9s - loss: 5.1913e-04\n",
      "Epoch 43/200\n",
      "596/596 - 9s - loss: 5.0853e-04\n",
      "Epoch 44/200\n",
      "596/596 - 9s - loss: 4.9990e-04\n",
      "Epoch 45/200\n",
      "596/596 - 10s - loss: 4.9895e-04\n",
      "Epoch 46/200\n",
      "596/596 - 9s - loss: 4.9465e-04\n",
      "Epoch 47/200\n",
      "596/596 - 9s - loss: 4.9158e-04\n",
      "Epoch 48/200\n",
      "596/596 - 9s - loss: 4.8450e-04\n",
      "Epoch 49/200\n",
      "596/596 - 9s - loss: 4.8177e-04\n",
      "Epoch 50/200\n",
      "596/596 - 8s - loss: 4.7785e-04\n",
      "Epoch 51/200\n",
      "596/596 - 9s - loss: 4.7130e-04\n",
      "Epoch 52/200\n",
      "596/596 - 8s - loss: 4.6925e-04\n",
      "Epoch 53/200\n",
      "596/596 - 9s - loss: 4.6705e-04\n",
      "Epoch 54/200\n",
      "596/596 - 8s - loss: 4.6144e-04\n",
      "Epoch 55/200\n",
      "596/596 - 9s - loss: 4.5873e-04\n",
      "Epoch 56/200\n",
      "596/596 - 8s - loss: 4.5488e-04\n",
      "Epoch 57/200\n",
      "596/596 - 9s - loss: 4.5129e-04\n",
      "Epoch 58/200\n",
      "596/596 - 8s - loss: 4.4924e-04\n",
      "Epoch 59/200\n",
      "596/596 - 8s - loss: 4.4882e-04\n",
      "Epoch 60/200\n",
      "596/596 - 9s - loss: 4.4110e-04\n",
      "Epoch 61/200\n",
      "596/596 - 9s - loss: 4.4121e-04\n",
      "Epoch 62/200\n",
      "596/596 - 9s - loss: 4.3570e-04\n",
      "Epoch 63/200\n",
      "596/596 - 9s - loss: 4.3548e-04\n",
      "Epoch 64/200\n",
      "596/596 - 9s - loss: 4.2978e-04\n",
      "Epoch 65/200\n",
      "596/596 - 9s - loss: 4.3088e-04\n",
      "Epoch 66/200\n",
      "596/596 - 8s - loss: 4.2717e-04\n",
      "Epoch 67/200\n",
      "596/596 - 9s - loss: 4.2317e-04\n",
      "Epoch 68/200\n",
      "596/596 - 8s - loss: 4.2146e-04\n",
      "Epoch 69/200\n",
      "596/596 - 8s - loss: 4.1985e-04\n",
      "Epoch 70/200\n",
      "596/596 - 9s - loss: 4.1979e-04\n",
      "Epoch 71/200\n",
      "596/596 - 9s - loss: 4.1702e-04\n",
      "Epoch 72/200\n",
      "596/596 - 8s - loss: 4.1298e-04\n",
      "Epoch 73/200\n",
      "596/596 - 9s - loss: 4.1018e-04\n",
      "Epoch 74/200\n",
      "596/596 - 9s - loss: 4.0845e-04\n",
      "Epoch 75/200\n",
      "596/596 - 9s - loss: 4.0863e-04\n",
      "Epoch 76/200\n",
      "596/596 - 8s - loss: 4.0624e-04\n",
      "Epoch 77/200\n",
      "596/596 - 8s - loss: 4.0355e-04\n",
      "Epoch 78/200\n",
      "596/596 - 9s - loss: 4.0121e-04\n",
      "Epoch 79/200\n",
      "596/596 - 8s - loss: 4.0401e-04\n",
      "Epoch 80/200\n",
      "596/596 - 8s - loss: 3.9767e-04\n",
      "Epoch 81/200\n",
      "596/596 - 8s - loss: 3.9567e-04\n",
      "Epoch 82/200\n",
      "596/596 - 8s - loss: 3.9430e-04\n",
      "Epoch 83/200\n",
      "596/596 - 9s - loss: 3.9270e-04\n",
      "Epoch 84/200\n",
      "596/596 - 9s - loss: 3.9152e-04\n",
      "Epoch 85/200\n",
      "596/596 - 9s - loss: 3.8895e-04\n",
      "Epoch 86/200\n",
      "596/596 - 8s - loss: 3.8870e-04\n",
      "Epoch 87/200\n",
      "596/596 - 8s - loss: 3.9009e-04\n",
      "Epoch 88/200\n",
      "596/596 - 8s - loss: 3.8831e-04\n",
      "Epoch 89/200\n",
      "596/596 - 9s - loss: 3.8142e-04\n",
      "Epoch 90/200\n",
      "596/596 - 9s - loss: 3.8156e-04\n",
      "Epoch 91/200\n",
      "596/596 - 9s - loss: 3.8142e-04\n",
      "Epoch 92/200\n",
      "596/596 - 8s - loss: 3.7910e-04\n",
      "Epoch 93/200\n",
      "596/596 - 8s - loss: 3.8008e-04\n",
      "Epoch 94/200\n",
      "596/596 - 9s - loss: 3.7690e-04\n",
      "Epoch 95/200\n",
      "596/596 - 9s - loss: 3.7648e-04\n",
      "Epoch 96/200\n",
      "596/596 - 9s - loss: 3.7361e-04\n",
      "Epoch 97/200\n",
      "596/596 - 8s - loss: 3.7430e-04\n",
      "Epoch 98/200\n",
      "596/596 - 8s - loss: 3.7106e-04\n",
      "Epoch 99/200\n",
      "596/596 - 8s - loss: 3.7200e-04\n",
      "Epoch 100/200\n",
      "596/596 - 8s - loss: 3.6783e-04\n",
      "Epoch 101/200\n",
      "596/596 - 9s - loss: 3.6741e-04\n",
      "Epoch 102/200\n",
      "596/596 - 8s - loss: 3.6567e-04\n",
      "Epoch 103/200\n",
      "596/596 - 8s - loss: 3.6477e-04\n",
      "Epoch 104/200\n",
      "596/596 - 9s - loss: 3.6531e-04\n",
      "Epoch 105/200\n",
      "596/596 - 8s - loss: 3.6344e-04\n",
      "Epoch 106/200\n",
      "596/596 - 8s - loss: 3.6222e-04\n",
      "Epoch 107/200\n",
      "596/596 - 8s - loss: 3.6123e-04\n",
      "Epoch 108/200\n",
      "596/596 - 8s - loss: 3.5909e-04\n",
      "Epoch 109/200\n",
      "596/596 - 9s - loss: 3.6037e-04\n",
      "Epoch 110/200\n",
      "596/596 - 9s - loss: 3.5857e-04\n",
      "Epoch 111/200\n",
      "596/596 - 8s - loss: 3.5893e-04\n",
      "Epoch 112/200\n",
      "596/596 - 8s - loss: 3.5679e-04\n",
      "Epoch 113/200\n",
      "596/596 - 9s - loss: 3.5395e-04\n",
      "Epoch 114/200\n",
      "596/596 - 8s - loss: 3.5413e-04\n",
      "Epoch 115/200\n",
      "596/596 - 8s - loss: 3.5186e-04\n",
      "Epoch 116/200\n",
      "596/596 - 8s - loss: 3.5328e-04\n",
      "Epoch 117/200\n",
      "596/596 - 8s - loss: 3.5261e-04\n",
      "Epoch 118/200\n",
      "596/596 - 8s - loss: 3.5015e-04\n",
      "Epoch 119/200\n",
      "596/596 - 8s - loss: 3.4722e-04\n",
      "Epoch 120/200\n",
      "596/596 - 8s - loss: 3.4924e-04\n",
      "Epoch 121/200\n",
      "596/596 - 9s - loss: 3.4813e-04\n",
      "Epoch 122/200\n",
      "596/596 - 9s - loss: 3.4568e-04\n",
      "Epoch 123/200\n",
      "596/596 - 8s - loss: 3.4532e-04\n",
      "Epoch 124/200\n",
      "596/596 - 8s - loss: 3.4539e-04\n",
      "Epoch 125/200\n",
      "596/596 - 9s - loss: 3.4500e-04\n",
      "Epoch 126/200\n",
      "596/596 - 9s - loss: 3.4178e-04\n",
      "Epoch 127/200\n",
      "596/596 - 8s - loss: 3.4305e-04\n",
      "Epoch 128/200\n",
      "596/596 - 8s - loss: 3.4243e-04\n",
      "Epoch 129/200\n",
      "596/596 - 8s - loss: 3.4124e-04\n",
      "Epoch 130/200\n",
      "596/596 - 8s - loss: 3.3970e-04\n",
      "Epoch 131/200\n",
      "596/596 - 8s - loss: 3.3747e-04\n",
      "Epoch 132/200\n",
      "596/596 - 8s - loss: 3.3887e-04\n",
      "Epoch 133/200\n",
      "596/596 - 9s - loss: 3.3794e-04\n",
      "Epoch 134/200\n",
      "596/596 - 9s - loss: 3.3701e-04\n",
      "Epoch 135/200\n",
      "596/596 - 9s - loss: 3.3675e-04\n",
      "Epoch 136/200\n",
      "596/596 - 9s - loss: 3.3564e-04\n",
      "Epoch 137/200\n",
      "596/596 - 8s - loss: 3.3478e-04\n",
      "Epoch 138/200\n",
      "596/596 - 8s - loss: 3.3247e-04\n",
      "Epoch 139/200\n",
      "596/596 - 8s - loss: 3.3421e-04\n",
      "Epoch 140/200\n",
      "596/596 - 8s - loss: 3.3245e-04\n",
      "Epoch 141/200\n",
      "596/596 - 8s - loss: 3.3113e-04\n",
      "Epoch 142/200\n",
      "596/596 - 8s - loss: 3.3111e-04\n",
      "Epoch 143/200\n",
      "596/596 - 8s - loss: 3.2920e-04\n",
      "Epoch 144/200\n",
      "596/596 - 8s - loss: 3.2950e-04\n",
      "Epoch 145/200\n",
      "596/596 - 8s - loss: 3.2857e-04\n",
      "Epoch 146/200\n",
      "596/596 - 8s - loss: 3.2736e-04\n",
      "Epoch 147/200\n",
      "596/596 - 8s - loss: 3.2886e-04\n",
      "Epoch 148/200\n",
      "596/596 - 9s - loss: 3.2712e-04\n",
      "Epoch 149/200\n",
      "596/596 - 8s - loss: 3.2590e-04\n",
      "Epoch 150/200\n",
      "596/596 - 8s - loss: 3.2743e-04\n",
      "Epoch 151/200\n",
      "596/596 - 8s - loss: 3.2318e-04\n",
      "Epoch 152/200\n",
      "596/596 - 8s - loss: 3.2428e-04\n",
      "Epoch 153/200\n",
      "596/596 - 8s - loss: 3.2428e-04\n",
      "Epoch 154/200\n",
      "596/596 - 8s - loss: 3.2215e-04\n",
      "Epoch 155/200\n",
      "596/596 - 8s - loss: 3.2115e-04\n",
      "Epoch 156/200\n",
      "596/596 - 9s - loss: 3.2185e-04\n",
      "Epoch 157/200\n",
      "596/596 - 8s - loss: 3.2128e-04\n",
      "Epoch 158/200\n",
      "596/596 - 8s - loss: 3.2096e-04\n",
      "Epoch 159/200\n",
      "596/596 - 8s - loss: 3.2020e-04\n",
      "Epoch 160/200\n",
      "596/596 - 8s - loss: 3.1769e-04\n",
      "Epoch 161/200\n",
      "596/596 - 8s - loss: 3.1788e-04\n",
      "Epoch 162/200\n",
      "596/596 - 8s - loss: 3.1655e-04\n",
      "Epoch 163/200\n",
      "596/596 - 8s - loss: 3.1865e-04\n",
      "Epoch 164/200\n",
      "596/596 - 8s - loss: 3.1713e-04\n",
      "Epoch 165/200\n",
      "596/596 - 9s - loss: 3.1504e-04\n",
      "Epoch 166/200\n",
      "596/596 - 8s - loss: 3.1552e-04\n",
      "Epoch 167/200\n",
      "596/596 - 9s - loss: 3.1476e-04\n",
      "Epoch 168/200\n",
      "596/596 - 8s - loss: 3.1326e-04\n",
      "Epoch 169/200\n",
      "596/596 - 8s - loss: 3.1464e-04\n",
      "Epoch 170/200\n",
      "596/596 - 8s - loss: 3.1310e-04\n",
      "Epoch 171/200\n",
      "596/596 - 8s - loss: 3.1273e-04\n",
      "Epoch 172/200\n",
      "596/596 - 8s - loss: 3.1129e-04\n",
      "Epoch 173/200\n",
      "596/596 - 8s - loss: 3.1146e-04\n",
      "Epoch 174/200\n",
      "596/596 - 8s - loss: 3.1140e-04\n",
      "Epoch 175/200\n",
      "596/596 - 8s - loss: 3.1153e-04\n",
      "Epoch 176/200\n",
      "596/596 - 8s - loss: 3.0808e-04\n",
      "Epoch 177/200\n",
      "596/596 - 8s - loss: 3.0950e-04\n",
      "Epoch 178/200\n",
      "596/596 - 8s - loss: 3.0937e-04\n",
      "Epoch 179/200\n",
      "596/596 - 17s - loss: 3.0925e-04\n",
      "Epoch 180/200\n",
      "596/596 - 8s - loss: 3.0752e-04\n",
      "Epoch 181/200\n",
      "596/596 - 8s - loss: 3.0909e-04\n",
      "Epoch 182/200\n",
      "596/596 - 8s - loss: 3.0479e-04\n",
      "Epoch 183/200\n",
      "596/596 - 8s - loss: 3.0656e-04\n",
      "Epoch 184/200\n",
      "596/596 - 8s - loss: 3.0567e-04\n",
      "Epoch 185/200\n",
      "596/596 - 8s - loss: 3.0553e-04\n",
      "Epoch 186/200\n",
      "596/596 - 8s - loss: 3.0369e-04\n",
      "Epoch 187/200\n",
      "596/596 - 8s - loss: 3.0312e-04\n",
      "Epoch 188/200\n",
      "596/596 - 8s - loss: 3.0535e-04\n",
      "Epoch 189/200\n",
      "596/596 - 8s - loss: 3.0224e-04\n",
      "Epoch 190/200\n",
      "596/596 - 8s - loss: 3.0264e-04\n",
      "Epoch 191/200\n",
      "596/596 - 8s - loss: 3.0320e-04\n",
      "Epoch 192/200\n",
      "596/596 - 8s - loss: 3.0087e-04\n",
      "Epoch 193/200\n",
      "596/596 - 9s - loss: 3.0154e-04\n",
      "Epoch 194/200\n",
      "596/596 - 8s - loss: 3.0078e-04\n",
      "Epoch 195/200\n",
      "596/596 - 9s - loss: 3.0128e-04\n",
      "Epoch 196/200\n",
      "596/596 - 9s - loss: 2.9799e-04\n",
      "Epoch 197/200\n",
      "596/596 - 9s - loss: 2.9930e-04\n",
      "Epoch 198/200\n",
      "596/596 - 9s - loss: 2.9783e-04\n",
      "Epoch 199/200\n",
      "596/596 - 9s - loss: 2.9726e-04\n",
      "Epoch 200/200\n",
      "596/596 - 9s - loss: 2.9826e-04\n",
      "Con2D-LSTM: [0.176752] 0.05144, 0.16292, 0.20049, 0.14471, 0.08769, 0.09671, 0.33364\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the number of subsequences and the length of subsequences\n",
    "n_steps, n_length = 2, 7\n",
    "# define the total days to use as input\n",
    "n_input = n_length * n_steps\n",
    "score, scores = evaluate_model(train, test, n_steps, n_length, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('Con2D-LSTM', score, scores)\n",
    "# plot scores\n",
    "days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thr', 'Fri', 'Sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the daily RMSE is created and shown as above.\n",
    "\n",
    "The plot shows that perhaps Thursdays are easier days to forecast than the other days and that perhaps Saturday at the end of a standard week is the hardest day to forecast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
